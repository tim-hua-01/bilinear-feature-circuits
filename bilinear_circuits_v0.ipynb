{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bilinear-feature-circuits\n"
     ]
    }
   ],
   "source": [
    "%cd /bilinear-feature-circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/tim/bilinear-feature-circuits'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.abspath('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/miniforge3/envs/new/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.abspath('.')\n",
    "sys.path.append(parent_dir + '/bilinear_interp_tim')\n",
    "sys.path.append(parent_dir + '/dictionary_learning')\n",
    "sys.path.append(parent_dir)\n",
    "import argparse\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import torch as t\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "\n",
    "from activation_utils import SparseAct\n",
    "from attribution import patching_effect, jvp\n",
    "from circuit_plotting import plot_circuit, plot_circuit_posaligned\n",
    "from dictionary_learning import AutoEncoder\n",
    "from loading_utils import load_examples, load_examples_nopair\n",
    "from nnsight import LanguageModel\n",
    "from language import Transformer, Sight\n",
    "from sae_adopter import DictionarySAE\n",
    "from bilinear_circuits_v0 import initialize_model_and_dictionaries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import einops\n",
    "def get_log_prob_from_resid(resid:t.Tensor, token_id:int):\n",
    "    return t.nn.functional.log_softmax(model.lm_head(resid), dim = -1)[0,-1,token_id].cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean and patch inputs of different shapes.\n",
      "Clean: 3 Patch: 4\n",
      "Clean and patch inputs of different shapes.\n",
      "Clean: 4 Patch: 3\n"
     ]
    }
   ],
   "source": [
    "device, model, embed, attns, mlps, resids, dictionaries, save_basename, examples, batch_size, num_examples, n_batches, batches = initialize_model_and_dictionaries(\n",
    "    device='cuda:0',\n",
    "    model_name=\"tdooms/fw-nano\",\n",
    "    dict_id='10',  # Note: This was originally an int, but the function expects a string.\n",
    "    d_model=1024,\n",
    "    dict_path='tdooms/fw-nano-scope',\n",
    "    dataset='simple_train',\n",
    "    num_examples=20,\n",
    "    example_length=None,\n",
    "    batch_size=4,\n",
    "    aggregation='sum',\n",
    "    nopair=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   330,  8366,   349,  5682,   298, 27854,   264,  3638,   302,\n",
       "           272, 15022, 28725,   390,  4249,  4155,  8708,  8570, 28725, 21750,\n",
       "           288,  5373, 28725, 10313, 19863, 28705,  2839,   272,   312, 13112,\n",
       "          1759,   442,  5681,   272,  2007,   794,  3324,   297,   516,  6125,\n",
       "         10573,  2696, 28723]], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor = t.tensor(model.tokenizer(\"A trigger is designed to activate a task of the virus, as display ing strange messages, deleting files, sending emails  begin the replicate process or whatever the programmer write in his malicious code.\")['input_ids'], device=device).unsqueeze(0)\n",
    "input_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.3740, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with t.no_grad():\n",
    "    basic_out = model._model.forward(input_ids = input_tensor, labels = input_tensor)\n",
    "print(basic_out.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current_token</th>\n",
       "      <th>current_token_id</th>\n",
       "      <th>next_token</th>\n",
       "      <th>next_token_id</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>330</td>\n",
       "      <td>-3.425913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>330</td>\n",
       "      <td>trigger</td>\n",
       "      <td>8366</td>\n",
       "      <td>-10.745538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trigger</td>\n",
       "      <td>8366</td>\n",
       "      <td>is</td>\n",
       "      <td>349</td>\n",
       "      <td>-1.523019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is</td>\n",
       "      <td>349</td>\n",
       "      <td>designed</td>\n",
       "      <td>5682</td>\n",
       "      <td>-7.466188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>designed</td>\n",
       "      <td>5682</td>\n",
       "      <td>to</td>\n",
       "      <td>298</td>\n",
       "      <td>-0.119676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>to</td>\n",
       "      <td>298</td>\n",
       "      <td>activate</td>\n",
       "      <td>27854</td>\n",
       "      <td>-3.481383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>activate</td>\n",
       "      <td>27854</td>\n",
       "      <td>a</td>\n",
       "      <td>264</td>\n",
       "      <td>-1.152388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a</td>\n",
       "      <td>264</td>\n",
       "      <td>task</td>\n",
       "      <td>3638</td>\n",
       "      <td>-6.570920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>task</td>\n",
       "      <td>3638</td>\n",
       "      <td>of</td>\n",
       "      <td>302</td>\n",
       "      <td>-5.884790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>of</td>\n",
       "      <td>302</td>\n",
       "      <td>the</td>\n",
       "      <td>272</td>\n",
       "      <td>-1.444379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>the</td>\n",
       "      <td>272</td>\n",
       "      <td>virus</td>\n",
       "      <td>15022</td>\n",
       "      <td>-9.084793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>virus</td>\n",
       "      <td>15022</td>\n",
       "      <td>,</td>\n",
       "      <td>28725</td>\n",
       "      <td>-2.764477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>,</td>\n",
       "      <td>28725</td>\n",
       "      <td>as</td>\n",
       "      <td>390</td>\n",
       "      <td>-4.901610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>as</td>\n",
       "      <td>390</td>\n",
       "      <td>display</td>\n",
       "      <td>4249</td>\n",
       "      <td>-12.524865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>display</td>\n",
       "      <td>4249</td>\n",
       "      <td>ing</td>\n",
       "      <td>4155</td>\n",
       "      <td>-13.160867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ing</td>\n",
       "      <td>4155</td>\n",
       "      <td>strange</td>\n",
       "      <td>8708</td>\n",
       "      <td>-15.359090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>strange</td>\n",
       "      <td>8708</td>\n",
       "      <td>messages</td>\n",
       "      <td>8570</td>\n",
       "      <td>-4.889879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>messages</td>\n",
       "      <td>8570</td>\n",
       "      <td>,</td>\n",
       "      <td>28725</td>\n",
       "      <td>-2.451844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>,</td>\n",
       "      <td>28725</td>\n",
       "      <td>delet</td>\n",
       "      <td>21750</td>\n",
       "      <td>-8.208416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>delet</td>\n",
       "      <td>21750</td>\n",
       "      <td>ing</td>\n",
       "      <td>288</td>\n",
       "      <td>-1.325034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ing</td>\n",
       "      <td>288</td>\n",
       "      <td>files</td>\n",
       "      <td>5373</td>\n",
       "      <td>-4.173593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>files</td>\n",
       "      <td>5373</td>\n",
       "      <td>,</td>\n",
       "      <td>28725</td>\n",
       "      <td>-0.712549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>,</td>\n",
       "      <td>28725</td>\n",
       "      <td>sending</td>\n",
       "      <td>10313</td>\n",
       "      <td>-6.363581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>sending</td>\n",
       "      <td>10313</td>\n",
       "      <td>emails</td>\n",
       "      <td>19863</td>\n",
       "      <td>-4.267689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>emails</td>\n",
       "      <td>19863</td>\n",
       "      <td></td>\n",
       "      <td>28705</td>\n",
       "      <td>-10.492948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td></td>\n",
       "      <td>28705</td>\n",
       "      <td>begin</td>\n",
       "      <td>2839</td>\n",
       "      <td>-17.157242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>begin</td>\n",
       "      <td>2839</td>\n",
       "      <td>the</td>\n",
       "      <td>272</td>\n",
       "      <td>-4.006895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>the</td>\n",
       "      <td>272</td>\n",
       "      <td>re</td>\n",
       "      <td>312</td>\n",
       "      <td>-5.835028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>re</td>\n",
       "      <td>312</td>\n",
       "      <td>plicate</td>\n",
       "      <td>13112</td>\n",
       "      <td>-5.295421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>plicate</td>\n",
       "      <td>13112</td>\n",
       "      <td>process</td>\n",
       "      <td>1759</td>\n",
       "      <td>-1.465232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>process</td>\n",
       "      <td>1759</td>\n",
       "      <td>or</td>\n",
       "      <td>442</td>\n",
       "      <td>-4.566404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>or</td>\n",
       "      <td>442</td>\n",
       "      <td>whatever</td>\n",
       "      <td>5681</td>\n",
       "      <td>-5.728073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>whatever</td>\n",
       "      <td>5681</td>\n",
       "      <td>the</td>\n",
       "      <td>272</td>\n",
       "      <td>-2.604283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>the</td>\n",
       "      <td>272</td>\n",
       "      <td>program</td>\n",
       "      <td>2007</td>\n",
       "      <td>-5.183568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>program</td>\n",
       "      <td>2007</td>\n",
       "      <td>mer</td>\n",
       "      <td>794</td>\n",
       "      <td>-2.122331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>mer</td>\n",
       "      <td>794</td>\n",
       "      <td>write</td>\n",
       "      <td>3324</td>\n",
       "      <td>-10.721448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>write</td>\n",
       "      <td>3324</td>\n",
       "      <td>in</td>\n",
       "      <td>297</td>\n",
       "      <td>-3.390911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>in</td>\n",
       "      <td>297</td>\n",
       "      <td>his</td>\n",
       "      <td>516</td>\n",
       "      <td>-5.019920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>his</td>\n",
       "      <td>516</td>\n",
       "      <td>mal</td>\n",
       "      <td>6125</td>\n",
       "      <td>-8.484257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>mal</td>\n",
       "      <td>6125</td>\n",
       "      <td>icious</td>\n",
       "      <td>10573</td>\n",
       "      <td>-0.805284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>icious</td>\n",
       "      <td>10573</td>\n",
       "      <td>code</td>\n",
       "      <td>2696</td>\n",
       "      <td>-0.443691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>code</td>\n",
       "      <td>2696</td>\n",
       "      <td>.</td>\n",
       "      <td>28723</td>\n",
       "      <td>-0.380909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   current_token  current_token_id next_token  next_token_id       loss\n",
       "0            <s>                 1          A            330  -3.425913\n",
       "1              A               330    trigger           8366 -10.745538\n",
       "2        trigger              8366         is            349  -1.523019\n",
       "3             is               349   designed           5682  -7.466188\n",
       "4       designed              5682         to            298  -0.119676\n",
       "5             to               298   activate          27854  -3.481383\n",
       "6       activate             27854          a            264  -1.152388\n",
       "7              a               264       task           3638  -6.570920\n",
       "8           task              3638         of            302  -5.884790\n",
       "9             of               302        the            272  -1.444379\n",
       "10           the               272      virus          15022  -9.084793\n",
       "11         virus             15022          ,          28725  -2.764477\n",
       "12             ,             28725         as            390  -4.901610\n",
       "13            as               390    display           4249 -12.524865\n",
       "14       display              4249        ing           4155 -13.160867\n",
       "15           ing              4155    strange           8708 -15.359090\n",
       "16       strange              8708   messages           8570  -4.889879\n",
       "17      messages              8570          ,          28725  -2.451844\n",
       "18             ,             28725      delet          21750  -8.208416\n",
       "19         delet             21750        ing            288  -1.325034\n",
       "20           ing               288      files           5373  -4.173593\n",
       "21         files              5373          ,          28725  -0.712549\n",
       "22             ,             28725    sending          10313  -6.363581\n",
       "23       sending             10313     emails          19863  -4.267689\n",
       "24        emails             19863                     28705 -10.492948\n",
       "25                           28705      begin           2839 -17.157242\n",
       "26         begin              2839        the            272  -4.006895\n",
       "27           the               272         re            312  -5.835028\n",
       "28            re               312    plicate          13112  -5.295421\n",
       "29       plicate             13112    process           1759  -1.465232\n",
       "30       process              1759         or            442  -4.566404\n",
       "31            or               442   whatever           5681  -5.728073\n",
       "32      whatever              5681        the            272  -2.604283\n",
       "33           the               272    program           2007  -5.183568\n",
       "34       program              2007        mer            794  -2.122331\n",
       "35           mer               794      write           3324 -10.721448\n",
       "36         write              3324         in            297  -3.390911\n",
       "37            in               297        his            516  -5.019920\n",
       "38           his               516        mal           6125  -8.484257\n",
       "39           mal              6125     icious          10573  -0.805284\n",
       "40        icious             10573       code           2696  -0.443691\n",
       "41          code              2696          .          28723  -0.380909"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the per token loss\n",
    "per_tok_loss = t.log_softmax(basic_out.logits[0,:-1,:], dim = -1).gather(dim = -1, index = input_tensor[0,1:].unsqueeze(-1)).squeeze(-1)\n",
    "#quick dataframe with what every token is and the next token that the loss is on\n",
    "df = pd.DataFrame({'current_token': [model.tokenizer.decode(s) for s in input_tensor[0, :-1].cpu().numpy()], \n",
    "                   'current_token_id': input_tensor[0, :-1].cpu().numpy(),\n",
    "                   'next_token': [model.tokenizer.decode(s) for s in input_tensor[0, 1:].cpu().numpy()],\n",
    "                   'next_token_id': input_tensor[0, 1:].cpu().numpy(),\n",
    "                     'loss': per_tok_loss.cpu().numpy()})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_submods = [embed] + [submod for layer_submods in zip(mlps, attns, resids) for submod in layer_submods]\n",
    "def single_tok_logit_metric(tok_ind:int) -> Callable[[LanguageModel],t.Tensor]:\n",
    "    def metric_fn(model: LanguageModel):\n",
    "        # Get the logits for the last token in the sequence\n",
    "        logits = model.lm_head.output[:, -1, :]\n",
    "        \n",
    "        # Apply log-softmax to convert logits to log probabilities\n",
    "        log_probs = t.nn.functional.log_softmax(logits, dim=-1)\n",
    "        \n",
    "        # Gather the log probability for the specified token index\n",
    "        log_prob = t.gather(log_probs, dim=-1, index=t.tensor([tok_ind], device=model.device).view(-1, 1)).squeeze(-1)\n",
    "        \n",
    "        return log_prob\n",
    "    return metric_fn\n",
    "short_input = input_tensor[:, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_diff_metric(tok1:int, tok2:int) -> Callable[[LanguageModel],t.Tensor]:\n",
    "    def metric_fn(model: LanguageModel):\n",
    "        # Get the logits for the last token in the sequence\n",
    "        logits1 = model.lm_head.output[:, -1, tok1]\n",
    "        logits2 = model.lm_head.output[:, -1, tok2]\n",
    "        diff = logits1 - logits2\n",
    "        diff.save()\n",
    "        return diff\n",
    "    return metric_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.1197], device='cuda:0', grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with model.trace(short_input):\n",
    "    log_loss = single_tok_logit_metric(298)(model)\n",
    "    log_loss.save()\n",
    "log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([13.8895, 10.3930,  9.5242,  9.4697,  9.3849,  9.2240,  8.6763,  8.6177],\n",
       "       device='cuda:0', grad_fn=<TopkBackward0>),\n",
       "indices=tensor([298, 354, 297, 390, 579, 486, 369, 304], device='cuda:0'))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.topk(model.forward(short_input).logits[0,-1], k = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to for'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode([298, 354])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Integrated Gradient estimation\n",
      "\n",
      "\n",
      "Initial trace\n",
      "\n",
      "\n",
      "Patching part\n",
      "\n"
     ]
    }
   ],
   "source": [
    "effects2, deltas2, grads2, total_effect2 = patching_effect(\n",
    "        clean = short_input,\n",
    "        patch = None,\n",
    "        model = model,\n",
    "        submodules = all_submods,\n",
    "        dictionaries = dictionaries,\n",
    "        metric_fn = logit_diff_metric(298, 354),\n",
    "        metric_kwargs=dict(),\n",
    "        method='ig' # get better approximations for early layers by using ig\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 0 \n",
      " Embedding(32000, 1024)\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.1885, 0.0785, 0.0083, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
      "indices=tensor([6113, 2894, 2963,   50,   36,   42,   39,   38,   28,   27,   23,   26,\n",
      "          30,   35,   37,   34,   15,   14,   10,    8,    0,    6,    3,    4,\n",
      "          17,   16,   19,   12,   21,   29,   32], device='cuda:0'))\n",
      "tensor([[[ 1.7470e-04],\n",
      "         [-1.8090e-03],\n",
      "         [-1.2685e-01],\n",
      "         [ 3.0078e-05],\n",
      "         [-2.6008e-02]]], device='cuda:0')\n",
      "Model: 1 \n",
      " MLP(\n",
      "  (w): Bilinear(\n",
      "    in_features=1024, out_features=8192, bias=True\n",
      "    (gate): Identity()\n",
      "  )\n",
      "  (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.3261, 0.1892, 0.1673, 0.1521, 0.1413, 0.1170, 0.1115, 0.0828, 0.0290,\n",
      "        0.0160, 0.0025, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
      "indices=tensor([1216, 6422, 6179, 7197, 6399, 2571, 4171,  391,  841, 7913, 2386,   40,\n",
      "          28,   37,   35,   33,    2,    1,    6,    0,    7,   25,   32,   23,\n",
      "          13,   11,    8,    9,   17,   14,   20], device='cuda:0'))\n",
      "tensor([[[ 0.0009],\n",
      "         [-0.0161],\n",
      "         [ 0.0926],\n",
      "         [-0.0077],\n",
      "         [-0.0507]]], device='cuda:0')\n",
      "Model: 2 \n",
      " Attention(\n",
      "  (rotary): Rotary()\n",
      "  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.5703, 0.4344, 0.1395, 0.0618, 0.0593, 0.0395, 0.0183, 0.0162, 0.0153,\n",
      "        0.0116, 0.0082, 0.0076, 0.0068, 0.0065, 0.0039, 0.0033, 0.0031, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
      "indices=tensor([3514, 2764, 5042, 2891, 6467, 6874, 8054, 6616, 2440, 6342,  693, 7805,\n",
      "          64, 6496, 3069, 3583, 2188,   22,   20,   21,   12,   10,   14,   16,\n",
      "           5,    3,    0,    2,    6,    4,    8], device='cuda:0'))\n",
      "tensor([[[-0.0009],\n",
      "         [ 0.0016],\n",
      "         [ 0.0609],\n",
      "         [ 0.0159],\n",
      "         [-0.0975]]], device='cuda:0')\n",
      "Model: 3 \n",
      " Layer(\n",
      "  (attn): Attention(\n",
      "    (rotary): Rotary()\n",
      "    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "    (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (mlp): MLP(\n",
      "    (w): Bilinear(\n",
      "      in_features=1024, out_features=8192, bias=True\n",
      "      (gate): Identity()\n",
      "    )\n",
      "    (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (n1): Norm(\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (n2): Norm(\n",
      "    (norm): Identity()\n",
      "  )\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([4.9573e-01, 1.4827e-01, 1.0028e-01, 8.1203e-02, 8.0425e-02, 5.9101e-02,\n",
      "        3.3550e-02, 3.1459e-02, 2.5265e-02, 2.4535e-02, 1.7388e-02, 1.1553e-04,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00], device='cuda:0'),\n",
      "indices=tensor([3338, 6715,  980, 7501, 3087, 1002, 3942, 5752,  633, 4148,  762, 7557,\n",
      "          31,   29,   28,   25,    2,    1,    4,    6,   23,   22,   24,   16,\n",
      "          11,    9,    7,    8,   13,   12,   15], device='cuda:0'))\n",
      "tensor([[[-0.0002],\n",
      "         [-0.0136],\n",
      "         [-0.1633],\n",
      "         [ 0.1036],\n",
      "         [ 0.1152]]], device='cuda:0')\n",
      "Model: 4 \n",
      " MLP(\n",
      "  (w): Bilinear(\n",
      "    in_features=1024, out_features=8192, bias=True\n",
      "    (gate): Identity()\n",
      "  )\n",
      "  (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.1158, 0.0198, 0.0185, 0.0122, 0.0118, 0.0062, 0.0056, 0.0056, 0.0009,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
      "indices=tensor([4025, 3321, 5469, 2293, 5806,  869, 4934,  647, 2948,   40,   37,   39,\n",
      "          31,   30,   32,   36,   10,    4,    2,    3,   13,    9,   14,   29,\n",
      "          22,   19,   17,   18,   26,   25,   27], device='cuda:0'))\n",
      "tensor([[[-0.0009],\n",
      "         [-0.0208],\n",
      "         [-0.1139],\n",
      "         [ 0.0763],\n",
      "         [ 0.0048]]], device='cuda:0')\n",
      "Model: 5 \n",
      " Attention(\n",
      "  (rotary): Rotary()\n",
      "  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.2278, 0.1006, 0.0665, 0.0599, 0.0547, 0.0489, 0.0316, 0.0308, 0.0248,\n",
      "        0.0094, 0.0057, 0.0045, 0.0015, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
      "indices=tensor([4035, 7604, 2148, 3971, 3539, 3271, 4382, 7517,  116, 7141, 2159, 1817,\n",
      "        4954,   39,   36,   38,    1,    0,    6,   35,   33,   29,   34,   26,\n",
      "          14,   11,    7,    8,   19,   17,   23], device='cuda:0'))\n",
      "tensor([[[-0.0016],\n",
      "         [-0.0049],\n",
      "         [-0.1189],\n",
      "         [ 0.0190],\n",
      "         [-0.0447]]], device='cuda:0')\n",
      "Model: 6 \n",
      " Layer(\n",
      "  (attn): Attention(\n",
      "    (rotary): Rotary()\n",
      "    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "    (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (mlp): MLP(\n",
      "    (w): Bilinear(\n",
      "      in_features=1024, out_features=8192, bias=True\n",
      "      (gate): Identity()\n",
      "    )\n",
      "    (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (n1): Norm(\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (n2): Norm(\n",
      "    (norm): Identity()\n",
      "  )\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.2013, 0.1682, 0.1383, 0.0685, 0.0397, 0.0167, 0.0157, 0.0142, 0.0048,\n",
      "        0.0003, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
      "indices=tensor([5723, 4902, 7551, 4411, 4425, 7934,  472, 2916, 5861, 7366,   35,   34,\n",
      "          27,   24,   31,   29,    6,    5,    3,    2,    7,   10,   28,   23,\n",
      "          16,   15,   11,   13,   19,   17,   20], device='cuda:0'))\n",
      "tensor([[[-0.0021],\n",
      "         [-0.0130],\n",
      "         [ 0.0890],\n",
      "         [ 0.0304],\n",
      "         [ 0.0222]]], device='cuda:0')\n",
      "Model: 7 \n",
      " MLP(\n",
      "  (w): Bilinear(\n",
      "    in_features=1024, out_features=8192, bias=True\n",
      "    (gate): Identity()\n",
      "  )\n",
      "  (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.0523, 0.0491, 0.0474, 0.0437, 0.0097, 0.0085, 0.0079, 0.0078, 0.0076,\n",
      "        0.0057, 0.0028, 0.0013, 0.0002, 0.0002, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
      "indices=tensor([7291, 3208, 4827, 7952,  418, 2332, 2875, 3866, 5684, 1651, 4938, 6060,\n",
      "        3767, 2668,   32,   26,    0,    1,   23,   22,   20,   19,   21,   17,\n",
      "          11,    9,    4,    5,   14,   13,   15], device='cuda:0'))\n",
      "tensor([[[-0.0026],\n",
      "         [-0.0016],\n",
      "         [-0.0184],\n",
      "         [-0.0262],\n",
      "         [-0.2940]]], device='cuda:0')\n",
      "Model: 8 \n",
      " Attention(\n",
      "  (rotary): Rotary()\n",
      "  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.2909, 0.0973, 0.0593, 0.0582, 0.0393, 0.0359, 0.0275, 0.0264, 0.0127,\n",
      "        0.0098, 0.0059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
      "indices=tensor([ 223, 1916, 2159, 3847,  775, 3342, 1594,  432, 6687, 7839, 5586,   32,\n",
      "          25,   31,   30,   29,    6,    4,    8,    3,    9,   24,   28,   23,\n",
      "          17,   16,   10,   12,   20,   18,   21], device='cuda:0'))\n",
      "tensor([[[ 3.0718e-05],\n",
      "         [-1.3173e-02],\n",
      "         [-4.4633e-02],\n",
      "         [-1.6957e-01],\n",
      "         [-2.4073e-01]]], device='cuda:0')\n",
      "Model: 9 \n",
      " Layer(\n",
      "  (attn): Attention(\n",
      "    (rotary): Rotary()\n",
      "    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "    (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (mlp): MLP(\n",
      "    (w): Bilinear(\n",
      "      in_features=1024, out_features=8192, bias=True\n",
      "      (gate): Identity()\n",
      "    )\n",
      "    (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (n1): Norm(\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (n2): Norm(\n",
      "    (norm): Identity()\n",
      "  )\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.0613, 0.0555, 0.0235, 0.0206, 0.0180, 0.0156, 0.0143, 0.0081, 0.0054,\n",
      "        0.0042, 0.0015, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
      "indices=tensor([6345, 6895, 1920, 5555,  757,   29, 1827,  214, 4013,  270, 7712,   48,\n",
      "          28,   47,   46,   40,   10,    6,   11,    1,   14,   27,   31,   26,\n",
      "          22,   20,   16,   18,   24,   23,   25], device='cuda:0'))\n",
      "tensor([[[-0.0012],\n",
      "         [ 0.0087],\n",
      "         [-0.0408],\n",
      "         [-0.0727],\n",
      "         [-0.3085]]], device='cuda:0')\n",
      "Model: 10 \n",
      " MLP(\n",
      "  (w): Bilinear(\n",
      "    in_features=1024, out_features=8192, bias=True\n",
      "    (gate): Identity()\n",
      "  )\n",
      "  (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.2283, 0.0827, 0.0735, 0.0556, 0.0522, 0.0450, 0.0422, 0.0351, 0.0270,\n",
      "        0.0183, 0.0111, 0.0063, 0.0045, 0.0035, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
      "indices=tensor([ 780, 6890, 7470, 2898, 2988, 1091, 1815, 5433, 1704, 2163, 7386, 3109,\n",
      "        5700, 5652,   29,   24,    1,    3,   23,   22,   20,   18,   21,   17,\n",
      "          12,    8,    4,    7,   15,   13,   16], device='cuda:0'))\n",
      "tensor([[[ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [-1.2810]]], device='cuda:0')\n",
      "Model: 11 \n",
      " Attention(\n",
      "  (rotary): Rotary()\n",
      "  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.0308, 0.0180, 0.0139, 0.0017, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
      "indices=tensor([6618, 8007, 3323, 2221,   54,   50,   49,   47,   35,   32,   28,   29,\n",
      "          43,   41,   46,   39,   16,   12,   10,   11,    8,    5,    0,    3,\n",
      "          22,   19,   24,   26,   37,   36,   38], device='cuda:0'))\n",
      "tensor([[[ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [-0.2322]]], device='cuda:0')\n",
      "Model: 12 \n",
      " Layer(\n",
      "  (attn): Attention(\n",
      "    (rotary): Rotary()\n",
      "    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "    (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (mlp): MLP(\n",
      "    (w): Bilinear(\n",
      "      in_features=1024, out_features=8192, bias=True\n",
      "      (gate): Identity()\n",
      "    )\n",
      "    (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (n1): Norm(\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (n2): Norm(\n",
      "    (norm): Identity()\n",
      "  )\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.2087, 0.0607, 0.0230, 0.0142, 0.0138, 0.0131, 0.0073, 0.0012, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
      "indices=tensor([3406, 2959, 5601, 8135, 5729, 5134, 4198, 4257,   50,   48,   43,   41,\n",
      "          36,   35,   39,   34,    8,    7,    2,    3,   12,    9,   16,   17,\n",
      "          26,   23,   18,   22,   30,   29,   33], device='cuda:0'))\n",
      "tensor([[[0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0868]]], device='cuda:0')\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.5703, 0.4957, 0.4344, 0.3618, 0.3261, 0.2909, 0.2642, 0.2283, 0.2278,\n",
      "        0.2213, 0.2087, 0.2013, 0.1985, 0.1892, 0.1885, 0.1719, 0.1682, 0.1673,\n",
      "        0.1562, 0.1541, 0.1521, 0.1503, 0.1483, 0.1413, 0.1395, 0.1383, 0.1303,\n",
      "        0.1170, 0.1158, 0.1115, 0.1077], device='cuda:0'),\n",
      "indices=tensor([ 63,  94,  64, 187,  32, 249,   1, 311, 156, 157, 373, 188,  95,  33,\n",
      "          2, 280, 189,  34, 190, 158,  35, 191,  96,  36,  65, 192, 218,  37,\n",
      "        125,  38, 193], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "all_effects2 = t.empty((1)).to(device)\n",
    "for i,m in enumerate(all_submods):\n",
    "    print(f\"Model: {i} \\n {m}\")\n",
    "    print(t.topk(effects2[m].act[0,-1], k = 31))\n",
    "    all_effects2 = t.cat((all_effects2, t.topk(effects2[m].act.flatten(), k = 31).values), dim = 0)\n",
    "    print(effects2[m].resc)\n",
    "print(t.topk(all_effects2, k = 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:02<00:00,  8.90it/s]\n",
      "100%|██████████| 150/150 [00:16<00:00,  9.10it/s]\n",
      "100%|██████████| 150/150 [00:16<00:00,  9.10it/s]\n",
      "100%|██████████| 150/150 [00:16<00:00,  9.20it/s]\n",
      "100%|██████████| 150/150 [00:16<00:00,  9.23it/s]\n",
      "100%|██████████| 150/150 [00:16<00:00,  9.09it/s]\n",
      "100%|██████████| 150/150 [00:16<00:00,  9.03it/s]\n",
      "100%|██████████| 150/150 [00:16<00:00,  9.05it/s]\n",
      "100%|██████████| 150/150 [00:16<00:00,  9.10it/s]\n",
      "100%|██████████| 150/150 [00:16<00:00,  9.12it/s]\n",
      "100%|██████████| 150/150 [00:16<00:00,  9.07it/s]\n",
      "100%|██████████| 150/150 [00:16<00:00,  9.11it/s]\n",
      "100%|██████████| 150/150 [00:16<00:00,  9.10it/s]\n"
     ]
    }
   ],
   "source": [
    "effects, deltas, grads, total_effect = patching_effect(\n",
    "        clean = short_input,\n",
    "        patch = None,\n",
    "        model = model,\n",
    "        submodules = all_submods,\n",
    "        dictionaries = dictionaries,\n",
    "        metric_fn = single_tok_logit_metric(298),\n",
    "        metric_kwargs=dict(),\n",
    "        method='exact' # get better approximations for early layers by using ig\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save effects\n",
    "t.save(effects[all_submods[0]].act, 'exact_effects_v0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.0117, 0.0027, 0.0022, 0.0015, 0.0013, 0.0005, 0.0005, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
       "indices=tensor([5700, 6005, 2988, 5433, 1704, 2163, 3046,   23,   12,   22,   21,   20,\n",
       "          18,   17,   19,   16,    4,    3,    1,    2,    6,    5,    7,    0,\n",
       "           8,   11,    9,   10,   14,   13,   15], device='cuda:0'))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.topk(effects[all_submods[-3]].act[0,-1], k = 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.3629, 0.0642, 0.0229, 0.0196, 0.0140, 0.0063, 0.0061, 0.0061, 0.0054,\n",
       "        0.0053, 0.0053, 0.0052, 0.0044, 0.0035, 0.0033, 0.0023, 0.0023, 0.0018,\n",
       "        0.0013, 0.0009, 0.0008, 0.0007, 0.0006, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000], device='cuda:0'),\n",
       "indices=tensor([1049,  780, 2768, 5492,  917, 1655, 8036, 7470, 1815, 2898, 7387, 6890,\n",
       "        6929, 3215, 7386, 3531, 6009, 5981, 1152, 2182, 3109, 1091, 5652,    7,\n",
       "           0,    6,    5,    4,    2,    1,    3], device='cuda:0'))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.topk(-effects[all_submods[-3]].act[0,-1], k = 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 0 \n",
      " Embedding(32000, 1024)\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.0200, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
      "indices=tensor([2894,   29,   27,   28,   24,   23,   25,   26,   18,   17,   15,   16,\n",
      "          20,   19,   21,   22,   10,    6,    3,    5,    1,    0,    2,    4,\n",
      "          12,    9,    7,    8,   13,   11,   14], device='cuda:0'))\n",
      "tensor([[ 1.1742e-05, -7.9274e-06, -1.7366e-02,  1.3538e-05, -3.0152e-03]],\n",
      "       device='cuda:0')\n",
      "Model: 1 \n",
      " MLP(\n",
      "  (w): Bilinear(\n",
      "    in_features=1024, out_features=8192, bias=True\n",
      "    (gate): Identity()\n",
      "  )\n",
      "  (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([2.3068e-02, 9.4175e-03, 3.0643e-03, 7.7426e-05, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00], device='cuda:0'),\n",
      "indices=tensor([4171,  391, 2838, 3318,   26,   25,   24,   23,   15,   14,   12,   13,\n",
      "          21,   20,   22,   19,    7,    6,    4,    5,    3,    2,    0,    1,\n",
      "           9,    8,   10,   11,   17,   16,   18], device='cuda:0'))\n",
      "tensor([[ 0.0016,  0.0022,  0.0076, -0.0002, -0.0176]], device='cuda:0')\n",
      "Model: 2 \n",
      " Attention(\n",
      "  (rotary): Rotary()\n",
      "  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.0023, 0.0020, 0.0016, 0.0015, 0.0013, 0.0009, 0.0008, 0.0006, 0.0006,\n",
      "        0.0005, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
      "indices=tensor([6342, 6874, 6467, 3514,   64, 2891, 2440, 3069, 2188, 7805,   20,   19,\n",
      "          15,   14,   18,   17,    3,    2,    1,    0,    4,    5,   16,   13,\n",
      "           9,    8,    6,    7,   11,   10,   12], device='cuda:0'))\n",
      "tensor([[-2.3372e-05, -6.8739e-05, -2.9311e-03,  2.6402e-04, -6.4884e-03]],\n",
      "       device='cuda:0')\n",
      "Model: 3 \n",
      " Layer(\n",
      "  (attn): Attention(\n",
      "    (rotary): Rotary()\n",
      "    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "    (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (mlp): MLP(\n",
      "    (w): Bilinear(\n",
      "      in_features=1024, out_features=8192, bias=True\n",
      "      (gate): Identity()\n",
      "    )\n",
      "    (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (n1): Norm(\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (n2): Norm(\n",
      "    (norm): Identity()\n",
      "  )\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([3.6859e-03, 3.4112e-03, 2.2893e-03, 5.5552e-04, 1.1159e-04, 6.2510e-05,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00], device='cuda:0'),\n",
      "indices=tensor([1703, 5691, 5689,  877, 7501, 6715,   24,   23,   13,   12,   22,   21,\n",
      "          19,   18,   20,   17,    5,    4,    2,    3,    7,    6,    1,    0,\n",
      "           8,    9,   10,   11,   15,   14,   16], device='cuda:0'))\n",
      "tensor([[ 0.0002, -0.0043,  0.0065,  0.0126, -0.0234]], device='cuda:0')\n",
      "Model: 4 \n",
      " MLP(\n",
      "  (w): Bilinear(\n",
      "    in_features=1024, out_features=8192, bias=True\n",
      "    (gate): Identity()\n",
      "  )\n",
      "  (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.0226, 0.0024, 0.0024, 0.0023, 0.0017, 0.0009, 0.0008, 0.0008, 0.0004,\n",
      "        0.0002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
      "indices=tensor([7054, 6532, 5969,  647, 4025, 2293, 2948, 5806, 6113, 5469,   20,   19,\n",
      "          15,   14,   18,   17,    3,    2,    1,    0,    4,    5,   16,   13,\n",
      "           9,    8,    6,    7,   11,   10,   12], device='cuda:0'))\n",
      "tensor([[-0.0002, -0.0007, -0.0074,  0.0054, -0.0354]], device='cuda:0')\n",
      "Model: 5 \n",
      " Attention(\n",
      "  (rotary): Rotary()\n",
      "  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.0241, 0.0135, 0.0078, 0.0069, 0.0041, 0.0038, 0.0036, 0.0034, 0.0030,\n",
      "        0.0026, 0.0021, 0.0021, 0.0018, 0.0015, 0.0015, 0.0015, 0.0002, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
      "indices=tensor([5321, 4035, 7604, 4382, 2159, 3271, 6633,  116, 3971, 7984, 7823, 2935,\n",
      "        4954, 3260, 7141, 1572, 1817,   13,   11,   12,    8,    7,    9,   10,\n",
      "           4,    2,    0,    1,    5,    3,    6], device='cuda:0'))\n",
      "tensor([[ 5.6483e-05,  5.7732e-04, -1.3482e-02,  2.5524e-03, -1.1044e-02]],\n",
      "       device='cuda:0')\n",
      "Model: 6 \n",
      " Layer(\n",
      "  (attn): Attention(\n",
      "    (rotary): Rotary()\n",
      "    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "    (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (mlp): MLP(\n",
      "    (w): Bilinear(\n",
      "      in_features=1024, out_features=8192, bias=True\n",
      "      (gate): Identity()\n",
      "    )\n",
      "    (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (n1): Norm(\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (n2): Norm(\n",
      "    (norm): Identity()\n",
      "  )\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.0093, 0.0034, 0.0029, 0.0020, 0.0012, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
      "indices=tensor([6042,  138, 6283, 5861, 2916,   25,   23,   24,   14,   13,   11,   22,\n",
      "          20,   19,   21,   18,    6,    5,    3,    4,    8,    2,    0,    1,\n",
      "           9,    7,   10,   12,   16,   15,   17], device='cuda:0'))\n",
      "tensor([[-0.0004, -0.0024, -0.0057,  0.0069,  0.0097]], device='cuda:0')\n",
      "Model: 7 \n",
      " MLP(\n",
      "  (w): Bilinear(\n",
      "    in_features=1024, out_features=8192, bias=True\n",
      "    (gate): Identity()\n",
      "  )\n",
      "  (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.0075, 0.0069, 0.0062, 0.0061, 0.0057, 0.0048, 0.0040, 0.0040, 0.0036,\n",
      "        0.0024, 0.0018, 0.0017, 0.0017, 0.0010, 0.0009, 0.0009, 0.0008, 0.0008,\n",
      "        0.0007, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
      "indices=tensor([7952, 2875, 3208, 4827, 3372, 7291, 3767,  921, 3866, 2902,  418, 6060,\n",
      "        1651, 4938, 2332,  900,  180, 2668, 5684,   11,    6,   10,    9,    8,\n",
      "           2,    1,    3,    0,    4,    5,    7], device='cuda:0'))\n",
      "tensor([[-0.0009, -0.0003, -0.0101, -0.0054,  0.0118]], device='cuda:0')\n",
      "Model: 8 \n",
      " Attention(\n",
      "  (rotary): Rotary()\n",
      "  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([1.4519e-02, 7.6479e-03, 7.3482e-03, 6.0581e-03, 5.9441e-03, 5.7412e-03,\n",
      "        5.6435e-03, 5.4811e-03, 4.7345e-03, 4.2552e-03, 4.0425e-03, 3.1709e-03,\n",
      "        3.1020e-03, 2.2381e-03, 1.7581e-03, 1.5816e-03, 1.5313e-03, 1.1633e-03,\n",
      "        3.4051e-04, 7.0855e-06, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00], device='cuda:0'),\n",
      "indices=tensor([ 223, 2159,  775, 1916, 5586, 7353, 3275, 5588, 1594,  143, 6687,  432,\n",
      "        3342, 2277, 3847, 7839, 1300, 7780,  217,  573,   10,    9,    8,    7,\n",
      "           1,    0,    2,    3,    5,    4,    6], device='cuda:0'))\n",
      "tensor([[ 7.2241e-05, -9.4920e-04,  1.1754e-03, -4.5927e-03,  7.6337e-03]],\n",
      "       device='cuda:0')\n",
      "Model: 9 \n",
      " Layer(\n",
      "  (attn): Attention(\n",
      "    (rotary): Rotary()\n",
      "    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "    (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (mlp): MLP(\n",
      "    (w): Bilinear(\n",
      "      in_features=1024, out_features=8192, bias=True\n",
      "      (gate): Identity()\n",
      "    )\n",
      "    (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (n1): Norm(\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (n2): Norm(\n",
      "    (norm): Identity()\n",
      "  )\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.0095, 0.0065, 0.0058, 0.0054, 0.0030, 0.0021, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
      "indices=tensor([6895, 7712,  214, 6345,  757, 4013,   24,   23,   13,   12,   22,   21,\n",
      "          19,   18,   20,   17,    5,    4,    2,    3,    7,    6,    1,    0,\n",
      "           8,    9,   10,   11,   15,   14,   16], device='cuda:0'))\n",
      "tensor([[ 0.0002, -0.0005, -0.0139, -0.0115, -0.0142]], device='cuda:0')\n",
      "Model: 10 \n",
      " MLP(\n",
      "  (w): Bilinear(\n",
      "    in_features=1024, out_features=8192, bias=True\n",
      "    (gate): Identity()\n",
      "  )\n",
      "  (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.0117, 0.0027, 0.0022, 0.0015, 0.0013, 0.0005, 0.0005, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
      "indices=tensor([5700, 6005, 2988, 5433, 1704, 2163, 3046,   23,   12,   22,   21,   20,\n",
      "          18,   17,   19,   16,    4,    3,    1,    2,    6,    5,    7,    0,\n",
      "           8,   11,    9,   10,   14,   13,   15], device='cuda:0'))\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.1598]], device='cuda:0')\n",
      "Model: 11 \n",
      " Attention(\n",
      "  (rotary): Rotary()\n",
      "  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.0014, 0.0011, 0.0009, 0.0004, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
      "indices=tensor([8007, 7634,  615, 3323,   26,   25,   24,   23,   15,   14,   12,   13,\n",
      "          21,   20,   22,   19,    7,    6,    4,    5,    3,    2,    0,    1,\n",
      "           9,    8,   10,   11,   17,   16,   18], device='cuda:0'))\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.0452]], device='cuda:0')\n",
      "Model: 12 \n",
      " Layer(\n",
      "  (attn): Attention(\n",
      "    (rotary): Rotary()\n",
      "    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "    (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (mlp): MLP(\n",
      "    (w): Bilinear(\n",
      "      in_features=1024, out_features=8192, bias=True\n",
      "      (gate): Identity()\n",
      "    )\n",
      "    (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (n1): Norm(\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (n2): Norm(\n",
      "    (norm): Identity()\n",
      "  )\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.0117, 0.0079, 0.0047, 0.0032, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
      "indices=tensor([4198, 3406,  708, 4375,   26,   25,   24,   23,   15,   14,   12,   13,\n",
      "          21,   20,   22,   19,    7,    6,    4,    5,    3,    2,    0,    1,\n",
      "           9,    8,   10,   11,   17,   16,   18], device='cuda:0'))\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.0998]], device='cuda:0')\n",
      "tensor([-0.0915,  0.0441,  0.0200,  0.0091,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0231,  0.0154,  0.0118,  0.0094,  0.0080,  0.0060,  0.0048,  0.0045,\n",
      "         0.0045,  0.0035,  0.0034,  0.0033,  0.0033,  0.0031,  0.0031,  0.0029,\n",
      "         0.0028,  0.0027,  0.0026,  0.0026,  0.0025,  0.0024,  0.0024,  0.0021,\n",
      "         0.0020,  0.0019,  0.0017,  0.0017,  0.0015,  0.0014,  0.0014,  0.0106,\n",
      "         0.0055,  0.0044,  0.0042,  0.0037,  0.0034,  0.0028,  0.0025,  0.0025,\n",
      "         0.0024,  0.0023,  0.0023,  0.0023,  0.0022,  0.0020,  0.0020,  0.0018,\n",
      "         0.0016,  0.0016,  0.0015,  0.0015,  0.0015,  0.0014,  0.0014,  0.0013,\n",
      "         0.0013,  0.0013,  0.0012,  0.0011,  0.0011,  0.0011,  0.0192,  0.0167,\n",
      "         0.0154,  0.0122,  0.0117,  0.0111,  0.0109,  0.0107,  0.0090,  0.0084,\n",
      "         0.0082,  0.0074,  0.0070,  0.0070,  0.0069,  0.0066,  0.0058,  0.0057,\n",
      "         0.0054,  0.0052,  0.0051,  0.0045,  0.0045,  0.0043,  0.0041,  0.0037,\n",
      "         0.0034,  0.0033,  0.0032,  0.0031,  0.0028,  0.0226,  0.0043,  0.0033,\n",
      "         0.0028,  0.0025,  0.0024,  0.0024,  0.0023,  0.0022,  0.0020,  0.0017,\n",
      "         0.0017,  0.0017,  0.0016,  0.0014,  0.0014,  0.0012,  0.0011,  0.0010,\n",
      "         0.0010,  0.0010,  0.0010,  0.0010,  0.0009,  0.0009,  0.0009,  0.0009,\n",
      "         0.0008,  0.0008,  0.0008,  0.0008,  0.0241,  0.0135,  0.0078,  0.0077,\n",
      "         0.0069,  0.0067,  0.0062,  0.0048,  0.0047,  0.0041,  0.0038,  0.0036,\n",
      "         0.0036,  0.0034,  0.0030,  0.0030,  0.0028,  0.0026,  0.0026,  0.0025,\n",
      "         0.0023,  0.0021,  0.0021,  0.0019,  0.0018,  0.0018,  0.0017,  0.0017,\n",
      "         0.0017,  0.0016,  0.0015,  0.0451,  0.0273,  0.0260,  0.0230,  0.0123,\n",
      "         0.0106,  0.0103,  0.0093,  0.0070,  0.0065,  0.0055,  0.0048,  0.0042,\n",
      "         0.0036,  0.0034,  0.0034,  0.0031,  0.0031,  0.0030,  0.0029,  0.0025,\n",
      "         0.0025,  0.0024,  0.0023,  0.0023,  0.0022,  0.0022,  0.0021,  0.0021,\n",
      "         0.0020,  0.0018,  0.0145,  0.0108,  0.0075,  0.0069,  0.0062,  0.0061,\n",
      "         0.0058,  0.0057,  0.0048,  0.0040,  0.0040,  0.0036,  0.0024,  0.0024,\n",
      "         0.0019,  0.0019,  0.0018,  0.0018,  0.0017,  0.0017,  0.0011,  0.0011,\n",
      "         0.0011,  0.0010,  0.0009,  0.0009,  0.0009,  0.0009,  0.0008,  0.0008,\n",
      "         0.0008,  0.0145,  0.0109,  0.0076,  0.0073,  0.0061,  0.0059,  0.0057,\n",
      "         0.0056,  0.0055,  0.0049,  0.0047,  0.0043,  0.0042,  0.0040,  0.0040,\n",
      "         0.0035,  0.0032,  0.0032,  0.0031,  0.0031,  0.0022,  0.0018,  0.0018,\n",
      "         0.0017,  0.0016,  0.0015,  0.0013,  0.0013,  0.0012,  0.0012,  0.0011,\n",
      "         0.0224,  0.0112,  0.0095,  0.0069,  0.0065,  0.0058,  0.0058,  0.0054,\n",
      "         0.0030,  0.0026,  0.0025,  0.0025,  0.0023,  0.0022,  0.0021,  0.0018,\n",
      "         0.0016,  0.0014,  0.0014,  0.0014,  0.0013,  0.0012,  0.0011,  0.0010,\n",
      "         0.0010,  0.0009,  0.0009,  0.0008,  0.0008,  0.0007,  0.0007,  0.0117,\n",
      "         0.0027,  0.0022,  0.0015,  0.0013,  0.0005,  0.0005,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0014,  0.0011,\n",
      "         0.0009,  0.0004,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0117,  0.0079,  0.0047,\n",
      "         0.0032,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "all_effects = t.empty((1)).to(device)\n",
    "for i,m in enumerate(all_submods):\n",
    "    print(f\"Model: {i} \\n {m}\")\n",
    "    print(t.topk(effects[m].act[0,-1], k = 31))\n",
    "    all_effects = t.cat((all_effects, t.topk(effects[m].act.flatten(), k = 31).values), dim = 0)\n",
    "    print(effects[m].resc)\n",
    "print(all_effects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.0451, 0.0441, 0.0273, 0.0260, 0.0241, 0.0231, 0.0230, 0.0226, 0.0224,\n",
       "        0.0200, 0.0192, 0.0167, 0.0154, 0.0154, 0.0145, 0.0145, 0.0135, 0.0123,\n",
       "        0.0122, 0.0118, 0.0117, 0.0117, 0.0117, 0.0112, 0.0111, 0.0109, 0.0109,\n",
       "        0.0108, 0.0107, 0.0106, 0.0106], device='cuda:0'),\n",
       "indices=tensor([187,   1, 188, 189, 156,  32, 190, 125, 280,   2,  94,  95,  33,  96,\n",
       "        249, 218, 157, 191,  97,  34, 311, 373,  98, 281,  99, 100, 250, 219,\n",
       "        101,  63, 192], device='cuda:0'))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.topk(all_effects, k = 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "with model.trace(short_input):\n",
    "    final_resid_mid = model._envoy.transformer.h[3].n2.input\n",
    "    final_resid_mid.save()\n",
    "    final_mlp_out = model._envoy.transformer.h[3].mlp.output\n",
    "    final_mlp_out.save()\n",
    "    final_resid_post = model._envoy.transformer.h[3].output\n",
    "    final_resid_post.save()\n",
    "    feature_acts = dictionaries[all_submods[-1]].encode(final_resid_post)\n",
    "    feature_acts.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([30.5674, 26.3677, 22.0909, 13.3291, 11.0112,  9.4994,  6.2468,  6.1364,\n",
       "         5.6926,  5.6152,  5.3664,  5.3422,  4.8915,  4.7654,  4.6877,  4.4282,\n",
       "         4.3245,  4.2570,  4.1588,  4.1180,  3.9214,  3.8790,  3.8148,  3.6669,\n",
       "         3.6364,  3.5652,  3.5551,  3.5463,  3.5067,  3.4497,  0.0000],\n",
       "       device='cuda:0', grad_fn=<TopkBackward0>),\n",
       "indices=tensor([4257, 8139, 5601, 7929, 5729, 1993, 3448, 4375, 2227, 5050, 5953, 3406,\n",
       "        6962,  731, 4198, 8135, 5134, 1875, 2081, 4762, 8088,  512, 2775, 3865,\n",
       "         708,  550, 7194, 2376, 3427, 2959,   11], device='cuda:0'))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.topk(feature_acts[0,-1], k = 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0032156705856323242"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.nn.functional.log_softmax(model.lm_head(final_resid_post), dim = -1)[0,-1,298]\n",
    "a = a.cpu().item()\n",
    "b = t.nn.functional.log_softmax(model.lm_head(final_resid_post + dictionaries[all_submods[-1]].w_dec.weight.data[:,5729]*0.1), dim = -1)[0,-1,298]\n",
    "b = b.cpu().item()\n",
    "(b - a)/0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.1371, 0.1180, 0.1056, 0.1043, 0.1011, 0.0741, 0.0709, 0.0590, 0.0527,\n",
       "        0.0515, 0.0509, 0.0488, 0.0475, 0.0470, 0.0466, 0.0447, 0.0427, 0.0389,\n",
       "        0.0371, 0.0347, 0.0314, 0.0311, 0.0308, 0.0291, 0.0290, 0.0288, 0.0287,\n",
       "        0.0259, 0.0256, 0.0252, 0.0252], device='cuda:0'),\n",
       "indices=tensor([187, 373,   1,  32,  94, 188,   2,  33, 280,   3, 189,   4, 311,  63,\n",
       "        190,  95,   5,  64,  65,   6, 281,  96, 191,   7,  97,  98, 192, 249,\n",
       "        193,  99, 100], device='cuda:0'))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.topk(all_effects, k = 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,  330, 8366,  349, 5682]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32000, 1024])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.w_u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logit_value_at_resid_and_modules(model: LanguageModel, token_id: int, input_tensor: t.Tensor):\n",
    "    resid_direction = model.w_u[token_id]\n",
    "\n",
    "    attn_outs = []\n",
    "    mlp_outs = []\n",
    "    resid_outs = []\n",
    "    with model.trace(input_tensor,validate = True, scan = True):\n",
    "        for layer in range(4):\n",
    "            attn_out = model._envoy.transformer.h[layer].attn.output\n",
    "            attn_out = attn_out[0,-1,:]\n",
    "            attn_out = t.dot(attn_out, resid_direction)\n",
    "            attn_out.save() #should be shape (batch, seq, d_model)\n",
    "            mlp_out = model._envoy.transformer.h[layer].mlp.output\n",
    "            mlp_out = mlp_out[0,-1,:]\n",
    "            mlp_out = t.dot(mlp_out, resid_direction)\n",
    "            mlp_out.save()\n",
    "            resid_out = model._envoy.transformer.h[layer].output\n",
    "            resid_out = resid_out[0,-1,:]\n",
    "            resid_out = t.dot(resid_out, resid_direction)\n",
    "            resid_out.save()\n",
    "            attn_outs.append(attn_out)\n",
    "            mlp_outs.append(mlp_out)\n",
    "            resid_outs.append(resid_out)\n",
    "    return attn_outs, mlp_outs, resid_outs\n",
    "\n",
    "attn_outs, mlp_outs, resid_outs = get_logit_value_at_resid_and_modules(model, 298, short_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.2589, device='cuda:0', grad_fn=<DotBackward0>),\n",
       " tensor(-0.3608, device='cuda:0', grad_fn=<DotBackward0>),\n",
       " tensor(0.0997, device='cuda:0', grad_fn=<DotBackward0>),\n",
       " tensor(8.5141, device='cuda:0', grad_fn=<DotBackward0>)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.6884, device='cuda:0', grad_fn=<DotBackward0>),\n",
       " tensor(0.7722, device='cuda:0', grad_fn=<DotBackward0>),\n",
       " tensor(0.8675, device='cuda:0', grad_fn=<DotBackward0>),\n",
       " tensor(8.4181, device='cuda:0', grad_fn=<DotBackward0>)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.9138, device='cuda:0', grad_fn=<DotBackward0>),\n",
       " tensor(1.5584, device='cuda:0', grad_fn=<DotBackward0>),\n",
       " tensor(2.4611, device='cuda:0', grad_fn=<DotBackward0>),\n",
       " tensor(13.8895, device='cuda:0', grad_fn=<DotBackward0>)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resid_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.8895, device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(short_input).logits[0,-1,298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_prob_at_resid(model: LanguageModel, token_id: int, input_tensor: t.Tensor):\n",
    "    resid_mids = []\n",
    "    resid_outs = []\n",
    "    with model.trace(input_tensor,validate = True, scan = True):\n",
    "        for layer in range(4):\n",
    "            resid_out = model._envoy.transformer.h[layer].output\n",
    "            resid_out = t.nn.functional.log_softmax(model.lm_head(resid_out), dim = -1)[0,-1,token_id]\n",
    "            resid_out.save()\n",
    "            resid_mid = model._envoy.transformer.h[layer].n2.input\n",
    "            resid_mid = t.nn.functional.log_softmax(model.lm_head(resid_mid), dim = -1)[0,-1,token_id]\n",
    "            resid_mid.save()\n",
    "            resid_mids.append(resid_mid)\n",
    "            resid_outs.append(resid_out)\n",
    "    return resid_mids, resid_outs\n",
    "\n",
    "resid_mids, resid_outs = get_log_prob_at_resid(model, 298, short_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resid mid: tensor(-10.2324, device='cuda:0', grad_fn=<SelectBackward0>), Resid out: tensor(-9.2326, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Resid mid: tensor(-9.4013, device='cuda:0', grad_fn=<SelectBackward0>), Resid out: tensor(-8.3498, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Resid mid: tensor(-8.3223, device='cuda:0', grad_fn=<SelectBackward0>), Resid out: tensor(-6.9268, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Resid mid: tensor(-3.0950, device='cuda:0', grad_fn=<SelectBackward0>), Resid out: tensor(-0.1197, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for mid, out in zip(resid_mids, resid_outs):\n",
    "    print(f\"Resid mid: {mid}, Resid out: {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-9.2326, device='cuda:0', grad_fn=<SelectBackward0>),\n",
       " tensor(-8.3498, device='cuda:0', grad_fn=<SelectBackward0>),\n",
       " tensor(-6.9268, device='cuda:0', grad_fn=<SelectBackward0>),\n",
       " tensor(-0.1197, device='cuda:0', grad_fn=<SelectBackward0>)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resid_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_mlp_dict = dictionaries[all_submods[-3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.0117, 0.0027, 0.0022, 0.0015, 0.0013, 0.0005, 0.0005, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
       "indices=tensor([5700, 6005, 2988, 5433, 1704, 2163, 3046,   23,   12,   22,   21,   20,\n",
       "          18,   17,   19,   16,    4,    3,    1,    2,    6,    5,    7,    0,\n",
       "           8,   11,    9,   10,   14,   13,   15], device='cuda:0'))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.topk(effects[all_submods[-3]].act[0,-1], k = 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([29.1835, 22.2516, 15.9731, 14.5302, 13.7950,  8.8346,  7.3819,  5.1124,\n",
       "         4.6354,  4.5241,  4.1688,  4.1099,  3.8820,  3.6814,  3.4158,  3.1766,\n",
       "         3.1443,  3.1332,  3.0301,  2.9100,  2.9078,  2.9051,  2.8587,  2.8534,\n",
       "         2.8001,  2.6894,  2.6483,  2.6281,  2.5836,  2.5582,  0.0000],\n",
       "       device='cuda:0', grad_fn=<TopkBackward0>),\n",
       "indices=tensor([2768,  917, 5700, 1815,  780, 6890, 6009, 5433, 6929, 2988, 5981, 7387,\n",
       "        1049, 1091, 8036, 3046, 2163, 7386, 3215, 1704, 3531, 6005, 7470, 3109,\n",
       "        1655, 2182, 2898, 5652, 5492, 1152,    4], device='cuda:0'))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.topk(fin_mlp_dict.encode(final_mlp_out)[0,-1], k = 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([29.1835, 22.2516, 15.9731, 14.5302, 13.7950,  8.8346,  7.3819,  5.1124,\n",
       "         4.6354,  4.5241,  4.1688,  4.1099,  3.8820], device='cuda:0',\n",
       "       grad_fn=<TopkBackward0>),\n",
       "indices=tensor([2768,  917, 5700, 1815,  780, 6890, 6009, 5433, 6929, 2988, 5981, 7387,\n",
       "        1049], device='cuda:0'))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.topk(fin_mlp_dict.encode(final_mlp_out)[0,-1], k = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To unembedding\n",
      "tensor([-0.0074, -0.0074,  0.0074, -0.0074, -0.0074, -0.0074, -0.0074, -0.0074,\n",
      "        -0.0074,  0.0074, -0.0074,  0.0074, -0.0074,  0.0074, -0.0074,  0.0074,\n",
      "         0.0074, -0.0074, -0.0074, -0.0074,  0.0074,  0.0074,  0.0074,  0.0074,\n",
      "        -0.0074, -0.0074, -0.0074,  0.0074,  0.0074,  0.0074], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#Test cosine sim with to unembedding and for unembedding\n",
    "print(\"To unembedding\")\n",
    "print(t.nn.functional.cosine_similarity(model._model.lm_head.weight.data[298], fin_mlp_dict.w_dec.weight.data.gather(dim = 1, index = t.tensor([2768,  917, 5700, 1815,  780, 6890, 6009, 5433, 6929, 2988, 5981, 7387,\n",
    "        1049, 1091, 8036, 3046, 2163, 7386, 3215, 1704, 3531, 6005, 7470, 3109,\n",
    "        1655, 2182, 2898, 5652, 5492, 1152], device = device).view(-1,1)), dim = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 8192])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_mlp_dict.w_dec.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0686,  0.0885, -0.0354, -0.0079,  0.1593,  0.1181,  0.0409,  0.1122,\n",
       "         0.0206,  0.0194,  0.0635,  0.0430,  0.7374,  0.0481,  0.0128,  0.0382,\n",
       "         0.0151,  0.0308,  0.0126, -0.0035,  0.0295,  0.0028,  0.1120,  0.0443,\n",
       "         0.0932,  0.0081,  0.1009, -0.0703,  0.1457,  0.0227], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.nn.functional.cosine_similarity(model._model.lm_head.weight.data[298].unsqueeze(0), fin_mlp_dict.w_dec.weight.data[:,[2768,  917, 5700, 1815,  780, 6890, 6009, 5433, 6929, 2988, 5981, 7387,\n",
    "        1049, 1091, 8036, 3046, 2163, 7386, 3215, 1704, 3531, 6005, 7470, 3109,\n",
    "        1655, 2182, 2898, 5652, 5492, 1152]].T, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7374], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.nn.functional.cosine_similarity(model._model.lm_head.weight.data[298].unsqueeze(0), fin_mlp_dict.w_dec.weight.data[:,1049], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1049"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[2768,  917, 5700, 1815,  780, 6890, 6009, 5433, 6929, 2988, 5981, 7387,\n",
    "        1049, 1091, 8036, 3046, 2163, 7386, 3215, 1704, 3531, 6005, 7470, 3109,\n",
    "        1655, 2182, 2898, 5652, 5492, 1152][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0611,  0.0848, -0.0342, -0.0031,  0.1825,  0.1306,  0.0373,  0.1209,\n",
       "         0.0076,  0.0374,  0.0277,  0.0244,  0.1479,  0.0667, -0.0093,  0.0141,\n",
       "         0.0241,  0.0358, -0.0075,  0.0114,  0.0193, -0.0329,  0.1507,  0.0468,\n",
       "         0.0579,  0.0038,  0.1324, -0.0667,  0.0776,  0.0106], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.nn.functional.cosine_similarity(model._model.lm_head.weight.data[354].unsqueeze(0), fin_mlp_dict.w_dec.weight.data[:,[2768,  917, 5700, 1815,  780, 6890, 6009, 5433, 6929, 2988, 5981, 7387,\n",
    "        1049, 1091, 8036, 3046, 2163, 7386, 3215, 1704, 3531, 6005, 7470, 3109,\n",
    "        1655, 2182, 2898, 5652, 5492, 1152]].T, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0164,  0.0111,  0.0240,  ..., -0.0193,  0.0025,  0.0007],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_mlp_dict.w_dec.weight.data[:,2768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2768,  917, 5700,  ..., 5652, 5492, 1152],\n",
       "        [2768,  917, 5700,  ..., 5652, 5492, 1152],\n",
       "        [2768,  917, 5700,  ..., 5652, 5492, 1152],\n",
       "        ...,\n",
       "        [2768,  917, 5700,  ..., 5652, 5492, 1152],\n",
       "        [2768,  917, 5700,  ..., 5652, 5492, 1152],\n",
       "        [2768,  917, 5700,  ..., 5652, 5492, 1152]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "einops.repeat(t.tensor([2768,  917, 5700, 1815,  780, 6890, 6009, 5433, 6929, 2988, 5981, 7387,\n",
    "        1049, 1091, 8036, 3046, 2163, 7386, 3215, 1704, 3531, 6005, 7470, 3109,\n",
    "        1655, 2182, 2898, 5652, 5492, 1152], device = device), \"d_active -> d_model d_active\", d_model = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_out_error = fin_mlp_dict.forward(final_mlp_out) - final_mlp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.7723, device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head(mlp_out_error)[0,-1,298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.4181, device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head(final_mlp_out)[0,-1,298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.27950629591941833"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_log_prob_from_resid(fin_mlp_dict.forward(final_mlp_out) + final_resid_mid, 298)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7561617278151684"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.exp(-0.2795)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1196756660938263"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_log_prob_from_resid(final_resid_post, 298)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.48257333040237427"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_log_prob_from_resid(final_resid_post - 3.8820*fin_mlp_dict.w_dec.weight.data[:,1049], 298)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.362897664308548"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.48257333040237427-0.1196756660938263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011735036969184875"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.10794062912464142--0.1196756660938263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model.trace(short_input,validate = True, scan = True):\n",
    "    attn_out = all_submods[2].output\n",
    "    attn_out.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 8192])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effects[all_submods[2]].act.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attention(\n",
       "  (rotary): Rotary()\n",
       "  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_submods[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attention(\n",
       "  (rotary): Rotary()\n",
       "  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_submods[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model.trace(short_input):\n",
    "    attn_out = all_submods[2].output\n",
    "    attn_out.save()\n",
    "    attn_features = dictionaries[all_submods[2]].encode(attn_out)\n",
    "    attn_features.save()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([4.7195, 3.8420, 3.6028, 2.7154, 2.5271, 1.9471, 1.0818, 1.0308, 0.8742,\n",
       "        0.7943, 0.6145, 0.5480, 0.4669, 0.4183, 0.4171, 0.4069, 0.3847, 0.3663,\n",
       "        0.3576, 0.3503, 0.3502, 0.3388, 0.3180, 0.3176, 0.3152, 0.3129, 0.3127,\n",
       "        0.3016, 0.3008, 0.2949, 0.0000], device='cuda:0',\n",
       "       grad_fn=<TopkBackward0>),\n",
       "indices=tensor([3514, 6153, 5042,   57,  993, 2764, 3472, 6342, 6874, 6467, 2177, 2452,\n",
       "        5177, 2891, 5705, 3069, 6496,  693, 1688, 3583, 2440,   64, 7546, 6616,\n",
       "        6597, 2188, 7805,  617, 8054,  507,   10], device='cuda:0'))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.topk(attn_features[0,-1], k=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.0064, 0.0053, 0.0018, 0.0017, 0.0015, 0.0014, 0.0011, 0.0010, 0.0007,\n",
       "        0.0006, 0.0005, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
       "indices=tensor([6342, 6874,   64, 2188, 7805, 6597, 2440, 7546, 8054, 5705,  693,   34,\n",
       "          28,   33,   31,   30,    3,    1,    4,    0,    5,   24,   29,   22,\n",
       "          15,   14,    6,    8,   20,   17,   21], device='cuda:0'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.topk(effects[all_submods[2]].act[0,-1], k=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.9308, 0.3497, 0.3330, 0.0747, 0.0476, 0.0359, 0.0260, 0.0211, 0.0207,\n",
       "        0.0183, 0.0147, 0.0147, 0.0107, 0.0100, 0.0093, 0.0040, 0.0039, 0.0027,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
       "indices=tensor([3514, 5042, 2764, 2891, 6467, 6874, 3583, 6616, 2440, 2188, 3472,   64,\n",
       "        6496, 8054, 3069,  617, 6342,  507,   25,   24,   20,   15,   23,   22,\n",
       "           5,    4,    1,    0,    8,   12,   21], device='cuda:0'))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.topk(effects[all_submods[2]].act[0,-1], k=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_act = t.linalg.inv(all_submods[2].o.weight.data) @ dictionaries[all_submods[2]].w_dec.weight.data[:,3514]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0168, -0.0135, -0.0028,  ...,  0.0112,  0.0228, -0.0876],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionaries[all_submods[2]].w_dec.weight.data[:,3514] #Attn feature vector. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0310, -0.0481, -0.0239,  ..., -0.0088,  0.0255,  0.0045],\n",
       "         [-0.0375, -0.0211, -0.0199,  ...,  0.0019, -0.0275,  0.0232],\n",
       "         [-0.0216, -0.0343, -0.0004,  ...,  0.0041, -0.0319, -0.0123],\n",
       "         ...,\n",
       "         [ 0.0213, -0.0161,  0.0003,  ..., -0.0366, -0.0450,  0.0189],\n",
       "         [-0.0372, -0.0171, -0.0287,  ...,  0.0051, -0.0021, -0.0093],\n",
       "         [-0.0122,  0.0034, -0.0295,  ...,  0.0287, -0.0288, -0.0323]],\n",
       "\n",
       "        [[ 0.0294,  0.0199,  0.0405,  ..., -0.0482, -0.0402,  0.0106],\n",
       "         [-0.0006, -0.0392, -0.0516,  ...,  0.0176,  0.0500,  0.0209],\n",
       "         [-0.0497, -0.0257, -0.0701,  ...,  0.0084,  0.0166, -0.0136],\n",
       "         ...,\n",
       "         [ 0.0116,  0.0372, -0.0124,  ..., -0.0397, -0.0293,  0.0179],\n",
       "         [-0.0245, -0.0056, -0.0351,  ..., -0.0306, -0.0576, -0.0090],\n",
       "         [ 0.0249, -0.0059,  0.0240,  ..., -0.0290,  0.0144, -0.0305]],\n",
       "\n",
       "        [[-0.0017,  0.0003, -0.0156,  ...,  0.0133,  0.0138, -0.0217],\n",
       "         [-0.0744,  0.0037,  0.0246,  ...,  0.0663,  0.0183, -0.0691],\n",
       "         [-0.0114,  0.0021, -0.0182,  ..., -0.0292, -0.0482,  0.0078],\n",
       "         ...,\n",
       "         [ 0.0487, -0.0433, -0.0560,  ..., -0.0193, -0.0383, -0.0138],\n",
       "         [ 0.0493, -0.0233,  0.0323,  ...,  0.0077, -0.0294, -0.0358],\n",
       "         [ 0.0318,  0.0384, -0.0408,  ..., -0.0142, -0.0241, -0.0138]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0399,  0.0192, -0.0215,  ...,  0.0041,  0.0098,  0.0525],\n",
       "         [-0.0569, -0.0581,  0.0172,  ..., -0.0179, -0.0101,  0.0182],\n",
       "         [ 0.0229, -0.0031, -0.0153,  ...,  0.0277,  0.0269, -0.0094],\n",
       "         ...,\n",
       "         [ 0.0291,  0.0188,  0.0446,  ...,  0.0195, -0.0195, -0.0479],\n",
       "         [ 0.0208, -0.0228,  0.0017,  ...,  0.0298, -0.0568, -0.0914],\n",
       "         [ 0.0056, -0.0042,  0.0234,  ...,  0.0005,  0.0847, -0.0033]],\n",
       "\n",
       "        [[ 0.0018, -0.0449,  0.0258,  ...,  0.0144, -0.0264, -0.0061],\n",
       "         [ 0.0358,  0.0207, -0.0121,  ..., -0.0436,  0.0213,  0.0334],\n",
       "         [ 0.0105,  0.0329, -0.0079,  ...,  0.0083, -0.0301,  0.0109],\n",
       "         ...,\n",
       "         [-0.0085,  0.0624, -0.0366,  ...,  0.0064,  0.0526, -0.0359],\n",
       "         [ 0.0312, -0.0530,  0.0190,  ...,  0.0299, -0.0502,  0.0149],\n",
       "         [ 0.0099, -0.0325, -0.0069,  ..., -0.0139, -0.0002,  0.0071]],\n",
       "\n",
       "        [[-0.0498,  0.0303, -0.0225,  ...,  0.0019, -0.0145, -0.0028],\n",
       "         [-0.0191, -0.0251,  0.0267,  ...,  0.0038, -0.0166, -0.0158],\n",
       "         [ 0.0254, -0.0240,  0.0095,  ..., -0.0256, -0.0149,  0.0116],\n",
       "         ...,\n",
       "         [-0.0397,  0.0357, -0.0129,  ..., -0.0321, -0.0306,  0.0117],\n",
       "         [-0.0282,  0.0100, -0.0151,  ...,  0.0226,  0.0054,  0.0092],\n",
       "         [-0.0238, -0.0400, -0.0146,  ..., -0.0385, -0.0544,  0.0605]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "einops.rearrange(all_submods[2].qkv.weight.data[-1024:,:], 'd_model (n_head d_head) ->n_head d_model d_head', n_head=all_submods[2].config.n_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vs = einops.rearrange(all_submods[2].qkv.weight.data[-1024:,:], '(n_head d_head) d_model -> n_head d_head d_model', n_head=all_submods[2].config.n_head) #v\n",
    "Os = einops.rearrange(all_submods[2].o.weight.data, 'd_model  (n_head d_head) -> n_head d_model d_head', n_head=all_submods[2].config.n_head) #o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import FactoredMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "OV = FactoredMatrix(Os,Vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V = OV.svd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.8220, 3.0563, 2.8632,  ..., 0.3376, 0.3148, 0.2436],\n",
       "        [4.3806, 2.4779, 2.4094,  ..., 0.3413, 0.2854, 0.2424],\n",
       "        [2.8725, 2.0972, 2.0136,  ..., 0.2061, 0.1790, 0.1253],\n",
       "        ...,\n",
       "        [2.1487, 1.9775, 1.7535,  ..., 0.2125, 0.1810, 0.1493],\n",
       "        [2.5054, 1.8842, 1.7610,  ..., 0.3061, 0.2746, 0.2129],\n",
       "        [6.4218, 4.4102, 4.0468,  ..., 0.2739, 0.2599, 0.2520]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1024, 64])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_vec =t.randn(1024, device=device)\n",
    "rand_vec = rand_vec / t.norm(rand_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vs = einops.rearrange(V, 'n_head d_model d_head -> (n_head d_head) d_model')\n",
    "res = einops.einsum(Vs, rand_vec, 'list_of_vs d_model, d_model -> list_of_vs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0785, device='cuda:0') tensor(-0.0905, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(t.max(res), t.min(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_sae_fet = einops.einsum(Vs, dictionaries[all_submods[2]].w_dec.weight.data[:,3514],'list_of_vs d_model, d_model -> list_of_vs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.3156, 0.2092, 0.1717, 0.1622, 0.1616, 0.1595, 0.1456, 0.1349, 0.1293,\n",
       "        0.1194, 0.1153, 0.1119, 0.1057, 0.1051, 0.1048, 0.1042, 0.1006, 0.0952,\n",
       "        0.0943, 0.0942, 0.0938, 0.0922, 0.0910, 0.0900, 0.0895, 0.0892, 0.0892,\n",
       "        0.0880, 0.0869, 0.0866], device='cuda:0'),\n",
       "indices=tensor([ 833,  769,  708,  902,  836,  766,  843,  712,  838, 1012,  857,  710,\n",
       "         960,  405,  899,   58,  891,  377,  856,  311,  303,   48,  380,  704,\n",
       "         302,   18,  742,  810,  597,  835], device='cuda:0'))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.topk(real_sae_fet, k = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.2275, 0.2246, 0.1811, 0.1663, 0.1552, 0.1537, 0.1494, 0.1480, 0.1474,\n",
       "        0.1423, 0.1422, 0.1404, 0.1373, 0.1342, 0.1305, 0.1284, 0.1205, 0.1186,\n",
       "        0.1168, 0.1126, 0.1116, 0.1112, 0.1102, 0.1086, 0.1070, 0.1048, 0.1046,\n",
       "        0.1041, 0.1003, 0.0980], device='cuda:0'),\n",
       "indices=tensor([ 997,  638,  711,   63,  707,  841,  706,  855, 1005,  998,  717,  759,\n",
       "         995,  321,  850,  746,  849,  714,  983,   37,  767,  848,  770,   40,\n",
       "         863,  892,  267,  771,  920,  737], device='cuda:0'))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.topk(-real_sae_fet, k = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.norm(dictionaries[all_submods[2]].w_dec.weight.data[:,3514])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 10])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(example.parameters())).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.9988, -1.4216,  2.6791,  2.5656,  2.3612, -4.1847, -2.0328, -3.6314,\n",
       "        -0.8202,  0.3459, -3.1903, -1.8296,  1.5329,  0.4042,  4.8260,  7.0481,\n",
       "        -2.3664, -1.9119,  0.7636, -0.9323])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.weight.data @ t.arange(10, dtype=t.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.9988, -1.4216,  2.6791,  2.5656,  2.3612, -4.1847, -2.0328, -3.6314,\n",
       "        -0.8202,  0.3459, -3.1903, -1.8296,  1.5329,  0.4042,  4.8260,  7.0481,\n",
       "        -2.3664, -1.9119,  0.7636, -0.9323], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example(t.arange(10, dtype=t.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(22), np.int64(3))"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unravel_index(223,(100,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1024, 64])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1024, 64])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from transformer_lens import FactoredMatrix\n",
    "import einops\n",
    "from typing import List, Tuple\n",
    "\n",
    "def get_top_n_svd_components(\n",
    "    attention_module: t.nn.Module,\n",
    "    input_vector: t.Tensor,\n",
    "    n: int = 10\n",
    ") -> List[Tuple[float, float, t.Tensor, t.Tensor]]:\n",
    "    \"\"\"\n",
    "    Computes the top n SVD components based on the dot product between the input vector and\n",
    "    the right singular vectors (V) of the attention module.\n",
    "\n",
    "    Args:\n",
    "        attention_module (t.nn.Module): The attention module (e.g., all_submods[2]).\n",
    "        input_vector (t.Tensor): A tensor of shape (d_model,) representing the input vector.\n",
    "        n (int, optional): Number of top components to return. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[float, float, t.Tensor, t.Tensor]]: A list of tuples containing:\n",
    "            - Dot product value (float)\n",
    "            - Singular value (float)\n",
    "            - Corresponding U vector (t.Tensor)\n",
    "            - Corresponding V vector (t.Tensor)\n",
    "    \"\"\"\n",
    "    # Ensure input_vector is normalized\n",
    "    input_vector = input_vector / input_vector.norm()\n",
    "\n",
    "    # Extract V from the attention module's qkv weights\n",
    "    # Assuming the last d_model rows correspond to V\n",
    "    qkv_weights = attention_module.qkv.weight.data  # Shape: (3*d_model, d_model)\n",
    "    d_model = qkv_weights.shape[1]\n",
    "    \n",
    "    # Extract V (assuming it is the last d_model rows)\n",
    "    V_weight = qkv_weights[-d_model:, :]  # Shape: (d_model, d_model)\n",
    "\n",
    "    # Reshape V for multi-head if necessary\n",
    "    n_head = attention_module.config.n_head\n",
    "\n",
    "    # Rearrange V to [n_head * d_head, d_model]\n",
    "    Vs = einops.rearrange(V_weight,  '(n_head d_head) d_model -> n_head d_head d_model', n_head=n_head)\n",
    "\n",
    "    # Extract O from the attention module's output weights\n",
    "    O_weight = attention_module.o.weight.data  # Shape: (d_model, d_model)\n",
    "    \n",
    "    # Rearrange O to [n_head, d_model, d_head]\n",
    "    Os = einops.rearrange(O_weight, 'd_model (n_head d_head) -> n_head d_model d_head',\n",
    "                         n_head=n_head)\n",
    "    \n",
    "    # Create FactoredMatrix and perform SVD\n",
    "    OV = FactoredMatrix(Os, Vs)\n",
    "    U, S, V = OV.svd()  # U and V are both [n_head, d_model, d_head, ...]\n",
    "    \n",
    "    # Compute dot products between input_vector and each V vector\n",
    "    # V should be of shape [n_head*d_head, d_model]\n",
    "    projections = einops.einsum(V, input_vector, 'n_head d_model d_head, d_model -> n_head d_head')\n",
    "    \n",
    "    # Get top n indices based on absolute dot product\n",
    "    _, top_indices = t.topk(projections.flatten().abs(), n)\n",
    "    \n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        loc = np.unravel_index(top_indices[i].cpu(), projections.shape)\n",
    "        dot_product = projections[loc].item()\n",
    "        singular_value = S[loc].item()\n",
    "        U_vector = U[loc[0],:,loc[1]]  # \n",
    "        V_vector = V[loc[0],:,loc[1]]  # \n",
    "        results.append((loc,dot_product, singular_value, U_vector, V_vector))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8274/348329627.py:64: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  loc = np.unravel_index(top_indices[i].cpu(), projections.shape)\n"
     ]
    }
   ],
   "source": [
    "top_3514_vecs = get_top_n_svd_components(all_submods[2], dictionaries[all_submods[2]].w_dec.weight.data[:,3514], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((np.int64(13), np.int64(1)),\n",
       "  0.31556427478790283,\n",
       "  1.9775331020355225,\n",
       "  tensor([-0.0297,  0.0364,  0.0232,  ..., -0.0123,  0.0067,  0.0146],\n",
       "         device='cuda:0'),\n",
       "  tensor([-0.0239, -0.0160,  0.0030,  ...,  0.0086,  0.0003, -0.0415],\n",
       "         device='cuda:0')),\n",
       " ((np.int64(15), np.int64(37)),\n",
       "  -0.22751836478710175,\n",
       "  0.6253384351730347,\n",
       "  tensor([ 0.0114, -0.0282,  0.0504,  ...,  0.0200,  0.0034, -0.0978],\n",
       "         device='cuda:0'),\n",
       "  tensor([-0.0381,  0.0093,  0.0063,  ..., -0.0460, -0.0253,  0.0135],\n",
       "         device='cuda:0')),\n",
       " ((np.int64(9), np.int64(62)),\n",
       "  -0.22455906867980957,\n",
       "  0.12267374247312546,\n",
       "  tensor([ 0.0738, -0.0117, -0.0450,  ..., -0.0199, -0.0330,  0.0257],\n",
       "         device='cuda:0'),\n",
       "  tensor([-0.0338, -0.0256,  0.0120,  ..., -0.0170,  0.0336,  0.0331],\n",
       "         device='cuda:0')),\n",
       " ((np.int64(12), np.int64(1)),\n",
       "  0.20916713774204254,\n",
       "  1.9326283931732178,\n",
       "  tensor([ 0.0074, -0.0290, -0.0082,  ..., -0.0527,  0.0395,  0.0433],\n",
       "         device='cuda:0'),\n",
       "  tensor([ 0.0110,  0.0192,  0.0234,  ..., -0.0316, -0.0197, -0.0308],\n",
       "         device='cuda:0')),\n",
       " ((np.int64(11), np.int64(7)),\n",
       "  -0.1811290830373764,\n",
       "  1.3664965629577637,\n",
       "  tensor([ 0.0079,  0.0452,  0.0077,  ..., -0.0141, -0.0316, -0.0121],\n",
       "         device='cuda:0'),\n",
       "  tensor([-0.0211, -0.0107,  0.0437,  ..., -0.0330,  0.0869,  0.0202],\n",
       "         device='cuda:0')),\n",
       " ((np.int64(11), np.int64(4)),\n",
       "  0.17170855402946472,\n",
       "  1.4803467988967896,\n",
       "  tensor([-0.0180, -0.0032,  0.0231,  ..., -0.0429,  0.0795,  0.0882],\n",
       "         device='cuda:0'),\n",
       "  tensor([-0.0142, -0.0092, -0.0669,  ...,  0.0039, -0.0543, -0.0313],\n",
       "         device='cuda:0')),\n",
       " ((np.int64(0), np.int64(63)),\n",
       "  -0.16628514230251312,\n",
       "  0.24360191822052002,\n",
       "  tensor([-0.0339, -0.0130, -0.0087,  ..., -0.0105,  0.0166, -0.0528],\n",
       "         device='cuda:0'),\n",
       "  tensor([-0.0085,  0.0071,  0.0781,  ...,  0.0147, -0.0308,  0.0014],\n",
       "         device='cuda:0')),\n",
       " ((np.int64(14), np.int64(6)),\n",
       "  0.16217821836471558,\n",
       "  1.55863356590271,\n",
       "  tensor([-0.0046,  0.0279, -0.0117,  ...,  0.0203, -0.0010,  0.0250],\n",
       "         device='cuda:0'),\n",
       "  tensor([ 0.0015, -0.0211,  0.0422,  ..., -0.0713, -0.0364, -0.0111],\n",
       "         device='cuda:0')),\n",
       " ((np.int64(13), np.int64(4)),\n",
       "  0.16156435012817383,\n",
       "  1.5794531106948853,\n",
       "  tensor([ 0.0128, -0.0003,  0.0387,  ...,  0.0100, -0.0248,  0.0259],\n",
       "         device='cuda:0'),\n",
       "  tensor([ 0.0006, -0.0097,  0.0121,  ..., -0.0161,  0.0120, -0.0451],\n",
       "         device='cuda:0')),\n",
       " ((np.int64(11), np.int64(62)),\n",
       "  0.1594909131526947,\n",
       "  0.33485209941864014,\n",
       "  tensor([ 0.0173,  0.0464, -0.0494,  ..., -0.0018,  0.0211,  0.0297],\n",
       "         device='cuda:0'),\n",
       "  tensor([-0.0158, -0.0363, -0.0058,  ..., -0.0092,  0.0144, -0.0309],\n",
       "         device='cuda:0'))]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_3514_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_no_head = all_submods[2].qkv.weight.data[-1024:,:]\n",
    "O_no_head = all_submods[2].o.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_token_embd = all_submods[0](t.tensor([330]).to(device = 'cuda'))\n",
    "is_token_embd = all_submods[0](t.tensor([349]).to(device = 'cuda'))\n",
    "designed_token_embd = all_submods[0](t.tensor([5682]).to(device = 'cuda'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0238], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.nn.functional.cosine_similarity(model._model.lm_head.weight.data[298], designed_token_embd + is_token_embd, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_13_out_no_bias = (designed_token_embd + is_token_embd) @ model._model.ov[0,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1024])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U1, S1, V1 = t.linalg.svd(model._model.ov[0,13])\n",
    "V1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.0001, device='cuda:0', grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V1[1] @ top_3514_vecs[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "own_tok = designed_token_embd @ model._model.ov[0,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1383], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.nn.functional.cosine_similarity(own_tok, top_3514_vecs[0][4], dim = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1244], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.nn.functional.cosine_similarity(own_tok, dictionaries[all_submods[2]].w_dec.weight.data[:,3514], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.1116, device='cuda:0', grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "obv = all_submods[2].o.weight.data @ all_submods[2].qkv.bias[-1024:]\n",
    "print(t.nn.functional.cosine_similarity(obv, dictionaries[all_submods[2]].w_dec.weight.data[:,3514], dim = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_submods[2].o.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0608], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.nn.functional.cosine_similarity(dictionaries[all_submods[2]].w_dec.weight.data[:,3514], head_13_out_no_bias, dim = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0147, device='cuda:0')"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.nn.functional.cosine_similarity(model._model.lm_head.weight.data[298], dictionaries[all_submods[2]].w_dec.weight.data[:,3514], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1617e-02,  3.7645e-03, -6.8461e-03,  ...,  6.7766e-03,\n",
       "           6.5290e-03, -7.2235e-03],\n",
       "         [-1.2089e-02, -9.5461e-03, -1.0197e-02,  ..., -2.8483e-04,\n",
       "           1.2634e-02,  9.2898e-03],\n",
       "         [ 1.7853e-02, -6.6746e-03, -1.3770e-02,  ...,  4.4956e-04,\n",
       "          -7.9831e-03, -4.6051e-03],\n",
       "         ...,\n",
       "         [ 3.2585e-03, -7.6406e-03,  6.0812e-03,  ..., -8.3066e-03,\n",
       "          -1.0763e-02, -3.5473e-03],\n",
       "         [ 7.3684e-04,  1.6678e-03,  1.0509e-02,  ..., -1.9866e-02,\n",
       "          -1.3328e-02, -2.2345e-03],\n",
       "         [-4.0602e-03, -1.6663e-02,  1.2854e-03,  ...,  1.4990e-02,\n",
       "          -8.2366e-03, -2.1329e-02]],\n",
       "\n",
       "        [[-3.4120e-03, -8.5110e-03, -5.3302e-04,  ...,  1.0945e-02,\n",
       "           3.9195e-03,  1.0821e-03],\n",
       "         [ 1.4866e-02,  1.6028e-03, -8.7989e-03,  ...,  4.8909e-03,\n",
       "           3.3940e-03, -8.3213e-04],\n",
       "         [-1.8998e-02, -1.0391e-02, -2.3331e-02,  ..., -1.9615e-04,\n",
       "          -1.3560e-02, -6.5725e-03],\n",
       "         ...,\n",
       "         [ 4.8359e-03,  8.9021e-03,  7.6877e-03,  ..., -7.5591e-03,\n",
       "           1.4414e-02,  1.6641e-03],\n",
       "         [ 1.1414e-02, -7.0085e-03, -6.6691e-03,  ...,  2.5924e-03,\n",
       "          -1.0427e-02,  3.0289e-03],\n",
       "         [ 8.5800e-03, -8.0121e-03, -1.3753e-02,  ..., -5.5489e-03,\n",
       "          -2.3106e-03, -8.5035e-03]],\n",
       "\n",
       "        [[-3.2823e-03, -5.5431e-03,  1.6200e-02,  ...,  1.5891e-02,\n",
       "           7.3397e-03, -3.8913e-03],\n",
       "         [-8.7204e-03, -1.6591e-02,  1.1115e-02,  ...,  1.2157e-02,\n",
       "           6.4255e-03,  6.4318e-03],\n",
       "         [-4.3217e-03,  3.6496e-03, -4.0480e-03,  ...,  1.9644e-03,\n",
       "           9.3869e-03, -8.5786e-03],\n",
       "         ...,\n",
       "         [ 5.3544e-03, -3.1283e-03, -2.0391e-02,  ..., -1.5641e-02,\n",
       "          -1.5501e-02,  3.4033e-03],\n",
       "         [ 6.4794e-04, -4.8764e-03,  3.1265e-03,  ..., -9.8947e-05,\n",
       "           9.3888e-03, -1.8955e-03],\n",
       "         [ 2.4779e-03, -1.8223e-03, -2.3798e-02,  ..., -8.3701e-03,\n",
       "          -9.6202e-03, -2.3497e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-9.1830e-03,  3.2484e-03,  8.0628e-03,  ...,  1.4376e-02,\n",
       "          -7.3339e-03,  9.9193e-03],\n",
       "         [ 1.2056e-03, -2.8821e-02, -4.7728e-03,  ...,  3.0911e-03,\n",
       "          -6.2993e-03, -1.5359e-02],\n",
       "         [ 9.0184e-03, -8.9734e-03, -1.3495e-02,  ...,  2.2421e-03,\n",
       "          -1.1413e-02, -1.2171e-02],\n",
       "         ...,\n",
       "         [-7.3874e-03,  1.0472e-04, -1.5586e-03,  ..., -3.3922e-02,\n",
       "           9.9958e-03,  2.6672e-03],\n",
       "         [ 4.0845e-03, -5.9526e-03,  3.3431e-03,  ...,  8.4101e-03,\n",
       "          -3.1346e-02,  5.2638e-04],\n",
       "         [ 6.6299e-03,  1.5593e-03, -6.4925e-03,  ..., -1.1673e-02,\n",
       "           4.3409e-03, -1.5415e-02]],\n",
       "\n",
       "        [[-4.2675e-02, -8.3997e-03, -4.7783e-03,  ...,  9.7603e-03,\n",
       "           6.9651e-03, -1.4526e-03],\n",
       "         [ 1.9314e-03, -1.1870e-02,  4.2743e-03,  ..., -7.0918e-03,\n",
       "          -2.0295e-03,  2.0571e-03],\n",
       "         [-1.1618e-03, -1.5857e-03, -3.2376e-02,  ..., -1.3872e-02,\n",
       "           4.8013e-03, -3.3836e-03],\n",
       "         ...,\n",
       "         [ 1.8716e-03,  5.7399e-03, -1.1793e-02,  ..., -3.4137e-02,\n",
       "           5.1751e-03, -7.8884e-03],\n",
       "         [ 6.9068e-03,  5.0687e-03,  5.5698e-03,  ...,  6.2792e-03,\n",
       "          -4.3533e-02,  8.1005e-03],\n",
       "         [ 1.4715e-03, -2.7180e-03, -1.5089e-03,  ..., -3.3455e-03,\n",
       "           8.1107e-03, -1.8969e-02]],\n",
       "\n",
       "        [[-1.0203e-02, -7.0510e-03,  3.0106e-04,  ..., -6.9651e-03,\n",
       "           3.0585e-03, -6.9779e-03],\n",
       "         [ 7.5654e-03, -1.9541e-02,  2.2546e-02,  ..., -4.2427e-03,\n",
       "          -1.0383e-02, -3.1923e-03],\n",
       "         [ 1.6660e-02,  1.3437e-03,  1.6437e-02,  ...,  8.4477e-03,\n",
       "           6.0280e-03, -3.4513e-03],\n",
       "         ...,\n",
       "         [-1.0970e-03, -1.1163e-02,  7.4984e-03,  ...,  6.8117e-03,\n",
       "           1.3140e-02, -2.0897e-02],\n",
       "         [ 4.8548e-04, -2.3591e-03,  1.8414e-02,  ..., -9.3146e-04,\n",
       "           1.3957e-02,  9.4700e-03],\n",
       "         [-7.7829e-03, -8.1239e-03, -2.0344e-03,  ...,  1.2502e-03,\n",
       "           1.7750e-02, -1.6173e-02]]], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._model.ov[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv_weights = all_submods[2].qkv.weight.data  # Shape: (3*d_model, d_model)\n",
    "d_model = qkv_weights.shape[1]\n",
    "\n",
    "# Extract V (assuming it is the last d_model rows)\n",
    "V_weight = qkv_weights[-d_model:, :]  # Shape: (d_model, d_model)\n",
    "\n",
    "# Reshape V for multi-head if necessary\n",
    "n_head = all_submods[2].config.n_head\n",
    "\n",
    "# Rearrange V to [n_head * d_head, d_model]\n",
    "Vs = einops.rearrange(V_weight,  '(n_head d_head) d_model -> n_head d_head d_model', n_head=n_head)\n",
    "\n",
    "# Extract O from the attention module's output weights\n",
    "O_weight = all_submods[2].o.weight.data  # Shape: (d_model, d_model)\n",
    "\n",
    "# Rearrange O to [n_head, d_model, d_head]\n",
    "Os = einops.rearrange(O_weight, 'd_model (n_head d_head) -> n_head d_model d_head',\n",
    "                        n_head=n_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0857, device='cuda:0')"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "up_projection = t.transpose(V_no_head @ O_no_head, 1,0) @ dictionaries[all_submods[2]].w_dec.weight.data[:,3514]\n",
    "t.norm(up_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4806, device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.nn.functional.cosine_similarity(attn_out[0,-1], dictionaries[all_submods[2]].w_dec.weight.data[:,3514], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0814, device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.nn.functional.cosine_similarity(attn_out[0,-1], top_3514_vecs[3][4], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index (np.int64(13), np.int64(1)) with singular value 1.977533 has cosine similarity 0.192065 with the attn output and a dot product of 1.880131\n",
      "Index (np.int64(15), np.int64(37)) with singular value 0.625338 has cosine similarity -0.055736 with the attn output and a dot product of -0.545597\n",
      "Index (np.int64(9), np.int64(62)) with singular value 0.122674 has cosine similarity -0.087071 with the attn output and a dot product of -0.852341\n",
      "Index (np.int64(12), np.int64(1)) with singular value 1.932628 has cosine similarity 0.081432 with the attn output and a dot product of 0.797134\n",
      "Index (np.int64(11), np.int64(7)) with singular value 1.366497 has cosine similarity -0.133414 with the attn output and a dot product of -1.305994\n",
      "Index (np.int64(11), np.int64(4)) with singular value 1.480347 has cosine similarity 0.089717 with the attn output and a dot product of 0.878239\n",
      "Index (np.int64(0), np.int64(63)) with singular value 0.243602 has cosine similarity -0.103852 with the attn output and a dot product of -1.016607\n",
      "Index (np.int64(14), np.int64(6)) with singular value 1.558634 has cosine similarity 0.065416 with the attn output and a dot product of 0.640357\n",
      "Index (np.int64(13), np.int64(4)) with singular value 1.579453 has cosine similarity 0.073071 with the attn output and a dot product of 0.715288\n",
      "Index (np.int64(11), np.int64(62)) with singular value 0.334852 has cosine similarity 0.101473 with the attn output and a dot product of 0.993323\n"
     ]
    }
   ],
   "source": [
    "for i in top_3514_vecs:\n",
    "    print(f\"Index {i[0]} with singular value {i[2]:4f} has cosine similarity {t.nn.functional.cosine_similarity(attn_out[0,-1], i[4], dim = -1):4f} with the attn output and a dot product of {einops.einsum(attn_out[0,-1], i[4], 'd_model, d_model -> '):4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_vec = t.randn(1024, device=device)\n",
    "rand_vec = rand_vec / t.norm(rand_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index (np.int64(13), np.int64(1)) with singular value 1.9775331020355225 has cosine similarity -0.03936593234539032 with the random vector\n",
      "Index (np.int64(15), np.int64(37)) with singular value 0.6253384351730347 has cosine similarity -0.008046098053455353 with the random vector\n",
      "Index (np.int64(9), np.int64(62)) with singular value 0.12267374247312546 has cosine similarity -0.021047594025731087 with the random vector\n",
      "Index (np.int64(12), np.int64(1)) with singular value 1.9326283931732178 has cosine similarity -0.0118798753246665 with the random vector\n",
      "Index (np.int64(11), np.int64(7)) with singular value 1.3664965629577637 has cosine similarity -0.03248085826635361 with the random vector\n",
      "Index (np.int64(11), np.int64(4)) with singular value 1.4803467988967896 has cosine similarity -0.04305107146501541 with the random vector\n",
      "Index (np.int64(0), np.int64(63)) with singular value 0.24360191822052002 has cosine similarity -0.0019873883575201035 with the random vector\n",
      "Index (np.int64(14), np.int64(6)) with singular value 1.55863356590271 has cosine similarity -0.04362739622592926 with the random vector\n",
      "Index (np.int64(13), np.int64(4)) with singular value 1.5794531106948853 has cosine similarity -0.024669788777828217 with the random vector\n",
      "Index (np.int64(11), np.int64(62)) with singular value 0.33485209941864014 has cosine similarity -0.03624114394187927 with the random vector\n"
     ]
    }
   ],
   "source": [
    "for i in top_3514_vecs:\n",
    "    print(f\"Index {i[0]} with singular value {i[2]} has cosine similarity {t.nn.functional.cosine_similarity(rand_vec, i[4], dim = -1)} with the random vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.7890, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.norm(attn_out[0,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0297,  0.0364,  0.0232,  ..., -0.0123,  0.0067,  0.0146],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looks like heads 13 and 12 are important since there's high dot product and it's second highest singular value. \n",
    "#Let's look at the U vector and see which space it reads in from\n",
    "all_submods[0] top_3514_vecs[0][3] #head 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10th percentile: 120.77\n",
      "25th percentile: 132.34\n",
      "50th percentile: 141.55\n",
      "75th percentile: 148.77\n",
      "90th percentile: 155.27\n",
      "99th percentile: 167.96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([144.7780,  78.3940, 140.8091,  ..., 147.2960, 137.4702, 143.4475],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_products = einops.einsum(all_submods[0].weight, all_submods[0].weight, 'd_vocab d_model, d_vocab d_model -> d_vocab')\n",
    "percentiles = t.tensor([\n",
    "    t.quantile(dot_products, 0.1),\n",
    "    t.quantile(dot_products, 0.25), \n",
    "    t.quantile(dot_products, 0.5),\n",
    "    t.quantile(dot_products, 0.75),\n",
    "    t.quantile(dot_products, 0.9),\n",
    "    t.quantile(dot_products, 0.99)\n",
    "])\n",
    "print(f\"10th percentile: {percentiles[0]:.2f}\")\n",
    "print(f\"25th percentile: {percentiles[1]:.2f}\")\n",
    "print(f\"50th percentile: {percentiles[2]:.2f}\")\n",
    "print(f\"75th percentile: {percentiles[3]:.2f}\") \n",
    "print(f\"90th percentile: {percentiles[4]:.2f}\")\n",
    "print(f\"99th percentile: {percentiles[5]:.2f}\")\n",
    "dot_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index\tValue\tToken\n",
      "----------------------------------------\n",
      "1\t2.0462 <s>\n",
      "7914\t1.6647 flag\n",
      "11094\t1.5519 ulator\n",
      "28702\t1.4964 FIFA\n",
      "25771\t1.4960 mania\n",
      "25811\t1.4917 ský\n",
      "15715\t1.4910 compare\n",
      "3393\t1.4857 Pr\n",
      "25712\t1.4655 inclu\n",
      "20877\t1.4581 volta\n",
      "21050\t1.4356 Malays\n",
      "23548\t1.4201 miner\n",
      "21017\t1.4116 hö\n",
      "17837\t1.4045 grim\n",
      "18975\t1.3905 projection\n",
      "5333\t1.3812 сле\n",
      "30276\t1.3704 父\n",
      "31136\t1.3326 佛\n",
      "8143\t1.3181 arse\n",
      "19675\t1.3056 Publish\n"
     ]
    }
   ],
   "source": [
    "vocab_dot_products_13 = einops.einsum(all_submods[0].weight, up_projection, 'd_vocab d_model, d_model -> d_vocab')\n",
    "\n",
    "# Decode the indices and print in a table\n",
    "token_indices = t.topk(vocab_dot_products_13, k=20).indices\n",
    "token_values = t.topk(vocab_dot_products_13, k=20).values\n",
    "\n",
    "# Create a table with token indices and their corresponding values\n",
    "print(\"Index\\tValue\\tToken\")\n",
    "print(\"-\" * 40)\n",
    "for idx, val in zip(token_indices, token_values):\n",
    "    print(f\"{idx.item()}\\t{val.item():.4f} {model.tokenizer.decode([idx.item()])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8274/348329627.py:64: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  loc = np.unravel_index(top_indices[i].cpu(), projections.shape)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((np.int64(3), np.int64(0)),\n",
       "  0.08837911486625671,\n",
       "  3.5056588649749756,\n",
       "  tensor([ 0.0187,  0.0529,  0.0045,  ...,  0.0666, -0.0359,  0.0732],\n",
       "         device='cuda:0'),\n",
       "  tensor([ 0.0022, -0.0251,  0.0245,  ..., -0.0024, -0.0252,  0.0026],\n",
       "         device='cuda:0')),\n",
       " ((np.int64(6), np.int64(54)),\n",
       "  -0.08759850263595581,\n",
       "  0.28654077649116516,\n",
       "  tensor([-0.0402,  0.0298,  0.0779,  ...,  0.0210,  0.0125, -0.0350],\n",
       "         device='cuda:0'),\n",
       "  tensor([ 0.0266,  0.0147, -0.0592,  ...,  0.0133, -0.0505,  0.0121],\n",
       "         device='cuda:0')),\n",
       " ((np.int64(12), np.int64(0)),\n",
       "  -0.08639997243881226,\n",
       "  2.1591038703918457,\n",
       "  tensor([ 0.0039, -0.0041,  0.0229,  ...,  0.0118,  0.0566,  0.0127],\n",
       "         device='cuda:0'),\n",
       "  tensor([-0.0088,  0.0123, -0.0136,  ..., -0.0059, -0.0344, -0.0054],\n",
       "         device='cuda:0')),\n",
       " ((np.int64(4), np.int64(5)),\n",
       "  -0.08568445593118668,\n",
       "  1.6612879037857056,\n",
       "  tensor([-0.0394, -0.0330, -0.0050,  ..., -0.0161,  0.0721,  0.0026],\n",
       "         device='cuda:0'),\n",
       "  tensor([ 0.0232,  0.0160,  0.0309,  ..., -0.0222, -0.0499, -0.0155],\n",
       "         device='cuda:0')),\n",
       " ((np.int64(6), np.int64(31)),\n",
       "  0.08021116256713867,\n",
       "  0.5854923129081726,\n",
       "  tensor([-0.0397, -0.0189,  0.0003,  ...,  0.0259,  0.0291, -0.0121],\n",
       "         device='cuda:0'),\n",
       "  tensor([ 0.0377,  0.0065,  0.0591,  ..., -0.0327,  0.0246,  0.0084],\n",
       "         device='cuda:0')),\n",
       " ((np.int64(2), np.int64(6)),\n",
       "  0.07900024205446243,\n",
       "  1.5619815587997437,\n",
       "  tensor([-0.0167, -0.0601,  0.0053,  ...,  0.0215,  0.0478,  0.0442],\n",
       "         device='cuda:0'),\n",
       "  tensor([ 0.0261,  0.0065, -0.0505,  ..., -0.0174, -0.0458, -0.0198],\n",
       "         device='cuda:0')),\n",
       " ((np.int64(14), np.int64(27)),\n",
       "  0.07871633023023605,\n",
       "  0.8340692520141602,\n",
       "  tensor([ 0.0774, -0.0029, -0.0209,  ...,  0.0076, -0.0039, -0.0528],\n",
       "         device='cuda:0'),\n",
       "  tensor([-0.0809, -0.0147,  0.0594,  ...,  0.0353, -0.0052,  0.0151],\n",
       "         device='cuda:0')),\n",
       " ((np.int64(3), np.int64(48)),\n",
       "  0.07830995321273804,\n",
       "  0.2834950387477875,\n",
       "  tensor([-0.0175,  0.0820,  0.0555,  ..., -0.0153, -0.0350, -0.0141],\n",
       "         device='cuda:0'),\n",
       "  tensor([ 0.0387,  0.0311, -0.0633,  ...,  0.0982, -0.0506,  0.0094],\n",
       "         device='cuda:0')),\n",
       " ((np.int64(10), np.int64(9)),\n",
       "  -0.07764284312725067,\n",
       "  1.4514942169189453,\n",
       "  tensor([-0.0060,  0.0027,  0.0248,  ..., -0.0742, -0.0569, -0.0463],\n",
       "         device='cuda:0'),\n",
       "  tensor([-0.0352, -0.0153, -0.0125,  ...,  0.0090,  0.0843, -0.0151],\n",
       "         device='cuda:0')),\n",
       " ((np.int64(14), np.int64(1)),\n",
       "  0.07651988416910172,\n",
       "  1.8842345476150513,\n",
       "  tensor([ 0.0068,  0.0547, -0.0023,  ...,  0.0219, -0.0187, -0.0035],\n",
       "         device='cuda:0'),\n",
       "  tensor([-0.0164,  0.0151,  0.0429,  ..., -0.0039, -0.0147,  0.0134],\n",
       "         device='cuda:0'))]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random vector svd\n",
    "rand_vec = t.randn(1024, device=device)\n",
    "rand_vec = rand_vec / t.norm(rand_vec)\n",
    "top_rand_vec_vecs = get_top_n_svd_components(all_submods[2], rand_vec, 10)\n",
    "top_rand_vec_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cuda.empty_cache()\n",
    "\n",
    "\n",
    "clean_inputs = t.cat([e['clean_prefix'] for e in batches[0]], dim=0).to(device)\n",
    "clean_answer_idxs = t.tensor([e['clean_answer'] for e in batches[0]], dtype=t.long, device=device)\n",
    "\n",
    "patch_inputs = t.cat([e['patch_prefix'] for e in batches[0]], dim=0).to(device)\n",
    "patch_answer_idxs = t.tensor([e['patch_answer'] for e in batches[0]], dtype=t.long, device=device)\n",
    "def metric_fn(model):\n",
    "    return (\n",
    "        t.gather(model.lm_head.output[:,-1,:], dim=-1, index=patch_answer_idxs.view(-1, 1)).squeeze(-1) - \\\n",
    "        t.gather(model.lm_head.output[:,-1,:], dim=-1, index=clean_answer_idxs.view(-1, 1)).squeeze(-1)\n",
    "    ) #We're only looking at the logit difference between two answers which is a very limited subset of the model's behavior aya. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   415,  4531],\n",
       "        [    1,   415,  3282],\n",
       "        [    1,   415,  4649],\n",
       "        [    1,   415, 13500]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,  415, 8066],\n",
       "        [   1,  415, 1832],\n",
       "        [   1,  415, 6246],\n",
       "        [   1,  415, 6676]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Integrated Gradient estimation\n",
      "\n",
      "\n",
      "Initial trace\n",
      "\n",
      "\n",
      "Patching part\n",
      "\n",
      "CPU times: user 16.3 s, sys: 424 ms, total: 16.8 s\n",
      "Wall time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "effects, deltas, grads, total_effect = patching_effect(\n",
    "        clean_inputs,\n",
    "        patch_inputs,\n",
    "        model,\n",
    "        all_submods,\n",
    "        dictionaries,\n",
    "        metric_fn,\n",
    "        metric_kwargs=dict(),\n",
    "        method='ig' # get better approximations for early layers by using ig\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unflatten(tensor): # will break if dictionaries vary in size between layers\n",
    "        b, s, f = effects[resids[0]].act.shape\n",
    "        unflattened = rearrange(tensor, '(b s x) -> b s x', b=b, s=s)\n",
    "        return SparseAct(act=unflattened[...,:f], res=unflattened[...,f:])\n",
    "    \n",
    "features_by_submod = {\n",
    "    submod : (effects[submod].to_tensor().flatten().abs() > 0.1).nonzero().flatten().tolist() for submod in all_submods\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effects[all_submods[0]].act.nonzero().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_by_submod[all_submods[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([Embedding(32000, 1024), MLP(\n",
       "  (w): Bilinear(\n",
       "    in_features=1024, out_features=8192, bias=True\n",
       "    (gate): Identity()\n",
       "  )\n",
       "  (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "), Attention(\n",
       "  (rotary): Rotary()\n",
       "  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "  (softmax): Softmax(dim=-1)\n",
       "), Layer(\n",
       "  (attn): Attention(\n",
       "    (rotary): Rotary()\n",
       "    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "    (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (w): Bilinear(\n",
       "      in_features=1024, out_features=8192, bias=True\n",
       "      (gate): Identity()\n",
       "    )\n",
       "    (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  )\n",
       "  (n1): Norm(\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (n2): Norm(\n",
       "    (norm): Identity()\n",
       "  )\n",
       "), MLP(\n",
       "  (w): Bilinear(\n",
       "    in_features=1024, out_features=8192, bias=True\n",
       "    (gate): Identity()\n",
       "  )\n",
       "  (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "), Attention(\n",
       "  (rotary): Rotary()\n",
       "  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "  (softmax): Softmax(dim=-1)\n",
       "), Layer(\n",
       "  (attn): Attention(\n",
       "    (rotary): Rotary()\n",
       "    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "    (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (w): Bilinear(\n",
       "      in_features=1024, out_features=8192, bias=True\n",
       "      (gate): Identity()\n",
       "    )\n",
       "    (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  )\n",
       "  (n1): Norm(\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (n2): Norm(\n",
       "    (norm): Identity()\n",
       "  )\n",
       "), MLP(\n",
       "  (w): Bilinear(\n",
       "    in_features=1024, out_features=8192, bias=True\n",
       "    (gate): Identity()\n",
       "  )\n",
       "  (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "), Attention(\n",
       "  (rotary): Rotary()\n",
       "  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "  (softmax): Softmax(dim=-1)\n",
       "), Layer(\n",
       "  (attn): Attention(\n",
       "    (rotary): Rotary()\n",
       "    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "    (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (w): Bilinear(\n",
       "      in_features=1024, out_features=8192, bias=True\n",
       "      (gate): Identity()\n",
       "    )\n",
       "    (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  )\n",
       "  (n1): Norm(\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (n2): Norm(\n",
       "    (norm): Identity()\n",
       "  )\n",
       "), MLP(\n",
       "  (w): Bilinear(\n",
       "    in_features=1024, out_features=8192, bias=True\n",
       "    (gate): Identity()\n",
       "  )\n",
       "  (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "), Attention(\n",
       "  (rotary): Rotary()\n",
       "  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "  (softmax): Softmax(dim=-1)\n",
       "), Layer(\n",
       "  (attn): Attention(\n",
       "    (rotary): Rotary()\n",
       "    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "    (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (w): Bilinear(\n",
       "      in_features=1024, out_features=8192, bias=True\n",
       "      (gate): Identity()\n",
       "    )\n",
       "    (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  )\n",
       "  (n1): Norm(\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (n2): Norm(\n",
       "    (norm): Identity()\n",
       "  )\n",
       ")])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effects.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(effects[all_submods[9]].act, 'ef9.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(grads[all_submods[9]].act,'ef9.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boing = grads[all_submods[9]] @ deltas[all_submods[9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000],\n",
       "         [ 0.0000],\n",
       "         [-1.3478]],\n",
       "\n",
       "        [[ 0.0000],\n",
       "         [ 0.0000],\n",
       "         [ 1.4131]],\n",
       "\n",
       "        [[ 0.0000],\n",
       "         [ 0.0000],\n",
       "         [-0.0593]],\n",
       "\n",
       "        [[ 0.0000],\n",
       "         [ 0.0000],\n",
       "         [-0.3202]]], device='cuda:0')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boing.resc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "boing2 = grads[all_submods[9]].__matmul__(deltas[all_submods[9]])\n",
    "boing2.res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.min(boing2.act == boing.act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 8192])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads[all_submods[9]].act.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 8192])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltas[all_submods[9]].act.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 8192])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boing.act.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.7520,  6.3497,  9.2779, 10.3250], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effects.keys().__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 8192])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effects[all_submods[9]].act.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000],\n",
       "         [ 0.0000],\n",
       "         [-0.6774]],\n",
       "\n",
       "        [[ 0.0000],\n",
       "         [ 0.0000],\n",
       "         [-0.1120]],\n",
       "\n",
       "        [[ 0.0000],\n",
       "         [ 0.0000],\n",
       "         [-0.0361]],\n",
       "\n",
       "        [[ 0.0000],\n",
       "         [ 0.0000],\n",
       "         [-0.1439]]], device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effects[all_submods[11]].resc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 8192])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effects[all_submods[10]].act.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"tdooms/fw-nano\",\n",
       "  \"architectures\": [\n",
       "    \"Transformer\"\n",
       "  ],\n",
       "  \"attention2\": false,\n",
       "  \"bias\": true,\n",
       "  \"bilinear\": true,\n",
       "  \"d_hidden\": 4096,\n",
       "  \"d_model\": 1024,\n",
       "  \"gate\": null,\n",
       "  \"n_ctx\": 512,\n",
       "  \"n_head\": 16,\n",
       "  \"n_layer\": 4,\n",
       "  \"normalization\": false,\n",
       "  \"repo\": \"tdooms/fw-nano\",\n",
       "  \"scale_attn\": true,\n",
       "  \"tokenizer\": \"mistral\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.47.1\"\n",
       "}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27493/3991189834.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cir = t.load('/root/bilinear-feature-circuits/circuits/simple_train_dict10_node0.2_edge0.02_n20_aggsum.pt')\n"
     ]
    }
   ],
   "source": [
    "cir = t.load('/root/bilinear-feature-circuits/circuits/simple_train_dict10_node0.2_edge0.02_n20_aggsum.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embed': SparseAct(act=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), resc=tensor([1.2417], device='cuda:0')),\n",
       " 'attn_0': SparseAct(act=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), resc=tensor([-0.2630], device='cuda:0')),\n",
       " 'mlp_0': SparseAct(act=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), resc=tensor([0.6300], device='cuda:0')),\n",
       " 'resid_0': SparseAct(act=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), resc=tensor([0.5510], device='cuda:0')),\n",
       " 'attn_1': SparseAct(act=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), resc=tensor([-0.1944], device='cuda:0')),\n",
       " 'mlp_1': SparseAct(act=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), resc=tensor([0.7722], device='cuda:0')),\n",
       " 'resid_1': SparseAct(act=tensor([0.0035, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000], device='cuda:0'), resc=tensor([0.4621], device='cuda:0')),\n",
       " 'attn_2': SparseAct(act=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), resc=tensor([0.1127], device='cuda:0')),\n",
       " 'mlp_2': SparseAct(act=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), resc=tensor([0.2283], device='cuda:0')),\n",
       " 'resid_2': SparseAct(act=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), resc=tensor([-0.1592], device='cuda:0')),\n",
       " 'attn_3': SparseAct(act=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), resc=tensor([-0.0987], device='cuda:0')),\n",
       " 'mlp_3': SparseAct(act=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), resc=tensor([-0.1334], device='cuda:0')),\n",
       " 'resid_3': SparseAct(act=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), resc=tensor([0.2452], device='cuda:0'))}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cir['nodes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'resid_3': {'y': tensor(indices=tensor([[  40,   54,   74,   94,  154,  197,  211,  323,  398,\n",
       "                           441,  475,  494,  502,  514,  615,  623,  677,  735,\n",
       "                           792,  839,  866,  891,  931,  987, 1010, 1020, 1062,\n",
       "                          1108, 1121, 1138, 1162, 1168, 1171, 1191, 1203, 1224,\n",
       "                          1289, 1387, 1460, 1483, 1488, 1507, 1512, 1514, 1545,\n",
       "                          1557, 1581, 1588, 1629, 1635, 1647, 1657, 1695, 1727,\n",
       "                          1786, 1812, 1831, 1842, 1849, 1886, 1909, 1944, 1949,\n",
       "                          1963, 1975, 2132, 2160, 2192, 2195, 2211, 2215, 2227,\n",
       "                          2286, 2342, 2373, 2378, 2480, 2489, 2506, 2565, 2611,\n",
       "                          2616, 2640, 2727, 2730, 2803, 2821, 2984, 3000, 3034,\n",
       "                          3193, 3204, 3211, 3241, 3272, 3281, 3299, 3306, 3307,\n",
       "                          3317, 3409, 3512, 3533, 3589, 3642, 3723, 3745, 3821,\n",
       "                          3846, 3975, 4008, 4026, 4042, 4057, 4105, 4110, 4122,\n",
       "                          4140, 4164, 4166, 4195, 4202, 4254, 4257, 4260, 4274,\n",
       "                          4307, 4359, 4369, 4381, 4423, 4481, 4563, 4656, 4674,\n",
       "                          4789, 4799, 4899, 4908, 4916, 4957, 4969, 5213, 5238,\n",
       "                          5310, 5335, 5416, 5451, 5527, 5546, 5554, 5578, 5742,\n",
       "                          5817, 5823, 5832, 5885, 5893, 6087, 6093, 6125, 6368,\n",
       "                          6371, 6428, 6506, 6520, 6540, 6608, 6627, 6684, 6765,\n",
       "                          6828, 6842, 6882, 6919, 6951, 6956, 6984, 7103, 7115,\n",
       "                          7170, 7188, 7223, 7256, 7290, 7394, 7452, 7584, 7614,\n",
       "                          7621, 7623, 7633, 7637, 7640, 7750, 7902, 7987, 8004,\n",
       "                          8008, 8015, 8035, 8071, 8139, 8162, 8192]]),\n",
       "         values=tensor([ 6.5203e-03,  7.2704e-04, -5.2946e-03,  7.7789e-03,\n",
       "                         1.0591e-03,  1.5328e-02,  3.3372e-02,  1.5469e-02,\n",
       "                         6.9091e-02, -1.3221e-01,  7.7920e-03,  1.0953e-01,\n",
       "                         3.3372e-03, -3.1143e-03, -1.4752e-02,  2.8973e-03,\n",
       "                         2.1823e-03, -3.3461e-04,  6.0495e-03,  1.0686e-04,\n",
       "                         9.8368e-02, -2.4093e-03, -7.2850e-03,  6.8567e-03,\n",
       "                        -2.6311e-03,  1.4804e-02, -2.2153e-03,  7.7915e-03,\n",
       "                         1.2951e-01, -4.0493e-04,  1.2672e-02,  3.5415e-02,\n",
       "                         1.3880e-02,  2.9639e+00,  4.8245e-02,  5.4744e-02,\n",
       "                         6.9931e-03,  2.1911e-04,  5.0589e-02,  9.9834e-02,\n",
       "                         8.6284e-03,  1.3393e-03,  5.3276e-03,  7.6404e-01,\n",
       "                        -5.6724e-03,  2.4163e-01, -1.1386e-04,  2.9944e-03,\n",
       "                        -3.4261e-03,  4.1832e-02,  8.9975e-03,  4.7366e-03,\n",
       "                        -1.1471e-03, -6.4856e-03, -1.1508e-03,  6.2380e-02,\n",
       "                         6.0508e-02,  2.0117e-03, -1.3178e-03,  3.2524e-04,\n",
       "                        -2.0831e-04,  3.2531e-02,  4.8172e-02,  1.8825e-05,\n",
       "                         4.1100e-01,  1.0176e-03,  5.0049e-02,  2.3279e-02,\n",
       "                         6.0615e-03, -1.3888e-03, -1.3029e-03,  3.5687e-04,\n",
       "                         2.5219e-03,  4.8500e-03,  7.7655e-03, -1.2679e-02,\n",
       "                        -3.0865e-03, -3.3108e-04,  1.6436e-02,  2.6067e-03,\n",
       "                        -7.9530e-04,  1.1313e-03,  3.2142e-03,  4.9925e-03,\n",
       "                         1.4684e-02,  4.8923e-02,  1.1881e-02,  8.5863e-04,\n",
       "                         3.6385e-04, -5.4419e-03,  1.1963e-02,  5.1811e-03,\n",
       "                         1.2511e-02, -3.8894e-03,  1.1575e-03,  3.6532e-01,\n",
       "                         1.4652e-02,  3.1921e-03, -3.7442e-02,  1.1229e+00,\n",
       "                         1.2219e-03, -1.7889e-03,  4.7557e-03,  9.4813e-03,\n",
       "                        -5.9768e-05,  1.3917e-02,  4.9773e-02,  3.5143e-03,\n",
       "                         4.1691e-02, -6.5460e-03, -3.1775e-02, -1.2232e-04,\n",
       "                         5.4518e-03,  5.4459e-04,  3.9341e-02,  2.0573e-02,\n",
       "                         1.1266e-04, -3.6746e-03, -1.1521e-03, -8.2650e-03,\n",
       "                         2.5384e-03,  5.7588e-03,  1.4600e-02, -2.4157e-02,\n",
       "                         7.6559e-03, -5.8251e-03, -1.4186e-03,  4.8655e-05,\n",
       "                         6.4616e-02, -5.0346e-03, -4.6317e-04,  4.2489e-05,\n",
       "                        -2.3428e-03, -1.5157e-02,  2.4698e-03, -1.0006e-03,\n",
       "                        -8.6813e-03,  4.1740e-03,  4.4910e-04, -1.3508e-03,\n",
       "                         3.2385e-02, -6.9530e-03,  8.3489e-01,  5.2721e-03,\n",
       "                        -2.1669e-03,  3.7211e-03, -7.6575e-03,  1.5363e-03,\n",
       "                        -2.0945e-03,  1.2778e-02, -2.9397e-03,  2.0900e-03,\n",
       "                         3.1117e-04,  1.0038e-03, -7.8546e-04, -1.0716e-02,\n",
       "                         5.5670e-02,  3.7364e-02,  5.3477e-02,  1.9687e-03,\n",
       "                         3.5881e-03,  3.1498e-02,  6.8160e-03,  6.7609e-03,\n",
       "                         6.7198e-02, -2.5669e-03,  2.6087e-03,  8.1349e-04,\n",
       "                         1.6991e-03,  2.6834e-03,  5.3629e-03,  5.6476e-04,\n",
       "                         4.1792e-02, -7.9165e-06,  1.8540e-03, -3.2187e-06,\n",
       "                        -4.2080e-03, -1.0072e-02,  2.2009e-02, -2.2017e-03,\n",
       "                         8.7279e-03,  1.6186e-04,  6.0092e-04,  7.4737e-03,\n",
       "                        -4.3918e-03,  4.5212e-03,  7.0315e-03,  4.1042e-04,\n",
       "                        -3.7665e-04, -3.6419e-02,  4.3542e-01,  5.2507e-03,\n",
       "                         1.9545e-02, -3.5023e-03,  5.3820e-03, -2.7930e-03,\n",
       "                         2.0662e-01,  9.5826e-03, -2.5702e-04,  4.2589e-02,\n",
       "                        -1.0783e-02,  9.6633e-03, -5.7162e-05,  5.2739e-03,\n",
       "                         2.4516e-01]),\n",
       "         device='cuda:0', size=(8193,), nnz=205, layout=torch.sparse_coo)},\n",
       " 'mlp_3': {'resid_3': tensor(indices=tensor([[ 398,  398,  398,  ..., 8192, 8192, 8192],\n",
       "                         [  84,   92,  333,  ..., 8137, 8142, 8192]]),\n",
       "         values=tensor([-1.1339e-04,  4.8803e-05, -8.6506e-05,  ...,\n",
       "                         6.7378e-03,  3.2715e-02, -8.4081e-02]),\n",
       "         device='cuda:0', size=(8193, 8193), nnz=2608, layout=torch.sparse_coo)},\n",
       " 'attn_3': {'resid_3': tensor(indices=tensor([[ 398,  398,  398,  ..., 8192, 8192, 8192],\n",
       "                         [  91,  449,  778,  ..., 8135, 8184, 8192]]),\n",
       "         values=tensor([ 3.0196e-04,  4.2427e-04,  9.8414e-07,  ...,\n",
       "                         1.2215e-03, -2.1751e-04, -4.7696e-02]),\n",
       "         device='cuda:0', size=(8193, 8193), nnz=2438, layout=torch.sparse_coo)},\n",
       " 'resid_2': {'mlp_3': tensor(indices=tensor([[1141, 1141, 1141,  ..., 8192, 8192, 8192],\n",
       "                         [  64,   84,  206,  ..., 8100, 8124, 8192]]),\n",
       "         values=tensor([ 1.4634e-05,  4.0105e-04,  9.5684e-04,  6.0495e-04,\n",
       "                         1.9631e-04, -6.7892e-05,  3.8929e-04,  2.2684e-04,\n",
       "                        -1.8729e-04,  3.4594e-04,  7.2216e-04,  1.6399e-04,\n",
       "                         9.8669e-04, -1.1454e-04, -7.9107e-05,  1.2346e-04,\n",
       "                         4.3115e-04,  9.9912e-04,  1.7973e-04,  7.7230e-05,\n",
       "                         1.5692e-06, -2.3214e-05, -1.3746e-05,  2.2660e-05,\n",
       "                         1.2624e-03,  3.4183e-04,  2.7481e-04, -1.0860e-03,\n",
       "                         4.9455e-03, -5.0687e-04, -5.8032e-04,  2.9074e-04,\n",
       "                        -1.3584e-04, -5.9671e-05, -2.5245e-05,  5.1629e-05,\n",
       "                         5.5258e-04, -3.8178e-04, -1.1461e-03,  5.2843e-05,\n",
       "                         1.0054e-03,  6.1288e-04, -1.4038e-05,  4.2871e-04,\n",
       "                         2.5438e-04,  2.4494e-04,  8.7164e-05,  3.1479e-04,\n",
       "                         3.3275e-04,  2.4949e-04, -6.5639e-05,  4.2557e-04,\n",
       "                         2.9918e-04,  3.0755e-05,  6.1089e-04,  7.0849e-04,\n",
       "                         3.2820e-05,  7.5148e-05,  1.9022e-04,  8.9786e-05,\n",
       "                        -1.9875e-04,  1.8947e-04, -2.2786e-04, -1.4150e-04,\n",
       "                         2.2701e-05,  3.4630e-04, -6.0096e-04,  1.1076e-04,\n",
       "                         3.6656e-04, -5.4854e-04,  2.5699e-04, -5.3931e-04,\n",
       "                         1.8426e-04, -7.7685e-04, -3.7902e-05,  2.1679e-04,\n",
       "                         1.1648e-04, -1.4366e-04,  4.2733e-05,  1.0757e-04,\n",
       "                        -1.4727e-05,  8.0062e-05, -2.4249e-03,  3.3874e-03,\n",
       "                        -1.5748e-03,  1.5158e-04, -6.5540e-03,  5.1134e-03,\n",
       "                         5.1916e-04, -2.1228e-04,  6.8698e-04, -8.1286e-04,\n",
       "                        -7.7653e-04,  8.4011e-04,  1.3544e-03, -5.5827e-04,\n",
       "                         4.4541e-03, -2.0143e-04, -1.4887e-03, -4.0011e-03,\n",
       "                        -1.3196e-03,  3.2826e-04,  9.9476e-04, -2.1235e-04,\n",
       "                         2.3319e-03,  2.4889e-04,  1.4427e-04,  5.0155e-04,\n",
       "                         6.8969e-04,  2.5973e-04, -4.3141e-05, -4.3482e-04,\n",
       "                         7.8335e-04,  1.2870e-03,  1.3844e-03,  3.7060e-04,\n",
       "                         1.7892e-04, -1.2071e-03,  6.3071e-04, -1.4495e-03,\n",
       "                        -4.0698e-04,  6.0161e-03, -5.8602e-05,  1.1292e-03,\n",
       "                        -2.4031e-04,  7.8722e-04, -2.0991e-04,  1.4158e-03,\n",
       "                         1.6001e-03, -2.6597e-03,  2.2530e-03,  5.2577e-04,\n",
       "                        -1.6587e-04,  1.9523e-03,  2.2971e-05,  1.6402e-03,\n",
       "                         9.3766e-04,  1.3479e-04,  6.0514e-05, -2.8425e-04,\n",
       "                        -5.7642e-04, -2.1215e-04, -1.7665e-03,  1.3835e-04,\n",
       "                         2.5652e-03, -1.6742e-04,  3.1984e-03, -1.5991e-04,\n",
       "                         1.3851e-03, -2.5583e-04,  1.8757e-04,  6.2305e-04,\n",
       "                         5.4291e-05,  8.2596e-05, -5.8961e-04,  8.0376e-04,\n",
       "                         7.2340e-04,  2.6032e-04, -3.9810e-04, -1.5615e-04,\n",
       "                         3.3964e-04, -1.8720e-05, -7.8219e-04, -1.9077e-05,\n",
       "                        -9.1575e-06,  1.0253e-03, -4.7420e-04,  4.7497e-05,\n",
       "                         4.9068e-04, -1.1405e-04, -3.0939e-05, -1.8981e-04,\n",
       "                        -3.0286e-05,  8.5027e-05,  1.8019e-05, -1.5946e-03,\n",
       "                         2.9470e-04, -1.3843e-04, -1.4277e-04, -1.8143e-03,\n",
       "                         1.1765e-05, -6.0918e-05,  3.1184e-03,  3.2411e-04,\n",
       "                        -6.2098e-06, -4.7319e-04, -4.2010e-04,  6.3237e-03,\n",
       "                        -4.6420e-04,  3.7452e-01, -2.3686e-04,  5.4619e-01,\n",
       "                         6.1287e-03, -1.7464e-03,  2.0356e-03,  2.7234e-03,\n",
       "                         4.2213e-04, -2.0493e-03,  1.5397e-02,  6.8372e-04,\n",
       "                        -1.4414e-03, -3.4354e-04, -1.1381e-03,  5.6739e-02,\n",
       "                         2.1201e-02, -4.5741e-03, -3.6811e-03,  1.5564e-02,\n",
       "                        -1.3270e-03,  5.0982e-04, -9.1098e-03, -1.0922e-03,\n",
       "                         1.1817e-02, -4.9427e-03,  1.8847e-02,  4.8020e-03,\n",
       "                         1.0128e-02,  5.4101e-04, -2.5122e-03,  1.2130e-03,\n",
       "                        -2.0846e-03, -3.0020e-03,  3.6742e-02, -5.0704e-04,\n",
       "                         1.8942e-03,  1.3044e-03,  1.4452e-04, -2.6401e-03,\n",
       "                         9.1727e-05, -4.2697e-03,  5.2680e-03, -1.6380e-03,\n",
       "                        -3.7977e-03, -4.8913e-04, -5.1073e-03,  1.4275e-03,\n",
       "                         4.6681e-03, -8.9509e-05,  8.9798e-03,  4.5129e-01,\n",
       "                         5.3647e-03, -3.9699e-04,  3.3982e-01,  3.9357e-03,\n",
       "                         3.3263e-02,  6.5700e-05,  1.9540e-02,  1.6090e-02,\n",
       "                         5.1526e-03,  1.0030e-02, -1.1425e-02,  1.3987e-02,\n",
       "                         2.8604e-02,  2.5247e-03,  3.9980e-04,  6.4398e-04,\n",
       "                         3.8738e-02, -3.1717e-03, -3.0976e-04, -6.8820e-03,\n",
       "                         1.5985e-03,  4.4887e-03,  5.8023e-03, -2.7404e-03,\n",
       "                        -1.9906e-03, -7.6451e-03, -2.3951e-03, -6.3339e-05,\n",
       "                         1.1733e-03,  1.6398e-03, -1.5049e-03,  2.3536e-03,\n",
       "                         4.6180e-03, -4.7733e-04,  1.4404e-04,  1.7445e-03,\n",
       "                         2.9256e-03,  6.0172e-02, -2.5915e-04,  2.3167e-02,\n",
       "                        -6.3522e-03, -1.8321e-02,  2.1675e-04, -2.1829e-04,\n",
       "                         7.2978e-03, -2.0065e-02,  4.7468e-03,  1.1396e-02,\n",
       "                         8.9706e-04, -2.6536e-03, -7.2331e-05,  1.4139e-03,\n",
       "                        -1.8218e-03, -2.2874e-03,  6.5990e-03, -1.0702e-02,\n",
       "                        -5.6747e-03,  2.4908e-03,  3.4495e-04,  1.4776e-03,\n",
       "                        -1.7643e-03, -8.7337e-04,  2.3308e-03,  5.7434e-03,\n",
       "                         1.4217e-04, -4.8782e-03, -3.2593e-03,  1.7504e-04,\n",
       "                        -5.9315e-04,  6.2424e-04, -2.7193e-02,  4.7846e-04,\n",
       "                        -1.7596e-02,  3.2281e-03, -5.8763e-03,  1.5740e-03,\n",
       "                         9.0047e-04, -3.6229e-04,  1.3841e-02,  5.4173e-04,\n",
       "                        -4.8134e-04,  1.2846e-03,  2.2772e-03,  3.3760e-03,\n",
       "                         3.1313e-04,  3.1889e-03,  7.6834e-03,  1.3080e-03,\n",
       "                         6.6807e-03,  1.5455e-03,  7.7421e-04,  1.3862e-03,\n",
       "                        -9.6123e-04, -2.4389e-03, -1.0160e-03,  3.6142e-03,\n",
       "                        -4.4290e-03, -2.0869e-03, -6.4565e-03, -3.0302e-02,\n",
       "                        -7.1456e-04, -3.6509e-03,  7.1230e-03,  2.3213e-02,\n",
       "                        -1.1144e-03, -6.9875e-04,  2.2464e-02,  2.4094e-03,\n",
       "                         2.1993e-02, -2.0767e-03, -7.9510e-03,  2.9109e-03,\n",
       "                         4.2142e-03,  1.4543e-02, -8.0043e-04, -1.0401e-03,\n",
       "                         2.8593e-03,  2.0499e-02, -1.2440e-02, -1.3267e-04,\n",
       "                         6.0109e-02, -7.4286e-05,  2.6105e-04, -4.7452e-04,\n",
       "                         5.2685e-05,  1.0812e-02, -2.7601e-03, -1.1164e-03,\n",
       "                         4.1052e-02,  3.5468e-03,  2.8359e-03, -2.1105e-03,\n",
       "                         1.1350e-03, -4.2453e-04,  3.0474e-03, -1.0337e-03,\n",
       "                        -8.7009e-04,  4.2239e-03,  2.7619e-02, -1.5596e-03,\n",
       "                        -1.7984e-04,  3.1160e-03,  3.6574e-03, -2.2858e-03,\n",
       "                         9.0824e-04, -3.0105e-03,  1.0106e-04,  1.0063e-02,\n",
       "                         8.5925e-02,  4.9080e-03,  7.1743e-03,  1.3139e-04,\n",
       "                         6.6961e-04,  6.2235e-03, -1.8941e-03,  9.5347e-04,\n",
       "                        -3.3241e-05,  1.8186e-03,  2.7650e-02,  1.0828e-02,\n",
       "                        -1.4851e-01, -3.0134e-04, -1.2877e-04,  2.5936e-05,\n",
       "                        -7.6276e-05, -1.8554e-04, -2.3630e-03,  1.4064e-04,\n",
       "                         8.3787e-05,  5.2170e-04, -3.6794e-04,  6.5741e-05,\n",
       "                        -5.7352e-05,  4.2248e-04, -1.8752e-05, -1.5362e-03,\n",
       "                        -7.8211e-04,  2.7903e-05,  2.7802e-04,  1.2235e-03,\n",
       "                        -4.3728e-04, -1.2065e-03, -1.2408e-04,  1.3156e-05,\n",
       "                        -1.3845e-04,  3.9440e-05,  2.9268e-05, -1.0544e-04,\n",
       "                         5.7556e-05,  9.2709e-05,  1.7320e-04,  3.6182e-05,\n",
       "                         2.2976e-04,  1.1771e-04, -9.5140e-05, -1.2439e-03,\n",
       "                        -9.4422e-05, -7.1561e-06, -2.8925e-04, -4.0375e-03,\n",
       "                        -8.9171e-05,  2.6731e-04, -6.2312e-05, -2.4195e-05,\n",
       "                        -1.4596e-04, -2.2490e-03, -9.7553e-05, -1.7479e-04,\n",
       "                        -4.4714e-05, -1.1822e-04, -5.0882e-04, -4.7778e-03,\n",
       "                        -5.6800e-04,  1.2070e-04,  1.2730e-04, -8.2484e-04,\n",
       "                        -6.7371e-04, -6.2583e-05,  1.0020e-04, -4.1805e-05,\n",
       "                        -4.2270e-04, -6.2442e-05, -5.6414e-04,  5.2967e-04,\n",
       "                         5.6974e-04,  4.0277e-04, -1.1832e-05,  4.8269e-04,\n",
       "                        -1.2556e-03, -3.5146e-04, -9.0576e-04,  6.6897e-04,\n",
       "                         2.0581e-04,  6.2056e-04,  5.6579e-04, -5.4706e-04,\n",
       "                        -2.6765e-04, -7.1293e-04,  2.6876e-05, -2.6633e-04,\n",
       "                        -7.5450e-04,  3.3545e-04,  1.2969e-04,  2.0471e-03,\n",
       "                         9.6965e-05, -2.7368e-04,  1.3749e-04, -2.7558e-03,\n",
       "                        -1.1794e-03,  4.4817e-04,  1.1434e-04, -2.5184e-04,\n",
       "                         7.4352e-04,  5.8729e-04, -8.0035e-04,  7.2953e-06,\n",
       "                        -2.6198e-05,  9.7944e-04,  1.2789e-01,  5.7503e-01,\n",
       "                        -3.6063e-03,  1.7580e-03, -5.5521e-04,  4.0292e-02,\n",
       "                         6.7597e-03,  2.6196e-02,  6.2068e-03,  6.4896e-03,\n",
       "                         1.4404e-02,  5.5162e-04,  7.0028e-04, -3.3342e-03,\n",
       "                         7.0295e-03, -2.5113e-02,  6.7350e-03,  6.1729e-04,\n",
       "                         2.3942e-02,  5.8929e-03,  2.2376e-02, -9.5157e-04,\n",
       "                         4.0616e-03,  6.1498e-03,  8.9447e-03, -6.1132e-04,\n",
       "                        -7.1614e-04,  1.0582e-02,  9.1535e-02,  9.2876e-04,\n",
       "                         1.6169e-05,  2.9329e-02, -4.7782e-03,  1.5993e-01,\n",
       "                        -3.7419e-03, -3.7316e-03,  2.2288e-01,  2.4662e-02,\n",
       "                         3.3430e-02, -1.4069e-02,  2.7272e-02, -1.4894e-04,\n",
       "                         3.5129e-03,  8.2502e-02,  1.3975e-03,  3.1267e-03,\n",
       "                        -5.1406e-03, -6.1830e-03, -4.0255e-05, -4.8926e-04,\n",
       "                         1.5635e-03, -4.9561e-03,  3.5777e-03, -7.0545e-03,\n",
       "                         2.1832e-02,  4.6067e-03,  4.0792e-03, -1.7528e-04,\n",
       "                        -3.2032e-04,  5.2110e-03, -1.1833e-03, -9.2667e-07,\n",
       "                        -5.5300e-04,  2.0391e-03,  1.4741e-02, -3.8374e-03,\n",
       "                         1.2495e-03, -2.3541e-04, -1.0773e-03,  1.4983e-02,\n",
       "                        -1.2045e-02,  5.0977e-03,  4.5389e-03, -1.5773e-03,\n",
       "                        -3.3860e-03, -3.1253e-03,  3.8748e-03,  9.9689e-04,\n",
       "                        -5.8723e-04, -3.7795e-04, -6.9824e-04,  5.9860e-03,\n",
       "                        -5.0205e-03,  5.9887e-04,  3.9398e-03,  2.4469e-03,\n",
       "                        -3.1854e-03, -4.0065e-03, -3.8211e-03,  1.1830e-03,\n",
       "                         3.4969e-03,  2.2644e-03, -3.3412e-03,  1.9002e-02,\n",
       "                        -2.8075e-03,  6.0605e-04,  1.6934e-02,  2.4313e-02,\n",
       "                        -8.6040e-03, -6.8273e-03, -1.8604e-02,  3.4159e-02,\n",
       "                         2.4646e-03, -1.6842e-03,  2.9510e-02,  1.7171e-02,\n",
       "                         9.8468e-02,  3.7918e-03, -1.2239e-03,  6.3466e-05,\n",
       "                         1.9555e-02,  1.2153e-03,  1.5070e-01,  1.8655e-02,\n",
       "                        -1.4672e-03,  4.7597e-03,  3.9937e-03, -3.0714e-04,\n",
       "                        -1.0246e-02, -1.1534e-03, -1.8550e-05,  3.1075e-03,\n",
       "                        -5.6189e-03, -1.5767e-03,  9.4790e-04, -6.1733e-02,\n",
       "                        -4.0489e-03,  1.4909e-02,  5.4926e-03, -1.5429e-03,\n",
       "                         4.5662e-03,  1.6051e-02,  4.6063e-03, -2.6570e-04,\n",
       "                         1.0969e-01,  4.1891e-02,  1.2251e-01, -5.0606e-04,\n",
       "                         6.1478e-03,  9.4467e-03,  1.3695e-03,  6.1941e-03,\n",
       "                         1.6362e-02,  7.1699e-04,  2.0343e-02,  1.2786e-03,\n",
       "                         5.1561e-03,  6.1460e-03,  9.1107e-04,  5.4317e-02,\n",
       "                         5.8962e-02,  1.1851e-03,  9.8081e-03, -2.7701e-03,\n",
       "                         3.2282e-03,  3.3322e-02,  2.6197e-03, -4.4918e-03,\n",
       "                         2.5832e-02,  5.5866e-03,  1.6745e-04,  5.8671e-02,\n",
       "                        -1.0807e-04,  4.9650e-03,  1.2739e-03,  9.8950e-04,\n",
       "                        -2.7003e-04, -3.5837e-03,  8.0036e-05,  5.3882e-03,\n",
       "                        -3.3009e-02, -1.5844e-04, -9.1565e-04,  1.6878e-03,\n",
       "                        -1.9990e-02, -3.6017e-04,  8.4499e-03, -1.1095e-03,\n",
       "                         6.3190e-04,  4.5732e-03, -1.0932e-03, -8.9408e-04,\n",
       "                        -1.3145e-03, -2.8356e-04, -2.7984e-03,  3.2917e-03,\n",
       "                         4.5155e-03, -1.8563e-03, -5.3016e-03,  2.9183e-04,\n",
       "                        -5.6649e-05,  2.3462e-03,  2.2685e-02, -2.0485e-03,\n",
       "                         2.3548e-03,  6.6700e-03,  6.5104e-03,  9.1127e-04,\n",
       "                         5.1048e-04,  3.4946e-03,  7.4829e-02, -1.2250e-03,\n",
       "                        -7.5821e-05,  1.5428e-02,  1.8049e-02,  2.9798e-05,\n",
       "                         4.4700e-04, -5.1008e-04, -1.4554e-03,  7.6173e-04,\n",
       "                         3.6741e-03, -1.6077e-03,  1.7086e-02,  5.3696e-04,\n",
       "                        -2.0647e-03,  3.0892e-03,  5.7289e-05,  5.6260e-04,\n",
       "                        -9.9994e-02, -1.8755e-03,  1.0227e-03,  5.3177e-04,\n",
       "                        -2.9812e-04,  5.7243e-03, -8.7013e-05,  1.9130e-04,\n",
       "                        -1.7347e-04,  2.7589e-04,  1.2557e-03,  4.9291e-05,\n",
       "                         1.7413e-03, -8.7411e-04, -3.8473e-04,  2.5587e-04,\n",
       "                         1.2255e-03, -4.1469e-04,  3.7884e-03,  4.2979e-04,\n",
       "                         1.7666e-04, -2.5769e-04,  1.0703e-03,  1.1026e-04,\n",
       "                         8.8850e-05,  8.7012e-06, -6.0141e-04,  7.8145e-04,\n",
       "                         1.4742e-03, -2.9645e-04,  3.6163e-04,  2.9164e-04,\n",
       "                         2.3663e-04,  9.8276e-04,  5.2240e-04,  4.7997e-04,\n",
       "                        -1.0794e-04,  9.5595e-04, -7.0587e-04, -2.4100e-04,\n",
       "                         3.3739e-04, -7.9356e-04,  4.0645e-04,  3.5021e-04,\n",
       "                         4.9465e-04,  1.6703e-03,  1.8963e-04,  4.8268e-05,\n",
       "                         5.4035e-04, -9.9712e-04, -3.1855e-04,  2.4269e-03,\n",
       "                         1.1016e-05,  3.2641e-04,  5.5191e-04, -4.9114e-04,\n",
       "                        -3.4950e-04, -7.4894e-04, -5.6732e-05,  7.5023e-03,\n",
       "                         3.5426e-04,  7.5564e-03, -8.1548e-04,  6.5582e-02,\n",
       "                        -1.5594e-03, -3.5014e-03,  6.0939e-03,  3.0464e-03,\n",
       "                        -4.4067e-04, -1.4064e-02,  1.2379e-03, -1.9152e-04,\n",
       "                         2.9416e-03, -4.8055e-02,  2.0919e-02,  3.2563e-03,\n",
       "                         1.2252e-02,  2.1786e-04,  9.5699e-03,  2.2424e-03,\n",
       "                        -1.2640e-03,  5.6834e-03,  4.0766e-03,  3.1403e-04,\n",
       "                        -3.0310e-03, -2.6958e-05, -3.6366e-03, -6.7432e-04,\n",
       "                         2.6629e-02, -1.8890e-03,  9.3013e-03, -7.1787e-04,\n",
       "                         2.0794e-03, -4.3264e-02,  2.8588e-04, -2.5642e-02,\n",
       "                         1.4335e-04, -9.0879e-05,  9.1458e-04,  2.5119e-03,\n",
       "                        -8.5958e-04, -5.9624e-03, -1.7059e-03, -8.7560e-04,\n",
       "                        -4.3132e-03,  3.5942e-04,  4.4432e-03, -2.0007e-03,\n",
       "                        -9.9512e-03, -1.0130e-04, -2.7873e-06, -2.5091e-03,\n",
       "                         1.8038e-03, -3.6946e-03, -1.4627e-03, -7.2425e-02,\n",
       "                         7.5363e-03,  1.3234e-02,  3.9955e-03, -7.3742e-03,\n",
       "                         2.8766e-02, -3.4466e-03,  3.1159e-03, -9.4346e-03,\n",
       "                        -1.3573e-02, -1.9357e-03, -3.1203e-04, -1.7095e-03,\n",
       "                        -2.3604e-03,  1.9214e-03, -2.6454e-02,  2.7734e-03,\n",
       "                         6.8719e-04, -3.5700e-03,  7.4992e-03,  3.2435e-04,\n",
       "                         4.4813e-03,  4.9639e-04, -6.9093e-03,  4.9801e-03,\n",
       "                        -1.9101e-03,  1.7754e-03, -5.8061e-04,  6.9692e-03,\n",
       "                        -2.9106e-03,  8.3319e-03, -2.1763e-03, -1.3413e-02,\n",
       "                         1.8409e-05, -9.7822e-03, -3.3572e-04, -3.1281e-03,\n",
       "                        -5.4886e-03, -1.2828e-03, -4.6945e-04, -1.1871e-03,\n",
       "                        -1.6023e-03, -9.3598e-04, -5.5161e-04, -1.2815e-02,\n",
       "                        -4.0697e-04, -5.5370e-03, -3.3076e-05, -2.6506e-04,\n",
       "                        -6.2022e-04,  7.1870e-04,  7.5001e-03,  1.1140e-02,\n",
       "                        -1.7999e-03,  7.9052e-04, -2.0677e-04,  3.3501e-03,\n",
       "                         1.1151e-03,  1.2423e-03, -2.3711e-03, -1.7704e-03,\n",
       "                        -2.6769e-03,  2.8532e-03,  2.0749e-04, -3.0469e-03,\n",
       "                        -7.7515e-04, -7.3359e-04,  2.7940e-02,  4.3315e-04,\n",
       "                         7.9430e-03, -1.4250e-03,  7.8830e-04, -4.5336e-03,\n",
       "                        -1.4863e-03,  5.8248e-04,  8.4833e-04, -1.9157e-03,\n",
       "                         5.0308e-03,  8.8149e-04, -1.4049e-03,  8.9679e-04,\n",
       "                        -4.2173e-03, -2.3329e-04,  5.6261e-03, -1.3027e-04,\n",
       "                         2.9864e-03, -9.6004e-03, -2.5706e-03, -1.1911e-03,\n",
       "                        -4.6937e-03, -6.2239e-04, -3.5718e-03,  9.3369e-04,\n",
       "                        -1.6264e-04,  6.8871e-03,  4.7248e-03, -3.3030e-03,\n",
       "                        -5.7097e-04,  1.1237e-03,  2.5202e-03,  6.8229e-03,\n",
       "                         2.8616e-03,  4.3620e-03,  2.0111e-03,  2.4300e-03,\n",
       "                        -3.8901e-04, -5.5964e-04,  2.8197e-03,  8.4206e-03,\n",
       "                        -2.7429e-03, -6.0418e-04,  7.7475e-03, -1.1370e-02,\n",
       "                         8.9332e-04, -2.5858e-03, -4.3433e-03,  1.8411e-03,\n",
       "                         1.4123e-02, -3.8341e-03, -7.2534e-02,  2.4846e-04,\n",
       "                         9.7647e-05,  2.2139e-03,  4.4082e-03, -2.7558e-04,\n",
       "                        -1.1934e-02, -2.3353e-03, -1.4390e-03,  2.3374e-02,\n",
       "                         1.4489e-03,  1.4210e-03, -4.7357e-04,  2.6301e-03,\n",
       "                         4.5196e-03,  3.7126e-04, -2.7185e-03,  1.2991e-03,\n",
       "                        -1.0703e-03, -1.7621e-02,  5.1028e-03, -2.0996e-03,\n",
       "                         1.0661e-03, -3.5099e-03,  2.7001e-03,  9.4080e-04,\n",
       "                         2.5110e-04, -6.5117e-04, -2.9272e-03, -4.0487e-03,\n",
       "                         5.6241e-03, -5.0057e-03, -1.3274e-03,  4.5774e-04,\n",
       "                         4.2503e-05,  2.0420e-03, -2.4133e-03, -1.3179e-05,\n",
       "                         1.3344e-03, -6.7767e-03, -4.3759e-03,  2.8143e-01]),\n",
       "         device='cuda:0', size=(8193, 8193), nnz=992, layout=torch.sparse_coo),\n",
       "  'attn_3': tensor(indices=tensor([[ 449,  449,  449,  ..., 8192, 8192, 8192],\n",
       "                         [  84,  206,  214,  ..., 8009, 8100, 8192]]),\n",
       "         values=tensor([ 8.3323e-02,  2.0892e-01, -3.5185e-03,  8.4392e-04,\n",
       "                        -1.8129e-04, -4.3729e-03,  1.7114e-02,  4.0720e-03,\n",
       "                         7.8282e-03,  5.4910e-03, -1.5541e-03, -1.3522e-04,\n",
       "                         6.5480e-04,  2.6342e-03,  3.4780e-03, -1.0014e-02,\n",
       "                        -8.6810e-04,  1.3516e-03, -2.8536e-03,  4.6961e-03,\n",
       "                         3.4711e-03,  1.8026e-03,  1.6873e-03,  3.3073e-03,\n",
       "                         6.5294e-03, -7.3988e-04,  1.2485e-03,  3.2870e-03,\n",
       "                         7.0257e-02, -2.2736e-03,  9.6969e-05,  9.4679e-02,\n",
       "                         3.4309e-03, -2.8191e-02, -3.1770e-03, -3.3567e-03,\n",
       "                         1.9034e-02,  1.4653e-02,  2.4297e-02,  2.2123e-02,\n",
       "                        -8.6289e-03, -1.0049e-04,  3.4337e-03,  1.3203e-02,\n",
       "                        -8.8406e-04,  1.1495e-03,  2.7086e-03, -1.7197e-03,\n",
       "                         9.1567e-04,  2.2299e-03,  1.0366e-03, -4.4914e-03,\n",
       "                        -1.6871e-03, -2.6568e-03,  2.6167e-02,  3.1501e-03,\n",
       "                        -1.3853e-02,  5.7039e-03, -1.1077e-03,  6.4100e-05,\n",
       "                        -1.8935e-03, -9.4872e-05, -2.7530e-04,  3.0043e-03,\n",
       "                        -5.7268e-03, -5.6413e-03,  1.2815e-03, -1.1896e-03,\n",
       "                        -1.1828e-04,  5.1483e-03, -6.0171e-04,  2.3027e-03,\n",
       "                         3.8435e-03, -8.9935e-04,  3.2128e-04, -3.0657e-03,\n",
       "                        -1.9864e-03, -5.7441e-04, -9.2930e-04, -6.3719e-05,\n",
       "                         2.3442e-04, -3.6077e-04,  4.3363e-03, -7.9001e-04,\n",
       "                        -1.4190e-03,  3.8373e-03,  7.2024e-03, -7.0773e-03,\n",
       "                         2.9916e-04, -1.3923e-03,  2.4101e-03,  5.0213e-04,\n",
       "                        -5.6946e-03,  8.6218e-03, -3.0084e-03,  1.8978e-04,\n",
       "                         7.5699e-03,  3.4174e-03, -6.5064e-03,  7.2135e-03,\n",
       "                        -1.7809e-03,  2.2657e-02, -2.0774e-03, -3.7607e-04,\n",
       "                         1.4173e-02,  5.7082e-03,  6.7388e-02, -1.6836e-03,\n",
       "                        -4.2745e-03,  6.1261e-05,  9.9617e-03,  1.0855e-03,\n",
       "                         2.2928e-03,  7.1625e-03, -5.7489e-04, -2.9693e-03,\n",
       "                         1.5998e-03, -6.2285e-05, -2.1599e-03, -9.8141e-04,\n",
       "                        -7.4626e-05,  2.5368e-03,  2.2002e-03, -2.8129e-04,\n",
       "                         3.2100e-04, -2.7976e-02, -7.2875e-05,  1.8726e-02,\n",
       "                         2.2069e-03, -1.7969e-03,  2.1807e-04,  5.3325e-04,\n",
       "                         6.3520e-04,  1.0441e-04, -5.1419e-03, -1.0402e-05,\n",
       "                        -4.3502e-02,  4.7944e-02, -5.0105e-03,  2.3013e-04,\n",
       "                         3.4011e-03, -7.9042e-04,  5.0825e-02,  4.2157e-04,\n",
       "                        -1.0081e-03, -4.7277e-03,  1.0235e-02,  3.9245e-03,\n",
       "                         1.0037e-03,  3.6607e-03,  1.2522e-02,  1.2865e-03,\n",
       "                         1.5414e-03, -1.5167e-03,  3.8903e-03,  7.7475e-04,\n",
       "                         6.3648e-03,  1.7438e-03,  7.7657e-03, -2.6777e-04,\n",
       "                         7.3174e-03,  2.3180e-03,  1.2223e-03,  2.1728e-03,\n",
       "                         3.7885e-03, -2.2514e-04,  8.0875e-03, -3.1014e-05,\n",
       "                        -8.7770e-05,  2.1636e-03,  4.5088e-03,  4.2495e-02,\n",
       "                         7.5383e-04, -4.2690e-03, -7.4783e-02, -3.7594e-03,\n",
       "                         3.1202e-02, -3.5321e-03, -1.9637e-03,  1.1843e-01,\n",
       "                         1.9602e-02,  1.3262e-02,  3.5839e-04,  9.0690e-03,\n",
       "                         7.1135e-04, -8.9629e-04, -5.9234e-04,  1.2046e-03,\n",
       "                         1.9906e-02, -1.0680e-03,  3.7497e-03, -8.6420e-03,\n",
       "                        -3.1230e-04,  5.1801e-04, -7.9819e-04,  7.2973e-04,\n",
       "                         8.8701e-04,  3.5578e-03, -1.1354e-03, -3.4645e-03,\n",
       "                        -1.6441e-02,  1.1006e-03, -1.3655e-02, -8.0831e-04,\n",
       "                        -9.4228e-04, -4.6712e-04,  5.3462e-04, -2.9923e-03,\n",
       "                        -5.4139e-04, -9.0386e-04,  1.7697e-03, -9.0635e-03,\n",
       "                         9.4381e-03, -2.9520e-03, -2.9742e-04,  2.4384e-04,\n",
       "                        -6.6677e-04, -1.5353e-03,  7.7171e-03, -9.6868e-03,\n",
       "                         4.5165e-03,  7.8662e-04,  1.0644e-04,  5.0363e-03,\n",
       "                        -2.2134e-03, -1.8226e-03, -2.5726e-03, -2.0753e-03,\n",
       "                         2.9152e-04,  8.3361e-04,  1.1096e-03, -2.0857e-04,\n",
       "                         1.7429e-03, -1.9053e-03, -8.0475e-04, -3.1863e-05,\n",
       "                         7.1633e-04, -4.6271e-05,  1.6086e-02, -1.5254e-04,\n",
       "                        -2.0903e-03, -3.0984e-03, -4.0655e-03, -2.7992e-03,\n",
       "                        -4.1955e-04, -3.6537e-03,  2.8185e-04,  1.2257e-02,\n",
       "                        -1.8438e-03, -9.7217e-04,  1.3194e-02,  7.4369e-04,\n",
       "                        -1.5976e-04, -8.2866e-03, -6.7458e-04,  3.5894e-03,\n",
       "                         2.6959e-02,  1.8069e-03, -1.2434e-03,  2.2614e-03,\n",
       "                         6.9059e-03,  3.2222e-02,  1.2362e-03, -1.2381e-03,\n",
       "                         1.1120e-05,  3.3590e-03,  1.7653e-03,  6.5080e-02,\n",
       "                        -5.1565e-06,  1.3656e-02, -2.2398e-03,  2.6104e-03,\n",
       "                         8.3900e-04,  1.0626e-03, -1.2449e-02, -2.2345e-03,\n",
       "                        -8.3602e-05,  2.6167e-03, -2.6040e-03, -5.8821e-04,\n",
       "                        -4.0279e-04, -1.9415e-04, -5.0608e-02, -2.1229e-03,\n",
       "                         8.7257e-03,  1.9798e-04, -2.5722e-03, -1.7154e-03,\n",
       "                         5.9908e-03,  1.9076e-03, -2.5037e-05, -3.1944e-03,\n",
       "                         3.3670e-02, -6.8697e-03,  4.2822e-04, -1.7261e-04,\n",
       "                         7.8031e-04,  1.6253e-03,  1.9193e-04,  5.1106e-04,\n",
       "                         1.1448e-03,  7.2898e-04,  3.9928e-04,  1.5646e-03,\n",
       "                        -8.6538e-04, -9.1593e-04, -8.1359e-04,  2.2609e-04,\n",
       "                         1.8465e-03, -6.8150e-04,  2.4463e-03,  5.6357e-04,\n",
       "                         1.0458e-03, -3.7958e-04,  1.2641e-03,  4.1287e-04,\n",
       "                         2.6338e-04,  4.1855e-04, -3.5609e-04,  2.1550e-03,\n",
       "                         1.3877e-03, -5.9036e-06,  8.1930e-04,  2.5344e-04,\n",
       "                        -1.3833e-04,  3.8179e-04, -3.2900e-04,  1.3916e-03,\n",
       "                        -6.5187e-04,  3.1333e-03, -1.1739e-03,  1.3112e-05,\n",
       "                         1.6461e-03,  2.0223e-04, -1.6211e-04, -6.4260e-05,\n",
       "                        -4.3455e-04,  1.8052e-03,  2.1634e-05, -2.2237e-03,\n",
       "                         2.0718e-03, -7.5784e-04, -9.7729e-05,  4.3354e-04,\n",
       "                        -1.1313e-05,  3.5299e-04,  7.4854e-04,  7.2011e-04,\n",
       "                         6.1551e-05, -2.3256e-04,  3.0416e-04,  4.4157e-04,\n",
       "                        -1.0072e-04,  5.0391e-02, -1.1627e-04,  1.3600e-02,\n",
       "                        -1.6494e-03,  2.5667e-04, -5.4460e-04, -2.2217e-04,\n",
       "                        -2.7748e-04,  1.0207e-02,  9.5876e-04, -2.4508e-04,\n",
       "                         1.7874e-04, -4.2654e-04,  1.1721e-02,  6.6111e-03,\n",
       "                        -7.3815e-04, -2.0969e-03, -7.5660e-04,  4.8808e-03,\n",
       "                         2.0102e-03,  2.3781e-04, -4.4536e-03,  1.1439e-03,\n",
       "                         1.7684e-04,  4.2391e-05,  7.7786e-03,  3.6762e-05,\n",
       "                        -2.2404e-04,  8.0208e-05, -2.7798e-03,  1.8545e-03,\n",
       "                        -1.6467e-03,  9.3985e-04,  7.9403e-05,  1.3308e-03,\n",
       "                         2.6552e-03, -2.6970e-05,  1.3056e-03,  5.0437e-02,\n",
       "                        -7.9158e-04,  7.1005e-04,  3.0178e-02,  4.9213e-04,\n",
       "                         2.1279e-02, -1.4817e-03,  9.1035e-03, -1.2008e-02,\n",
       "                        -3.3595e-03, -1.0152e-03,  1.0381e-03,  5.9304e-03,\n",
       "                         1.4642e-04,  1.8922e-04,  6.6960e-05,  4.0938e-03,\n",
       "                        -7.3196e-05,  6.2551e-04,  3.7849e-04,  1.5321e-04,\n",
       "                         8.5526e-04, -8.3765e-04,  2.5808e-04, -5.7276e-04,\n",
       "                        -3.8689e-04,  5.8077e-04, -3.9118e-05,  1.9894e-03,\n",
       "                        -5.9895e-04,  1.3633e-03, -1.1796e-03,  7.7423e-04,\n",
       "                        -7.3672e-04,  1.0459e-02,  1.1399e-04, -3.5586e-04,\n",
       "                         1.0956e-04,  1.1155e-03,  4.2742e-04,  1.7083e-03,\n",
       "                        -6.6827e-03, -7.1410e-04, -1.4520e-03, -5.3783e-05,\n",
       "                         7.5544e-04, -3.4483e-03, -5.0792e-03, -7.9897e-03,\n",
       "                        -7.9635e-04,  5.4831e-04,  1.1467e-04, -8.2221e-07,\n",
       "                         2.1519e-03,  2.5111e-04, -8.6913e-04,  1.5025e-05,\n",
       "                        -9.9654e-05,  8.9026e-04, -1.3058e-04,  5.6138e-04,\n",
       "                        -1.6714e-03,  4.1807e-04, -7.4586e-04, -2.7074e-05,\n",
       "                         6.8372e-04,  5.1205e-04, -7.2724e-04, -2.4490e-04,\n",
       "                        -9.2886e-03,  4.3789e-05,  1.1541e-02, -9.4658e-04,\n",
       "                         2.0576e-04,  5.0622e-05,  4.0056e-04,  3.7952e-05,\n",
       "                        -1.0752e-04,  1.0136e-02, -1.5298e-04,  4.1788e-03,\n",
       "                         4.7308e-05, -1.7454e-04,  3.9623e-03,  6.6625e-03,\n",
       "                        -1.3312e-03,  3.4779e-05,  8.1794e-04,  2.2284e-04,\n",
       "                         4.3704e-03, -1.0466e-04,  6.3962e-05, -1.3021e-03,\n",
       "                        -3.9167e-05, -1.1947e-03,  7.6946e-03,  5.9251e-04,\n",
       "                         7.4225e-04,  1.0182e-04, -5.5978e-04, -4.5117e-04,\n",
       "                        -1.2477e-03,  4.2502e-04,  3.3012e-04,  3.6531e-03,\n",
       "                        -6.7888e-04, -1.0747e-03, -1.0696e-03,  1.5497e-03,\n",
       "                        -3.1967e-04, -5.0002e-05, -1.2240e-05,  2.5283e-02,\n",
       "                         1.9657e-04,  8.1487e-04,  5.7958e-04,  1.0464e-05,\n",
       "                         4.4854e-04,  8.7121e-03,  6.4113e-04, -1.0471e-01,\n",
       "                        -7.3354e-05, -2.2946e-03,  9.4518e-04,  3.8997e-04,\n",
       "                         2.1524e-04, -8.0228e-05,  2.5239e-05,  1.9547e-04,\n",
       "                        -3.2664e-04,  4.6705e-05,  7.8245e-06, -1.0432e-04,\n",
       "                         2.8181e-05, -1.8087e-03, -7.4702e-04,  3.8496e-04,\n",
       "                         1.9069e-03, -1.3795e-04, -2.0204e-04, -2.4291e-04,\n",
       "                         5.1841e-04,  9.8834e-04, -1.3576e-03, -4.5994e-04,\n",
       "                         1.0485e-03,  5.6041e-04,  4.8363e-04,  2.8069e-04,\n",
       "                         2.0786e-05,  4.5951e-04,  3.3662e-05,  5.4887e-04,\n",
       "                        -3.4646e-04, -1.1855e-04, -5.0496e-04, -2.0629e-04,\n",
       "                         4.8468e-04, -3.2267e-04, -1.8422e-04,  2.3604e-04,\n",
       "                        -1.7786e-04, -1.4638e-04, -1.5808e-05, -1.4810e-04,\n",
       "                         4.9554e-03, -4.6654e-04,  5.4910e-02,  1.3400e-04,\n",
       "                         5.2820e-02, -2.2229e-03, -6.9706e-04, -8.9367e-05,\n",
       "                        -5.0972e-04, -1.1458e-03, -1.2210e-02, -4.5684e-04,\n",
       "                        -8.3223e-04,  1.0853e-03, -1.8666e-03,  5.8566e-03,\n",
       "                         2.6023e-03,  9.1597e-05, -1.3751e-03, -1.1321e-03,\n",
       "                        -4.8369e-04, -9.9959e-04,  1.0190e-04,  2.0029e-04,\n",
       "                        -1.5963e-03,  1.1338e-03, -2.8073e-03,  2.2175e-03,\n",
       "                         5.3989e-05, -4.0654e-03,  6.0648e-05, -1.9253e-02,\n",
       "                         1.1119e-04,  1.7308e-04, -1.2061e-03,  3.3778e-04,\n",
       "                        -4.2696e-03, -4.6465e-04, -2.5387e-03, -3.5600e-04,\n",
       "                         2.1479e-04, -1.7110e-03, -1.1132e-03,  4.3232e-04,\n",
       "                        -1.9569e-04, -1.9013e-03,  2.4178e-02,  2.3414e-03,\n",
       "                         1.5638e-03,  1.0774e-01,  1.8521e-03, -8.7144e-03,\n",
       "                         1.0981e-04,  1.5130e-03, -1.6222e-02,  1.1634e-03,\n",
       "                        -9.7475e-03, -4.4492e-03,  2.8189e-03,  2.6024e-03,\n",
       "                         1.5465e-03,  2.1397e-03,  7.8843e-06,  1.4775e-02,\n",
       "                         1.1144e-03, -2.4460e-03, -5.9881e-04,  3.8191e-04,\n",
       "                         1.5840e-03,  7.1995e-03,  2.3563e-03,  4.8077e-04,\n",
       "                        -7.7656e-05, -1.4667e-03, -1.2028e-03,  5.0733e-05,\n",
       "                        -1.0263e-03,  3.8822e-04, -4.2964e-04, -3.1806e-04,\n",
       "                        -3.0583e-04,  2.1688e-03,  1.5532e-04,  2.1537e-02,\n",
       "                         1.0753e-03,  3.2639e-04,  1.7270e-03,  6.7972e-03,\n",
       "                         8.9465e-04,  2.4552e-04, -1.0538e-03, -1.7074e-03,\n",
       "                         8.3786e-04,  3.8720e-04,  4.0721e-04, -1.9374e-03,\n",
       "                         9.0747e-04, -2.9058e-03, -2.7587e-03, -5.1656e-03,\n",
       "                         1.0527e-03,  9.9045e-04, -1.7044e-03, -1.5395e-04,\n",
       "                         4.0990e-04,  7.0762e-04,  1.9083e-03, -8.7020e-05,\n",
       "                        -8.7007e-04, -1.4261e-03, -1.4348e-05, -1.9931e-03,\n",
       "                         1.0664e-03,  6.0262e-04, -8.0474e-04,  2.0405e-03,\n",
       "                         6.6770e-04, -3.9520e-04,  2.4024e-04,  3.7339e-04,\n",
       "                         5.2521e-03, -1.2928e-03, -4.5356e-05, -5.7402e-04,\n",
       "                         1.6691e-03,  3.7877e-06, -2.8947e-03, -1.3438e-04,\n",
       "                         2.1093e-03, -6.7193e-04, -1.2403e-04,  6.6750e-04,\n",
       "                         4.2786e-04, -1.0769e-03, -5.7136e-04, -2.0772e-03,\n",
       "                         2.1865e-04,  5.1967e-04, -1.2135e-03,  6.3463e-03,\n",
       "                        -6.5188e-06, -7.1353e-05,  3.7910e-03,  4.2664e-03,\n",
       "                         4.0400e-03, -1.3185e-03, -1.1431e-03, -1.6754e-04,\n",
       "                        -1.2118e-03, -5.9084e-03, -7.8861e-04, -5.1499e-04,\n",
       "                         4.1081e-04,  7.1415e-03, -7.7135e-05,  3.3583e-03,\n",
       "                        -8.0205e-06,  3.9348e-05, -7.2433e-04, -1.6065e-04,\n",
       "                         1.1112e-04,  4.1797e-04, -2.5058e-04, -2.4220e-02,\n",
       "                        -1.7505e-04, -2.9101e-05, -1.9774e-03, -1.0735e-04,\n",
       "                         1.7837e-04, -4.0085e-04, -6.9055e-04,  2.6920e-04,\n",
       "                         7.4100e-03,  1.0074e-03, -1.6038e-03, -1.0727e-04,\n",
       "                         2.1272e-03, -2.2416e-03, -2.4740e-04,  8.2184e-04,\n",
       "                         3.4588e-04, -8.6120e-04,  2.7628e-02, -1.9130e-03,\n",
       "                         2.9872e-04, -1.1991e-04,  1.9197e-03,  1.1833e-03,\n",
       "                        -1.0249e-03, -2.2903e-04, -1.7034e-05,  7.1867e-05,\n",
       "                         3.2496e-03,  2.6557e-03,  5.6190e-04,  1.2128e-04,\n",
       "                         1.3515e-03,  3.4453e-04,  3.6391e-04,  1.4523e-04,\n",
       "                         3.6137e-04,  5.8505e-04, -1.8716e-04,  7.9906e-04,\n",
       "                        -3.9146e-04, -3.2583e-04, -8.9158e-05,  3.5259e-04,\n",
       "                         9.9001e-04,  2.4475e-05, -3.7641e-05,  7.4760e-04,\n",
       "                         5.8197e-04,  3.7013e-04,  4.9148e-04, -3.0500e-04,\n",
       "                        -7.5787e-04, -5.2192e-04,  8.8025e-04,  1.1323e-04,\n",
       "                        -4.4429e-05,  7.8095e-05,  5.9688e-04,  5.2051e-04,\n",
       "                        -7.7497e-05, -1.8542e-04,  7.2912e-07, -1.2049e-04,\n",
       "                        -8.0906e-05,  2.1532e-04, -1.4558e-04,  3.8901e-04,\n",
       "                         5.6055e-04,  7.0717e-04,  5.9841e-06,  3.8818e-04,\n",
       "                         5.2421e-04,  3.0320e-04, -1.5564e-04,  1.8523e-04,\n",
       "                         8.5313e-03,  2.1693e-02,  9.5606e-02, -1.4684e-03,\n",
       "                        -6.5568e-04,  9.8718e-04,  1.2629e-03,  6.6058e-03,\n",
       "                        -2.1404e-03, -3.9802e-03,  8.4939e-04,  8.4427e-06,\n",
       "                        -4.8918e-04,  7.2807e-04,  2.9773e-03, -1.2136e-03,\n",
       "                         9.0740e-05,  4.2126e-03,  5.8759e-04,  2.6661e-03,\n",
       "                        -9.9858e-04,  7.8164e-05,  1.5720e-03, -1.0648e-02,\n",
       "                        -1.0414e-03,  3.7459e-04,  3.7203e-03,  2.3347e-04,\n",
       "                         2.3024e-03,  2.8858e-05,  2.5433e-03, -3.5280e-03,\n",
       "                         6.0424e-02, -1.2612e-04, -3.9696e-03,  3.2933e-05,\n",
       "                        -1.0633e-03,  2.0419e-04, -1.6093e-03,  1.8903e-02,\n",
       "                        -5.6530e-04,  9.6659e-03,  3.9260e-04,  2.6611e-03,\n",
       "                        -1.3544e-03, -8.9256e-04, -3.2710e-03, -7.8100e-05,\n",
       "                        -7.2472e-04, -8.9330e-04, -7.2415e-04, -4.4457e-04,\n",
       "                        -2.2592e-04, -7.3588e-04, -7.9074e-04, -3.1347e-04,\n",
       "                        -5.8153e-04, -3.2539e-03, -1.2799e-03,  1.9697e-03,\n",
       "                        -3.4076e-04,  5.0445e-03,  4.3032e-04, -1.3126e-02,\n",
       "                        -3.5516e-04,  1.9259e-04,  2.7333e-03, -1.8672e-03,\n",
       "                         1.2339e-03,  9.8730e-04,  3.7928e-04, -1.4621e-04,\n",
       "                         3.6727e-03,  2.1116e-03,  2.8604e-04,  2.2765e-04,\n",
       "                         9.0174e-04,  6.5565e-04,  6.5936e-03,  1.2093e-04,\n",
       "                         5.0634e-04,  1.9227e-04, -2.2082e-03, -8.0509e-04,\n",
       "                        -4.2880e-04,  9.9724e-04,  1.4401e-03, -5.7162e-03,\n",
       "                        -5.3518e-04, -2.2308e-04,  1.3942e-03, -3.6777e-04,\n",
       "                         1.4926e-04,  9.2799e-05, -1.9109e-04,  2.4671e-04,\n",
       "                         1.6891e-03, -1.2049e-03,  1.2540e-03,  2.2708e-03,\n",
       "                         5.6228e-03, -2.8048e-04,  4.4707e-03, -1.4770e-03,\n",
       "                         1.1889e-02,  9.0083e-04, -5.8468e-04, -4.6755e-03,\n",
       "                        -1.6114e-03,  8.7183e-04, -1.1746e-03,  1.3891e-03,\n",
       "                         1.7718e-03, -1.6719e-03,  1.2111e-03, -7.6451e-05,\n",
       "                        -6.2995e-04,  1.0300e-03,  1.0507e-02, -3.7321e-04,\n",
       "                        -3.3676e-04,  1.5348e-03, -1.7566e-04, -4.1578e-04,\n",
       "                         5.1292e-04,  5.0099e-05,  4.3754e-04,  2.7961e-04,\n",
       "                        -9.1588e-04,  1.5234e-03, -5.3424e-04,  1.3292e-03,\n",
       "                         1.5865e-04, -1.1832e-03, -1.4223e-04, -3.7356e-04,\n",
       "                         7.6144e-03]),\n",
       "         device='cuda:0', size=(8193, 8193), nnz=933, layout=torch.sparse_coo),\n",
       "  'resid_3': tensor(indices=tensor([[ 398,  398,  398,  ..., 8192, 8192, 8192],\n",
       "                         [  64,   84,  206,  ..., 8071, 8100, 8192]]),\n",
       "         values=tensor([ 2.8492e-05,  8.3145e-05,  1.2246e-03,  ...,\n",
       "                        -1.6045e-04,  1.0258e-02,  2.5382e-02]),\n",
       "         device='cuda:0', size=(8193, 8193), nnz=2785, layout=torch.sparse_coo)},\n",
       " 'mlp_2': {'resid_2': tensor(indices=tensor([[  84,   84,   84,  ..., 8192, 8192, 8192],\n",
       "                         [   3,   12,   27,  ..., 8159, 8184, 8192]]),\n",
       "         values=tensor([-0.0002, -0.0019,  0.0020,  ..., -0.0301, -0.0107,\n",
       "                        -0.1439]),\n",
       "         device='cuda:0', size=(8193, 8193), nnz=3521, layout=torch.sparse_coo)},\n",
       " 'attn_2': {'resid_2': tensor(indices=tensor([[  84,   84,   84,  ..., 8192, 8192, 8192],\n",
       "                         [  20,   37,   53,  ..., 8153, 8159, 8192]]),\n",
       "         values=tensor([-0.0019, -0.0011,  0.0002,  ...,  0.0046,  0.0036,\n",
       "                        -0.0269]),\n",
       "         device='cuda:0', size=(8193, 8193), nnz=2697, layout=torch.sparse_coo)},\n",
       " 'resid_1': {'mlp_2': tensor(indices=tensor([[  27,   27,   27,  ..., 8192, 8192, 8192],\n",
       "                         [ 245,  489,  820,  ..., 8008, 8160, 8192]]),\n",
       "         values=tensor([-1.0190e-04, -5.8158e-04, -1.3446e-04,  ...,\n",
       "                         5.9687e-04,  3.2067e-03,  2.5732e-01]),\n",
       "         device='cuda:0', size=(8193, 8193), nnz=1703, layout=torch.sparse_coo),\n",
       "  'attn_2': tensor(indices=tensor([[ 973,  973,  973,  ..., 8192, 8192, 8192],\n",
       "                         [ 158,  165,  190,  ..., 8008, 8160, 8192]]),\n",
       "         values=tensor([-2.0492e-04,  2.6416e-04, -7.4760e-04, -1.2776e-03,\n",
       "                        -9.2470e-04, -8.1801e-05, -1.8135e-04, -1.2295e-03,\n",
       "                         6.9325e-04,  1.6682e-02, -1.3735e-03,  4.4196e-04,\n",
       "                        -8.9801e-04, -1.2822e-04, -3.0082e-04, -1.0887e-03,\n",
       "                        -9.5571e-05, -3.6155e-03, -5.2912e-04, -1.5598e-04,\n",
       "                         2.8410e-04,  7.7843e-04,  5.8341e-03, -5.6627e-04,\n",
       "                        -7.8278e-05,  4.5082e-04,  1.8406e-03,  1.9101e-03,\n",
       "                         8.6088e-04, -7.3879e-03, -1.4860e-03, -2.3125e-03,\n",
       "                        -2.1191e-04, -2.7478e-02,  1.5036e-03,  9.9456e-04,\n",
       "                        -2.2562e-03,  3.4148e-04, -2.7193e-03, -2.1011e-03,\n",
       "                        -1.7871e-04, -1.6838e-04, -2.8096e-03, -4.4511e-04,\n",
       "                        -1.4543e-03,  4.7735e-05,  3.6409e-03, -2.7069e-04,\n",
       "                        -8.7244e-05,  2.8804e-03,  1.0253e-03,  3.2337e-04,\n",
       "                         3.4132e-03,  2.3875e-03,  3.2573e-02, -2.7338e-04,\n",
       "                         1.7622e-03, -8.1210e-05, -1.2243e-03, -6.0975e-06,\n",
       "                        -8.1560e-03, -3.7550e-05, -8.7204e-02,  3.9870e-04,\n",
       "                        -3.8015e-04,  3.3896e-04, -1.5157e-02,  1.2645e-03,\n",
       "                        -8.9097e-04,  1.7901e-03, -2.4247e-03,  1.2323e-03,\n",
       "                        -1.8408e-03, -3.5658e-04, -3.0824e-03, -8.8045e-04,\n",
       "                         8.7608e-05, -1.5588e-04,  1.0062e-04, -5.1347e-04,\n",
       "                        -2.1180e-03, -2.5442e-03,  1.3730e-02,  1.7166e-04,\n",
       "                        -9.6093e-05, -1.6962e-03, -2.1670e-05, -1.0500e-03,\n",
       "                        -4.8010e-04,  2.0827e-04, -1.4844e-03, -3.2030e-02,\n",
       "                        -1.6166e-04,  2.8182e-04, -2.7390e-04,  3.8428e-03,\n",
       "                        -2.3555e-03,  2.6992e-03, -2.1005e-03,  2.3595e-03,\n",
       "                        -3.2172e-03,  2.9920e-04,  4.2754e-05,  1.6233e-02,\n",
       "                         3.5814e-04, -7.1612e-02, -2.9098e-04,  3.0820e-04,\n",
       "                         5.2693e-04,  3.4393e-04,  3.6768e-04,  6.3878e-04,\n",
       "                        -8.3418e-04, -4.5690e-05,  5.0062e-04, -1.5425e-03,\n",
       "                        -1.3751e-05, -3.6431e-03, -6.2602e-04,  1.6741e-04,\n",
       "                         3.3087e-03,  1.9253e-04,  2.0879e-03, -8.3414e-02,\n",
       "                         1.2000e-04,  8.3141e-04, -5.0720e-04,  7.8672e-04,\n",
       "                        -1.7686e-04, -4.7990e-03,  1.5640e-04, -2.3111e-05,\n",
       "                         3.0884e-03, -2.4802e-04,  2.3530e-03,  9.3974e-04,\n",
       "                        -3.4857e-03,  4.1415e-04, -1.2809e-04,  3.0066e-03,\n",
       "                         2.9291e-04, -1.4481e-03, -9.9150e-04, -2.1960e-03,\n",
       "                         6.7780e-04, -4.5462e-03,  1.8569e-03,  3.0838e-04,\n",
       "                         7.1267e-03,  5.9410e-04, -2.7713e-04,  8.7150e-04,\n",
       "                        -2.1523e-04,  1.1596e-03,  9.1141e-05,  2.3358e-03,\n",
       "                         5.0284e-04,  1.1549e-03, -2.7651e-04,  3.6424e-07,\n",
       "                        -3.4837e-04, -9.0404e-05, -2.5057e-03,  9.6782e-04,\n",
       "                         1.5195e-03,  2.2450e-04, -5.3110e-04, -5.1144e-04,\n",
       "                         2.8355e-04, -1.6316e-05, -2.2431e-05,  6.3720e-04,\n",
       "                        -8.8641e-05,  1.5367e-02, -2.7057e-04,  4.6631e-04,\n",
       "                        -1.6167e-03, -2.9666e-04,  6.9544e-04, -5.7116e-05,\n",
       "                        -4.0027e-04,  1.5789e-03,  8.7724e-04, -1.9895e-03,\n",
       "                         6.2023e-04,  3.9514e-03, -9.1871e-03,  3.8390e-05,\n",
       "                         1.8345e-04, -6.2684e-04, -4.0032e-04, -1.1828e-04,\n",
       "                        -1.2622e-03,  6.2267e-05, -3.0407e-03,  4.2791e-05,\n",
       "                        -2.2973e-03,  4.7754e-04, -3.5462e-04, -2.4038e-04,\n",
       "                         1.8888e-04,  9.1248e-04, -9.2625e-04, -3.4152e-04,\n",
       "                         5.8308e-05, -2.7429e-04, -2.5510e-04, -4.6975e-04,\n",
       "                        -6.0749e-04,  1.4105e-05,  2.1745e-05,  4.9593e-05,\n",
       "                        -3.0576e-04, -2.9633e-05,  2.7993e-04,  2.5075e-03,\n",
       "                        -1.6236e-03, -9.9420e-04, -7.3221e-04, -5.6653e-05,\n",
       "                         4.6614e-05,  3.6498e-04, -1.2009e-03, -8.0420e-03,\n",
       "                         4.5035e-04,  7.2906e-04,  1.9606e-03,  6.0559e-04,\n",
       "                        -1.6767e-03,  2.6129e-04, -8.0374e-04, -9.7186e-05,\n",
       "                        -4.3927e-03, -2.1555e-03,  9.5092e-04,  1.0455e-03,\n",
       "                        -1.5475e-03, -3.9843e-04,  5.2016e-03,  1.1743e-03,\n",
       "                        -2.5310e-04, -3.6414e-04,  1.4461e-03,  4.1894e-03,\n",
       "                         4.9884e-04,  2.8092e-04, -7.5150e-04, -1.8616e-03,\n",
       "                         1.0996e-03, -1.6260e-03,  1.1898e-03, -2.6536e-04,\n",
       "                         5.5285e-03, -8.7119e-05,  1.2712e-05,  5.4329e-04,\n",
       "                         3.7196e-03,  1.1161e-03, -1.3283e-05,  1.9398e-02,\n",
       "                         3.0042e-03,  1.2516e-04, -4.1624e-04, -1.7157e-04,\n",
       "                         3.8101e-05, -7.1094e-04, -6.3945e-05, -8.3177e-04,\n",
       "                        -1.7518e-04, -7.4911e-05,  4.4157e-05,  3.4942e-04,\n",
       "                         8.3038e-05, -1.5650e-04,  5.7080e-04, -7.5841e-04,\n",
       "                         3.9549e-04, -1.1352e-03,  3.9472e-02, -8.7330e-05,\n",
       "                        -9.5829e-04, -2.6614e-04,  3.1592e-03,  1.6742e-03,\n",
       "                        -3.3146e-03,  1.7615e-03, -1.2706e-03, -4.8790e-03,\n",
       "                         1.4466e-04, -4.8465e-03, -4.1776e-04,  3.4988e-04,\n",
       "                        -5.9917e-04,  1.2318e-03, -1.5939e-04, -2.3691e-05,\n",
       "                        -9.2248e-05,  9.2395e-04, -4.5164e-04,  3.4581e-05,\n",
       "                         1.0779e-03,  5.6859e-04, -1.0094e-03, -6.5020e-04,\n",
       "                         1.4542e-03,  1.2629e-03, -5.3050e-04,  6.9431e-04,\n",
       "                         2.8842e-04,  1.4110e-03,  8.5479e-04,  2.3113e-03,\n",
       "                        -2.2715e-03, -1.9025e-04, -1.8094e-03, -3.7241e-04,\n",
       "                         1.5825e-03,  1.3941e-05, -1.6345e-03,  1.1836e-03,\n",
       "                         6.9598e-03,  1.1937e-05,  1.7075e-03,  5.4258e-04,\n",
       "                         1.5298e-02,  6.7098e-05, -1.2277e-03,  1.9553e-03,\n",
       "                        -1.3266e-03,  1.3917e-03,  3.2301e-04, -9.6969e-04,\n",
       "                        -5.2311e-04,  5.5501e-04, -2.3440e-05, -2.0122e-05,\n",
       "                         1.1934e-03,  4.9363e-04,  1.7256e-03, -2.5463e-04,\n",
       "                        -3.2531e-04,  1.6313e-02,  2.1169e-03, -3.9261e-04,\n",
       "                         2.5181e-04,  4.3835e-04,  1.7470e-03,  6.5162e-05,\n",
       "                         1.1382e-03,  5.5452e-04,  5.7322e-04, -1.8420e-03,\n",
       "                         4.9735e-04,  9.2039e-04,  3.1785e-03, -3.5319e-03,\n",
       "                        -7.9642e-06, -3.6852e-04, -1.6068e-03,  1.0050e-03,\n",
       "                        -1.3030e-04,  3.2646e-03, -1.5222e-04, -1.9304e-03,\n",
       "                         2.5616e-03, -2.2765e-03,  4.1554e-04,  1.4135e-03,\n",
       "                        -1.1372e-04,  1.2689e-04, -1.2219e-03,  1.1237e-03,\n",
       "                         2.2371e-04,  4.8923e-03,  1.8807e-05,  2.0993e-03,\n",
       "                         8.0884e-04, -2.6549e-04,  7.3277e-04, -3.9610e-04,\n",
       "                        -1.7647e-04,  1.4342e-03,  1.2700e-04,  1.0293e-03,\n",
       "                        -5.9603e-04,  4.4021e-04, -7.8101e-04,  2.5938e-04,\n",
       "                        -2.4797e-03,  2.4603e-04, -1.9323e-04,  9.3543e-04,\n",
       "                        -4.3845e-04,  2.7510e-04, -2.1368e-04, -1.8465e-04,\n",
       "                         1.0279e-04, -8.6769e-04, -1.9570e-03,  5.4396e-05,\n",
       "                        -6.0492e-04, -2.6822e-04, -6.5148e-06,  4.6063e-05,\n",
       "                        -9.1926e-04, -6.4574e-05, -3.4531e-05, -3.4040e-04,\n",
       "                        -2.7147e-04, -4.6631e-04, -1.4910e-04, -1.0397e-03,\n",
       "                        -2.9252e-04, -4.0042e-04,  2.8769e-04,  4.0972e-04,\n",
       "                         2.5278e-04, -7.8110e-04,  8.9835e-05, -1.5254e-04,\n",
       "                         2.8947e-04, -4.1145e-04,  5.6217e-04, -1.6007e-04,\n",
       "                         2.9194e-05,  3.1552e-04, -8.3778e-06, -7.2127e-05,\n",
       "                        -4.6140e-05, -7.4810e-05, -1.9598e-03, -3.4325e-04,\n",
       "                        -2.4887e-04, -2.5312e-04,  5.3961e-04, -1.7261e-05,\n",
       "                         1.8986e-04, -5.3188e-05, -5.1156e-03,  7.1420e-04,\n",
       "                         1.6299e-03,  4.5205e-03,  1.0697e-03, -2.9052e-04,\n",
       "                        -3.8622e-04, -1.4839e-03,  1.6392e-03,  6.7645e-04,\n",
       "                         3.7189e-03,  4.5863e-03,  5.9171e-04, -2.8465e-03,\n",
       "                         4.7181e-04,  1.3308e-03,  2.2812e-03,  1.3983e-03,\n",
       "                         3.5250e-03, -2.9542e-04,  2.7557e-02, -8.9628e-04,\n",
       "                        -3.4628e-03,  3.8462e-04, -2.5177e-03,  1.2429e-03,\n",
       "                        -2.9536e-04,  9.6930e-04,  2.9070e-03,  2.2736e-03,\n",
       "                         3.9347e-03,  4.3946e-03,  3.2499e-03, -6.8672e-04,\n",
       "                        -7.1359e-05, -2.8209e-03, -4.8232e-03,  9.3650e-04,\n",
       "                        -1.3970e-03, -7.3541e-03,  6.1835e-03,  7.9638e-04,\n",
       "                        -1.2282e-03, -1.1681e-05,  1.3460e-02, -4.3896e-03,\n",
       "                         2.3724e-03,  1.6265e-03,  1.5040e-02, -1.5090e-03,\n",
       "                        -8.0013e-04, -1.2435e-04,  8.7647e-04,  2.0699e-02,\n",
       "                         6.9165e-02, -1.0904e-03, -2.4060e-04,  2.4138e-04,\n",
       "                         7.3031e-04, -1.3080e-05,  4.1898e-05, -1.7298e-03,\n",
       "                         1.4282e-03, -1.2075e-04, -5.5385e-03, -3.4238e-03,\n",
       "                        -2.2820e-03, -8.5550e-04, -3.6808e-04,  1.0468e-03,\n",
       "                         8.8569e-03,  2.2873e-03,  5.1001e-04, -7.2723e-04,\n",
       "                         5.9530e-04,  3.6371e-04, -1.8429e-03,  8.1494e-04,\n",
       "                        -9.2555e-05, -4.0651e-04,  3.9137e-03,  1.9059e-03,\n",
       "                         2.6816e-03, -7.4914e-02,  3.5006e-03, -2.9404e-03,\n",
       "                         9.3275e-04, -4.5744e-05, -6.1342e-03,  4.3511e-03,\n",
       "                        -1.0053e-03,  7.3945e-02,  4.3129e-04,  1.6938e-02,\n",
       "                        -2.1162e-03,  2.4444e-03,  2.5353e-03, -3.3602e-02,\n",
       "                         4.6748e-03, -3.0873e-02,  3.6830e-03,  2.1396e-03,\n",
       "                        -9.2241e-04,  5.7308e-04,  2.3360e-03,  1.2689e-03,\n",
       "                        -8.2615e-04,  5.5579e-03, -7.7297e-04, -2.5756e-03,\n",
       "                         4.2241e-03,  2.1751e-03, -2.1644e-03, -2.0328e-03,\n",
       "                         8.9023e-04,  3.5389e-03, -4.5620e-04, -1.4096e-03,\n",
       "                         1.7053e-03,  3.1720e-03, -1.7534e-03, -9.5940e-05,\n",
       "                         6.6836e-04,  3.6006e-02,  4.1371e-03, -5.1234e-04,\n",
       "                         1.2154e-03, -1.4991e-03,  9.4390e-04,  2.2579e-04,\n",
       "                         8.5447e-04, -3.3096e-03,  1.4609e-03,  3.8269e-03,\n",
       "                        -2.8642e-03, -5.0997e-04,  2.2876e-03,  2.3394e-03,\n",
       "                         2.3720e-03,  2.2920e-03,  5.2803e-04, -1.2304e-03,\n",
       "                        -1.5913e-02, -3.1064e-04, -8.9351e-03, -1.5334e-03,\n",
       "                        -1.0964e-03,  4.1613e-04, -3.0352e-05, -2.7117e-03,\n",
       "                        -9.5651e-03, -1.1538e-03, -9.1301e-03, -3.5667e-04,\n",
       "                         5.2170e-03, -2.9644e-03, -4.4139e-03, -8.7152e-04,\n",
       "                        -1.6417e-03,  7.5612e-04,  9.0647e-04, -4.9610e-04,\n",
       "                         9.2778e-04,  5.5779e-03, -2.1354e-03,  5.1858e-04,\n",
       "                        -1.2813e-03, -1.3646e-03,  4.0419e-03,  5.8117e-05,\n",
       "                         8.2611e-04,  1.3332e-03,  3.0364e-03,  2.7279e-04,\n",
       "                        -2.7793e-04, -1.7911e-03,  2.2904e-03, -2.0687e-04,\n",
       "                         3.8780e-03,  4.0360e-02]),\n",
       "         device='cuda:0', size=(8193, 8193), nnz=618, layout=torch.sparse_coo),\n",
       "  'resid_2': tensor(indices=tensor([[  84,   84,   84,  ..., 8192, 8192, 8192],\n",
       "                         [   0,   54,  156,  ..., 8159, 8160, 8192]]),\n",
       "         values=tensor([ 1.0659e-03,  6.4445e-04, -4.0186e-05,  ...,\n",
       "                         8.3203e-03, -2.2214e-03,  8.5306e-02]),\n",
       "         device='cuda:0', size=(8193, 8193), nnz=3156, layout=torch.sparse_coo)},\n",
       " 'mlp_1': {'resid_1': tensor(indices=tensor([[ 190,  190,  190,  ..., 8192, 8192, 8192],\n",
       "                         [ 191,  425,  609,  ..., 8176, 8187, 8192]]),\n",
       "         values=tensor([ 1.4787e-03, -2.7860e-05,  5.2974e-05,  ...,\n",
       "                        -2.5142e-03,  2.3179e-04,  2.3465e-01]),\n",
       "         device='cuda:0', size=(8193, 8193), nnz=4426, layout=torch.sparse_coo)},\n",
       " 'attn_1': {'resid_1': tensor(indices=tensor([[ 190,  190,  190,  ..., 8192, 8192, 8192],\n",
       "                         [ 103,  176,  189,  ..., 7916, 7923, 8192]]),\n",
       "         values=tensor([-1.7376e-04, -2.5112e-04,  3.3570e-05,  ...,\n",
       "                        -7.6392e-04, -3.6484e-04, -2.3733e-02]),\n",
       "         device='cuda:0', size=(8193, 8193), nnz=2827, layout=torch.sparse_coo)},\n",
       " 'resid_0': {'mlp_1': tensor(indices=tensor([[ 191,  191,  191,  ..., 8192, 8192, 8192],\n",
       "                         [ 104,  234,  307,  ..., 8148, 8163, 8192]]),\n",
       "         values=tensor([-7.4999e-05,  2.2325e-04,  5.2508e-04,  ...,\n",
       "                         3.5221e-03, -7.7382e-03,  1.3083e-01]),\n",
       "         device='cuda:0', size=(8193, 8193), nnz=2129, layout=torch.sparse_coo),\n",
       "  'attn_1': tensor(indices=tensor([[1364, 1364, 1364,  ..., 8192, 8192, 8192],\n",
       "                         [ 121,  139,  147,  ..., 8148, 8163, 8192]]),\n",
       "         values=tensor([ 7.0538e-04,  7.6853e-04,  2.1097e-03, -2.8617e-03,\n",
       "                         3.1468e-03, -5.7703e-04,  1.6141e-05, -6.9683e-04,\n",
       "                         3.6558e-04,  1.0618e-03, -1.7127e-04,  7.6681e-04,\n",
       "                        -6.4650e-04,  1.5753e-03, -8.9069e-03,  9.8955e-04,\n",
       "                         5.4998e-04, -3.5195e-03,  1.5764e-03, -1.3686e-04,\n",
       "                         4.6507e-06, -2.2794e-03,  1.1793e-03,  3.3803e-03,\n",
       "                         1.6925e-04,  8.3337e-04, -2.2841e-03,  2.4501e-03,\n",
       "                        -5.5756e-04,  1.8776e-03, -8.4091e-04, -9.4953e-04,\n",
       "                        -4.2650e-04, -1.9817e-03, -3.4073e-03, -2.3134e-03,\n",
       "                         2.1891e-04,  2.2331e-03, -1.1971e-03,  4.1261e-04,\n",
       "                         1.2974e-04, -5.2997e-04, -2.2731e-03, -3.8303e-05,\n",
       "                        -3.3238e-03, -1.3467e-03,  1.0869e-03, -1.6476e-03,\n",
       "                         3.3343e-05,  2.5063e-03,  7.2660e-05, -7.4501e-04,\n",
       "                        -5.3659e-04, -7.8770e-04, -7.0112e-03,  2.1767e-04,\n",
       "                        -1.8853e-03,  8.5385e-03, -3.0464e-03, -2.4768e-03,\n",
       "                        -9.0152e-03, -1.7288e-03,  1.2274e-02, -7.9901e-05,\n",
       "                        -8.3678e-03,  1.7782e-04, -2.7393e-03, -6.9381e-03,\n",
       "                        -8.0770e-03, -5.5999e-04,  1.5155e-02, -3.3213e-03,\n",
       "                        -3.3998e-03,  8.5905e-04,  1.2351e-03,  5.6320e-03,\n",
       "                        -5.2193e-04, -3.4706e-03, -8.2291e-05,  3.9768e-03,\n",
       "                        -7.9059e-03,  1.9759e-05, -3.3597e-04,  2.8510e-03,\n",
       "                        -3.1110e-03,  7.8557e-03, -1.8623e-03, -1.2830e-04,\n",
       "                         6.3695e-04,  2.6674e-03,  2.6218e-03, -2.8166e-03,\n",
       "                        -1.1936e-02,  6.5142e-04,  2.6672e-03,  3.1517e-05,\n",
       "                         8.1298e-04,  4.2706e-04, -1.2895e-03, -2.9934e-03,\n",
       "                        -4.8626e-03, -1.5940e-03,  4.0000e-04,  2.2063e-03,\n",
       "                        -5.1901e-05, -1.7731e-03, -1.5260e-03, -1.9415e-03,\n",
       "                         3.2098e-03, -3.6983e-03, -1.5100e-02,  3.2906e-03,\n",
       "                        -4.4678e-03,  1.1129e-02,  2.6657e-04,  1.8807e-03,\n",
       "                        -3.8545e-04,  5.8194e-04,  1.0624e-03,  1.8658e-03,\n",
       "                        -2.0465e-03,  6.2335e-04, -1.0211e-02,  6.3836e-05,\n",
       "                        -1.7303e-04, -4.0609e-03,  2.4467e-03, -6.6468e-04,\n",
       "                        -5.2881e-03, -1.5300e-03, -8.4314e-04,  4.0764e-03,\n",
       "                        -2.8848e-03, -1.2989e-03,  9.0891e-04,  2.2830e-03,\n",
       "                         3.3094e-03,  2.1661e-03,  1.8767e-03, -3.9432e-04,\n",
       "                        -6.4212e-03,  1.7586e-03, -7.5657e-03, -1.4150e-04,\n",
       "                         6.8629e-03, -2.7126e-04, -9.2807e-04,  3.9416e-04,\n",
       "                        -1.2713e-03, -1.9634e-03,  9.5482e-03, -3.6456e-03,\n",
       "                        -4.0586e-03, -6.6867e-03, -1.0580e-03,  1.7065e-02,\n",
       "                         7.7297e-04,  1.9746e-04,  3.1084e-04, -2.4305e-03,\n",
       "                        -2.2005e-04, -1.0021e-03, -4.2045e-04,  9.9960e-04,\n",
       "                        -1.6647e-04,  9.5916e-04, -5.1913e-04, -4.7216e-04,\n",
       "                         9.0067e-05, -2.8308e-04, -2.6633e-03,  1.5653e-03,\n",
       "                         3.6730e-04, -9.4721e-04, -4.4461e-04, -4.3176e-03,\n",
       "                        -4.8385e-04, -2.6044e-04, -1.8143e-04, -3.8130e-03,\n",
       "                         5.4984e-04,  6.6335e-04,  1.4076e-03,  1.0827e-03,\n",
       "                         3.9056e-04, -2.3648e-03, -6.4378e-04,  1.8301e-04,\n",
       "                        -9.4735e-05, -2.4382e-03,  1.6119e-03, -7.7301e-04,\n",
       "                        -6.3352e-04, -3.1573e-05,  2.1128e-04, -1.3358e-03,\n",
       "                        -9.4518e-05, -1.0589e-03, -4.5540e-04, -2.9360e-03,\n",
       "                        -7.4054e-04, -5.3286e-05,  1.9209e-03, -1.7268e-03,\n",
       "                        -2.9199e-03, -4.8240e-04, -1.0417e-03,  3.3018e-04,\n",
       "                         5.1522e-04, -1.7706e-04,  1.3149e-03,  8.1586e-04,\n",
       "                        -1.6314e-03,  3.0831e-03, -4.8307e-05, -1.6508e-04,\n",
       "                        -1.3685e-03,  6.3967e-04,  9.2792e-05, -5.0926e-04,\n",
       "                         3.0139e-03,  6.7722e-04,  4.4632e-04, -2.5813e-04,\n",
       "                        -9.9100e-04,  1.2704e-04, -1.2676e-03, -1.1247e-03,\n",
       "                        -3.6715e-04, -5.6057e-03, -1.1421e-03,  1.2195e-04,\n",
       "                        -9.0310e-04, -9.4208e-04, -1.9753e-03,  6.0279e-04,\n",
       "                         1.3249e-04,  2.4242e-03,  6.6737e-04,  1.1945e-03,\n",
       "                         2.8809e-04, -1.4129e-03, -2.4411e-04,  8.8706e-04,\n",
       "                        -4.2752e-03,  1.1316e-03, -1.5755e-05,  7.2828e-04,\n",
       "                        -1.7880e-03,  4.7973e-04, -6.9800e-03,  7.8802e-04,\n",
       "                        -3.3998e-04, -1.0968e-02,  1.0222e-02, -1.5262e-03,\n",
       "                        -1.1463e-03, -1.0070e-03, -2.1829e-03, -7.0296e-04,\n",
       "                        -4.8440e-03, -4.2458e-03,  7.7643e-05, -2.8079e-03,\n",
       "                        -7.4837e-03,  7.6892e-03, -1.3495e-02, -2.0359e-03,\n",
       "                         4.1312e-03,  4.0245e-03, -2.1044e-03, -3.3147e-03,\n",
       "                         6.1544e-03, -3.8790e-03, -4.2614e-05, -6.3825e-04,\n",
       "                         7.7031e-03,  4.2160e-03, -5.1353e-03, -4.1293e-03,\n",
       "                         2.0274e-03, -1.1361e-04,  1.5916e-05, -1.7051e-03,\n",
       "                        -2.9506e-03, -2.2957e-03, -2.5272e-03, -2.9322e-03,\n",
       "                         6.4429e-04,  1.8629e-03,  1.4661e-03, -1.4692e-02,\n",
       "                        -1.3893e-03,  9.8153e-04,  3.2494e-03,  3.7727e-04,\n",
       "                         1.2106e-03,  2.2870e-02, -5.1101e-03, -3.1751e-03,\n",
       "                        -5.4382e-03, -1.4999e-02, -2.4033e-03,  5.7720e-03,\n",
       "                         8.0545e-04, -1.1679e-02, -4.4354e-04,  5.1976e-04,\n",
       "                         2.7313e-03, -5.6671e-04, -1.9874e-03,  4.0069e-03,\n",
       "                        -1.9936e-02, -4.8209e-03, -1.1391e-03,  1.1374e-02,\n",
       "                        -5.8624e-03,  1.3087e-02,  3.6041e-03, -6.4974e-04,\n",
       "                        -5.1960e-03, -1.6599e-02, -5.2364e-04, -5.3597e-03,\n",
       "                         2.1435e-02, -4.2170e-03,  1.8313e-04, -1.3248e-03,\n",
       "                         5.9723e-03,  2.2981e-04, -2.6549e-03, -2.1385e-02,\n",
       "                        -2.7258e-04, -8.7842e-03,  5.6978e-04, -6.7465e-03,\n",
       "                         8.9391e-04, -1.5529e-02,  2.2870e-03, -2.4117e-03,\n",
       "                        -1.2820e-02, -3.8084e-04,  6.7825e-04,  2.7159e-03,\n",
       "                         7.9868e-03, -3.8029e-05, -1.9645e-03,  1.1618e-03,\n",
       "                         1.8022e-03, -3.0737e-03,  2.0641e-03,  2.5355e-03,\n",
       "                        -5.4766e-04,  2.8057e-03, -4.0187e-03, -2.6181e-04,\n",
       "                        -1.6108e-03,  1.0998e-03,  7.0555e-04, -5.2669e-03,\n",
       "                         2.9859e-04, -4.0376e-03,  4.9821e-03, -2.6940e-03,\n",
       "                        -1.8729e-03,  3.7939e-03, -7.5720e-04,  1.1474e-03,\n",
       "                         6.0149e-03, -1.9413e-03, -6.2233e-03,  9.1123e-06,\n",
       "                         6.4748e-06, -2.2264e-03,  1.1666e-03,  2.4009e-03,\n",
       "                        -1.3042e-02,  9.2790e-03, -2.2092e-03,  3.4180e-04,\n",
       "                         2.0520e-03, -1.6503e-02,  5.5468e-03,  9.3338e-04,\n",
       "                        -3.6701e-03, -2.8476e-03,  1.8002e-03, -2.5021e-03,\n",
       "                        -1.2375e-03,  1.6075e-03, -5.9760e-03, -3.1654e-03,\n",
       "                        -4.8351e-03,  1.6174e-03,  3.1885e-03,  2.3039e-03,\n",
       "                        -1.2666e-02,  1.6951e-04,  5.1757e-03,  6.9708e-04,\n",
       "                        -1.1642e-03,  6.2921e-03,  5.3054e-04, -3.2402e-03,\n",
       "                         4.5342e-04,  2.2147e-03, -2.2317e-02,  3.4885e-03,\n",
       "                         2.8179e-03,  3.3568e-03,  5.6290e-04,  2.1423e-03,\n",
       "                        -4.4547e-03, -7.8670e-04, -1.7377e-03,  1.3059e-04,\n",
       "                         2.0489e-03, -6.4879e-04, -3.6754e-05,  1.1358e-03,\n",
       "                         8.1298e-04,  1.0555e-03,  5.3468e-04, -1.5371e-03,\n",
       "                         3.7749e-03,  2.1462e-03,  6.6607e-03,  5.1328e-03,\n",
       "                         9.9638e-03, -3.0740e-03, -2.6622e-03, -1.1270e-03,\n",
       "                         7.9722e-05,  4.2854e-04,  2.7149e-03,  1.7033e-03,\n",
       "                        -4.4106e-03, -4.9313e-03,  5.0997e-03,  4.2544e-03,\n",
       "                        -5.4334e-03, -3.1009e-02, -9.7389e-03, -3.0673e-03,\n",
       "                        -3.8447e-03,  5.3735e-03,  1.6824e-02,  5.1076e-03,\n",
       "                         6.1021e-04, -1.8240e-03,  5.0244e-03, -3.2077e-03,\n",
       "                        -1.1277e-03,  2.5376e-03, -9.8159e-03, -5.3811e-03,\n",
       "                        -1.0853e-03, -3.8392e-03, -9.7212e-04, -4.4529e-03,\n",
       "                        -3.2770e-03, -5.3645e-04,  1.7689e-03, -1.9164e-03,\n",
       "                        -1.7984e-02,  4.6443e-03,  4.1651e-03,  1.0937e-04,\n",
       "                        -3.0627e-03,  2.9573e-03,  3.8995e-04, -5.7571e-03,\n",
       "                        -3.3782e-03,  8.3592e-03,  1.7380e-03, -9.1889e-03,\n",
       "                        -3.2542e-03,  2.4407e-04,  8.4696e-03, -2.3616e-03,\n",
       "                        -2.6747e-02, -3.3677e-03,  7.5182e-03,  2.7541e-03,\n",
       "                         3.6069e-03,  1.4522e-03,  1.8044e-03, -4.6029e-03,\n",
       "                        -3.9252e-03,  3.8890e-03, -9.4684e-04,  2.7141e-03,\n",
       "                        -1.0359e-03,  1.5903e-02,  1.0364e-02, -5.3410e-04,\n",
       "                        -2.9851e-03,  1.7780e-03,  3.2932e-04,  1.1397e-03,\n",
       "                        -1.5518e-03, -1.7060e-04,  2.0014e-03, -1.6143e-03,\n",
       "                        -1.0455e-04, -7.8582e-03,  4.9564e-03, -1.2268e-03,\n",
       "                        -1.1598e-03,  1.7370e-02, -7.6421e-03, -7.6782e-06,\n",
       "                         3.0267e-03,  1.4416e-04, -5.4251e-03,  2.3445e-03,\n",
       "                        -1.3054e-02, -4.0851e-03, -1.3490e-02, -1.0486e-02,\n",
       "                        -1.5678e-03,  4.6455e-04,  4.5747e-04, -2.4342e-03,\n",
       "                         7.7775e-05, -3.8070e-03, -1.6304e-03, -4.4301e-04,\n",
       "                         6.3028e-04,  1.9135e-03, -1.7513e-03,  2.0239e-03,\n",
       "                         8.1162e-03,  2.0720e-03, -1.3926e-02, -7.9247e-04,\n",
       "                         1.9976e-02, -4.5436e-03,  1.8747e-03, -5.0753e-03,\n",
       "                        -1.3771e-02, -4.0600e-06,  2.6064e-04, -5.4729e-03,\n",
       "                        -1.3794e-02, -1.6778e-03,  8.1072e-03,  1.4738e-02,\n",
       "                        -2.8145e-03, -4.3125e-03, -2.5669e-03, -1.2467e-02,\n",
       "                        -2.9189e-03,  1.2488e-03, -4.9282e-04, -2.0963e-03,\n",
       "                         6.3756e-03,  2.1235e-03, -2.1903e-03, -1.7193e-03,\n",
       "                         1.1681e-02, -1.7938e-03, -3.8754e-03, -8.5013e-04,\n",
       "                        -5.9237e-05,  3.2598e-03, -4.8126e-03, -3.8034e-03,\n",
       "                        -1.4811e-03, -6.4941e-03,  1.8717e-02, -9.1296e-04,\n",
       "                        -2.7470e-02, -6.6708e-03, -3.8825e-03,  1.2447e-03,\n",
       "                        -2.1547e-03,  3.5309e-03,  4.1309e-03,  1.0107e-03,\n",
       "                        -1.0232e-02,  1.0329e-02, -2.4523e-03,  8.9219e-04,\n",
       "                        -8.9183e-04,  2.5882e-03, -7.0918e-05, -1.0654e-03,\n",
       "                        -1.0406e-03,  2.4689e-02, -9.2377e-04, -5.2228e-03,\n",
       "                        -2.2545e-02, -2.9848e-02,  3.4362e-03, -1.5298e-03,\n",
       "                        -4.7713e-03, -5.9135e-04, -3.2791e-03,  1.4225e-02,\n",
       "                        -3.7731e-03, -9.1741e-03, -2.9791e-04, -2.2447e-03,\n",
       "                        -6.5731e-04,  2.6983e-02]),\n",
       "         device='cuda:0', size=(8193, 8193), nnz=606, layout=torch.sparse_coo),\n",
       "  'resid_1': tensor(indices=tensor([[ 190,  190,  190,  ..., 8192, 8192, 8192],\n",
       "                         [  99,  113,  150,  ..., 8148, 8163, 8192]]),\n",
       "         values=tensor([-0.0008,  0.0008,  0.0003,  ..., -0.0040,  0.0028,\n",
       "                         0.1213]),\n",
       "         device='cuda:0', size=(8193, 8193), nnz=4417, layout=torch.sparse_coo)},\n",
       " 'mlp_0': {'resid_0': tensor(indices=tensor([[  48,   48,   48,  ..., 8192, 8192, 8192],\n",
       "                         [ 431,  449,  687,  ..., 8157, 8167, 8192]]),\n",
       "         values=tensor([-4.5256e-05, -4.1829e-05,  1.7142e-03,  ...,\n",
       "                         3.5831e-03,  1.4061e-02,  5.9327e-01]),\n",
       "         device='cuda:0', size=(8193, 8193), nnz=10428, layout=torch.sparse_coo)},\n",
       " 'attn_0': {'resid_0': tensor(indices=tensor([[  48,   48,   48,  ..., 8192, 8192, 8192],\n",
       "                         [  36,   64,  693,  ..., 8141, 8182, 8192]]),\n",
       "         values=tensor([-1.6418e-04, -6.6240e-05, -9.4461e-05,  ...,\n",
       "                         3.3422e-03,  3.3134e-04, -7.0094e-02]),\n",
       "         device='cuda:0', size=(8193, 8193), nnz=8798, layout=torch.sparse_coo)},\n",
       " 'embed': {'mlp_0': tensor(indices=tensor([[  34,   34,   34,  ..., 8192, 8192, 8192],\n",
       "                         [ 913, 2837, 4170,  ..., 8093, 8150, 8192]]),\n",
       "         values=tensor([ 0.0055, -0.0069,  0.0080,  ..., -0.0028,  0.0244,\n",
       "                        -0.3697]),\n",
       "         device='cuda:0', size=(8193, 8193), nnz=2318, layout=torch.sparse_coo),\n",
       "  'attn_0': tensor(indices=tensor([[2407, 2407, 2407, 2407, 2407, 2407, 2407, 2407, 2407,\n",
       "                          2407, 2407, 2764, 2764, 2764, 2764, 2764, 2764, 2764,\n",
       "                          2764, 2764, 2764, 2764, 2764, 2764, 2764, 2764, 2764,\n",
       "                          2764, 2764, 2764, 2764, 2764, 2891, 2891, 2891, 2891,\n",
       "                          2891, 2891, 2891, 2891, 2891, 2891, 2891, 3514, 3514,\n",
       "                          3514, 3514, 3514, 3514, 3514, 3514, 3514, 3514, 3514,\n",
       "                          3718, 3718, 3718, 3718, 3718, 3718, 3718, 3718, 3718,\n",
       "                          3718, 3718, 5108, 5108, 5108, 5108, 5108, 5108, 5108,\n",
       "                          5108, 5108, 5108, 5108, 6816, 6816, 6816, 6816, 6816,\n",
       "                          6816, 6816, 6816, 6816, 6816, 6816, 7929, 7929, 7929,\n",
       "                          7929, 7929, 7929, 7929, 7929, 7929, 7929, 7929, 8192,\n",
       "                          8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192,\n",
       "                          8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192,\n",
       "                          8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192,\n",
       "                          8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192,\n",
       "                          8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192,\n",
       "                          8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192,\n",
       "                          8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192,\n",
       "                          8192, 8192, 8192, 8192, 8192, 8192],\n",
       "                         [ 913, 2837, 4170, 4388, 4673, 4730, 4779, 4927, 5947,\n",
       "                          7672, 8192,  478, 2036, 2051, 2055, 2202, 2827, 3650,\n",
       "                          3656, 3691, 4275, 4704, 4860, 5698, 6243, 6462, 6551,\n",
       "                          6954, 7058, 7905, 8093, 8192,  498,  690, 3868, 5550,\n",
       "                          6254, 6791, 6965, 7728, 7867, 8001, 8192,  913, 2837,\n",
       "                          4170, 4388, 4673, 4730, 4779, 4927, 5947, 7672, 8192,\n",
       "                           354, 1860, 3388, 4001, 5021, 6726, 6795, 7118, 7177,\n",
       "                          8150, 8192, 2682, 2947, 4281, 5475, 6524, 6619, 6840,\n",
       "                          6948, 7099, 7858, 8192,  354, 1860, 3388, 4001, 5021,\n",
       "                          6726, 6795, 7118, 7177, 8150, 8192, 2682, 2947, 4281,\n",
       "                          5475, 6524, 6619, 6840, 6948, 7099, 7858, 8192,  166,\n",
       "                           253,  354,  456,  470,  478,  498,  690,  812,  818,\n",
       "                          1220, 1410, 1860, 1884, 2036, 2051, 2055, 2202, 2366,\n",
       "                          2581, 2827, 3388, 3650, 3656, 3688, 3691, 3783, 3868,\n",
       "                          4001, 4236, 4275, 4374, 4635, 4704, 4860, 4960, 5021,\n",
       "                          5192, 5322, 5402, 5550, 5698, 5731, 5744, 5966, 6053,\n",
       "                          6243, 6254, 6401, 6462, 6551, 6726, 6730, 6791, 6795,\n",
       "                          6810, 6954, 6965, 7058, 7118, 7177, 7389, 7728, 7867,\n",
       "                          7905, 7953, 8001, 8093, 8150, 8192]]),\n",
       "         values=tensor([-7.3644e-03,  7.4213e-04, -5.1734e-04,  8.5820e-03,\n",
       "                         8.3242e-04, -5.8963e-03,  1.3063e-03,  3.7683e-04,\n",
       "                        -6.7548e-03, -2.0038e-03, -4.5023e-03, -1.6457e-03,\n",
       "                         1.2274e-03, -1.8051e-03,  1.8500e-04,  2.0403e-03,\n",
       "                        -3.3015e-03, -7.2427e-04,  9.9008e-04,  1.9926e-03,\n",
       "                         4.7192e-03, -8.2722e-04,  8.7212e-04,  1.4374e-03,\n",
       "                        -1.0480e-03,  1.2583e-03,  4.9665e-03, -3.0458e-03,\n",
       "                        -7.0622e-05, -1.3696e-03,  5.3378e-03,  2.7893e-03,\n",
       "                        -1.2976e-03, -7.3420e-03,  5.2950e-03,  3.7711e-03,\n",
       "                         5.2552e-03, -3.0170e-03,  6.0977e-03, -7.2759e-03,\n",
       "                        -4.3365e-03, -5.7191e-03,  3.0208e-04,  8.8750e-03,\n",
       "                        -2.1728e-03,  3.0669e-03,  4.6764e-04, -2.5766e-03,\n",
       "                         3.3914e-03,  1.2333e-03, -7.2135e-04,  4.4514e-03,\n",
       "                         5.4436e-03,  5.9192e-03, -3.3076e-03,  1.9513e-02,\n",
       "                        -7.7080e-03, -2.2811e-02, -3.8188e-03, -2.8022e-03,\n",
       "                        -3.3647e-03, -1.1107e-03,  5.8972e-03, -3.0973e-03,\n",
       "                        -1.0568e-04, -9.1201e-03, -3.3265e-03, -1.4954e-03,\n",
       "                         6.0141e-04, -7.5254e-03,  4.4567e-04, -1.3020e-03,\n",
       "                        -4.5666e-05,  4.5465e-04,  9.2583e-05, -1.2758e-04,\n",
       "                        -1.3526e-03, -2.3299e-02, -1.1332e-02, -6.9269e-04,\n",
       "                         7.0906e-03, -7.8208e-03,  3.6337e-04, -1.1142e-02,\n",
       "                         5.4011e-03, -2.7431e-03,  3.9165e-04,  7.5250e-03,\n",
       "                         2.6791e-03,  3.0101e-03, -6.8423e-03, -5.9442e-03,\n",
       "                        -1.4305e-02, -5.2822e-03, -7.9609e-03, -3.9447e-03,\n",
       "                        -2.2216e-02,  1.0994e-04, -9.9193e-04,  6.6064e-03,\n",
       "                        -1.1802e-02,  4.3328e-03, -5.9596e-03, -7.9077e-03,\n",
       "                        -2.3889e-02, -1.3742e-02,  3.6980e-03, -4.0513e-03,\n",
       "                        -5.5581e-03, -1.8536e-03, -1.3673e-02, -2.0337e-03,\n",
       "                        -1.2991e-02,  8.3362e-04, -1.8303e-02,  5.8474e-03,\n",
       "                        -4.1165e-03, -4.2898e-03,  5.9739e-03,  5.4291e-02,\n",
       "                         1.9565e-03,  3.5921e-03,  3.1195e-03, -1.1892e-02,\n",
       "                        -5.1316e-03, -4.6253e-04, -7.4095e-03,  1.1605e-03,\n",
       "                        -1.6487e-03,  3.2023e-03,  1.1209e-03,  3.1007e-03,\n",
       "                        -7.1495e-03,  8.6327e-03, -1.0978e-02, -7.5597e-03,\n",
       "                        -4.1130e-02, -5.1963e-03, -5.9356e-03, -2.3315e-02,\n",
       "                         3.3815e-03, -3.0796e-03, -1.0664e-02, -1.7312e-03,\n",
       "                         8.5158e-03,  3.5137e-02,  5.4951e-04,  6.2589e-03,\n",
       "                        -7.6142e-03, -4.3625e-02, -4.0457e-04, -3.2747e-03,\n",
       "                        -1.4705e-02,  2.7375e-03, -6.0563e-03, -1.8569e-02,\n",
       "                         8.9786e-03,  1.8819e-02,  6.8568e-03,  8.6870e-04,\n",
       "                        -2.7668e-03, -2.8079e-03, -1.7165e-02, -2.5177e-02,\n",
       "                        -3.6976e-02, -1.2458e-02, -9.1831e-03, -8.3182e-02]),\n",
       "         device='cuda:0', size=(8193, 8193), nnz=168, layout=torch.sparse_coo),\n",
       "  'resid_0': tensor(indices=tensor([[  48,   48,   48,  ..., 8192, 8192, 8192],\n",
       "                         [2682, 2947, 4281,  ..., 8093, 8150, 8192]]),\n",
       "         values=tensor([ 0.0221, -0.0319, -0.0028,  ...,  0.0685,  0.0584,\n",
       "                        -0.4720]),\n",
       "         device='cuda:0', size=(8193, 8193), nnz=1970, layout=torch.sparse_coo)}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cir['edges']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
