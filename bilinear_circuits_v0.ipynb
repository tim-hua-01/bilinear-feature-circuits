{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.abspath('.')\n",
    "sys.path.append(parent_dir + '/bilinear_interp_tim')\n",
    "import argparse\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import torch as t\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "\n",
    "from activation_utils import SparseAct\n",
    "from attribution import patching_effect, jvp\n",
    "from circuit_plotting import plot_circuit, plot_circuit_posaligned\n",
    "from dictionary_learning import AutoEncoder\n",
    "from loading_utils import load_examples, load_examples_nopair\n",
    "from nnsight import LanguageModel\n",
    "from language import Transformer, Sight\n",
    "from sae_adopter import DictionarySAE\n",
    "from bilinear_circuits_v0 import initialize_model_and_dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean and patch inputs of different shapes.\n",
      "Clean: 3 Patch: 4\n",
      "Clean and patch inputs of different shapes.\n",
      "Clean: 4 Patch: 3\n"
     ]
    }
   ],
   "source": [
    "device, model, embed, attns, mlps, resids, dictionaries, save_basename, examples, batch_size, num_examples, n_batches, batches = initialize_model_and_dictionaries(\n",
    "    device='cuda:0',\n",
    "    model_name=\"tdooms/fw-nano\",\n",
    "    dict_id='10',  # Note: This was originally an int, but the function expects a string.\n",
    "    d_model=1024,\n",
    "    dict_path='tdooms/fw-nano-scope',\n",
    "    dataset='simple_train',\n",
    "    num_examples=20,\n",
    "    example_length=None,\n",
    "    batch_size=4,\n",
    "    aggregation='sum',\n",
    "    nopair=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cuda.empty_cache()\n",
    "\n",
    "all_submods = [embed] + [submod for layer_submods in zip(mlps, attns, resids) for submod in layer_submods]\n",
    "clean_inputs = t.cat([e['clean_prefix'] for e in batches[0]], dim=0).to(device)\n",
    "clean_answer_idxs = t.tensor([e['clean_answer'] for e in batches[0]], dtype=t.long, device=device)\n",
    "\n",
    "patch_inputs = t.cat([e['patch_prefix'] for e in batches[0]], dim=0).to(device)\n",
    "patch_answer_idxs = t.tensor([e['patch_answer'] for e in batches[0]], dtype=t.long, device=device)\n",
    "def metric_fn(model):\n",
    "    return (\n",
    "        t.gather(model.lm_head.output[:,-1,:], dim=-1, index=patch_answer_idxs.view(-1, 1)).squeeze(-1) - \\\n",
    "        t.gather(model.lm_head.output[:,-1,:], dim=-1, index=clean_answer_idxs.view(-1, 1)).squeeze(-1)\n",
    "    ) #We're only looking at the logit difference between two answers which is a very limited subset of the model's behavior aya. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   415,  4531],\n",
       "        [    1,   415,  3282],\n",
       "        [    1,   415,  4649],\n",
       "        [    1,   415, 13500]], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,  415, 8066],\n",
       "        [   1,  415, 1832],\n",
       "        [   1,  415, 6246],\n",
       "        [   1,  415, 6676]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Integrated Gradient estimation\n",
      "\n",
      "\n",
      "Initial trace\n",
      "\n",
      "torch.Size([4, 3, 1024])\n",
      "tensor(6.0038, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor([[[ 4.2607e-02,  7.5031e-03, -1.7864e-02,  ...,  4.6599e-04,\n",
      "          -3.8948e-03, -1.8446e-02],\n",
      "         [ 1.1957e-01,  8.2422e-03,  1.2530e-01,  ..., -1.1062e-01,\n",
      "           5.9906e-02,  3.3296e-01],\n",
      "         [ 7.0522e-01,  3.2143e-01,  5.6787e-01,  ..., -4.0202e-01,\n",
      "           2.5385e+00, -5.4219e-01]],\n",
      "\n",
      "        [[ 4.2607e-02,  7.5031e-03, -1.7864e-02,  ...,  4.6599e-04,\n",
      "          -3.8948e-03, -1.8446e-02],\n",
      "         [ 1.1957e-01,  8.2422e-03,  1.2530e-01,  ..., -1.1062e-01,\n",
      "           5.9906e-02,  3.3296e-01],\n",
      "         [ 1.8139e+00, -3.0347e+00, -2.9061e-01,  ...,  1.1858e+00,\n",
      "          -7.5644e-01,  1.0659e+00]],\n",
      "\n",
      "        [[ 4.2607e-02,  7.5031e-03, -1.7864e-02,  ...,  4.6599e-04,\n",
      "          -3.8948e-03, -1.8446e-02],\n",
      "         [ 1.1957e-01,  8.2422e-03,  1.2530e-01,  ..., -1.1062e-01,\n",
      "           5.9906e-02,  3.3296e-01],\n",
      "         [ 1.2066e+00, -7.1073e-01, -5.6219e-02,  ..., -2.7851e-01,\n",
      "          -8.4023e-01, -8.3970e-01]],\n",
      "\n",
      "        [[ 4.2607e-02,  7.5031e-03, -1.7864e-02,  ...,  4.6599e-04,\n",
      "          -3.8948e-03, -1.8446e-02],\n",
      "         [ 1.1957e-01,  8.2422e-03,  1.2530e-01,  ..., -1.1062e-01,\n",
      "           5.9906e-02,  3.3296e-01],\n",
      "         [ 1.0923e+00,  5.0686e-01,  2.6614e-01,  ..., -1.2290e+00,\n",
      "          -2.5980e-01, -2.2697e+00]]], device='cuda:0', grad_fn=<SubBackward0>)\n",
      "\n",
      "Patching part\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f269d96ae50>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.86 s, sys: 236 ms, total: 7.09 s\n",
      "Wall time: 7.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "effects, deltas, grads, total_effect = patching_effect(\n",
    "        clean_inputs,\n",
    "        patch_inputs,\n",
    "        model,\n",
    "        all_submods,\n",
    "        dictionaries,\n",
    "        metric_fn,\n",
    "        metric_kwargs=dict(),\n",
    "        method='ig' # get better approximations for early layers by using ig\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
