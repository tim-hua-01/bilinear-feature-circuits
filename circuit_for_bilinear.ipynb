{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e083409d-a77c-4692-abaf-c3e99e1c6041",
   "metadata": {},
   "source": [
    "Run the following commands for vast ai:\n",
    "\n",
    "conda install gh --channel conda-forge\n",
    "\n",
    "gh auth login\n",
    "\n",
    "gh repo clone bilinear-feature-circuits\n",
    "\n",
    "cd bilinear-feature-circuits\n",
    "\n",
    "git submodule update --init\n",
    "\n",
    "wget https://huggingface.co/saprmarks/pythia-70m-deduped-saes/resolve/main/dictionaries_pythia-70m-deduped_10.zip\n",
    "\n",
    "#unzip dictionaries_pythia-70m-deduped_10.zip\n",
    "\n",
    "git config --global user.email \"tim_hua@outlook.com\"\n",
    "\n",
    "git config --global user.name \"tim-hua-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c374ce5-945d-4b41-91fd-5bc975ea0f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.abspath('.')\n",
    "sys.path.append(parent_dir + '/bilinear_interp_tim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ae16c0f-233d-4800-b147-f1eec6eacc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
    "from IPython.display import clear_output\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fab26e4-c28b-46a4-b8ba-2f28b6a53e81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install einops\n",
    "!pip install graphviz\n",
    "!pip install nnsight\n",
    "!pip install torchtyping\n",
    "!pip install jaxtyping\n",
    "!pip install transformer_lens\n",
    "!pip install umap\n",
    "!pip install zstandard\n",
    "!pip install plotly\n",
    "!pip install matplotlib\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2710ac3f-0e1e-4a26-aba5-b7e2eded3cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "from language import Transformer, Sight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eda91488-72fc-4ac5-a459-7ea42cf7b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer.from_pretrained(\"tdooms/fw-nano\")\n",
    "model = Sight(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "821fdd72-84b5-4f9c-b6aa-20a90cf55cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (wte): Embedding(32000, 1024)\n",
       "  (h): ModuleList(\n",
       "    (0-3): 4 x Layer(\n",
       "      (attn): Attention(\n",
       "        (rotary): Rotary()\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (w): Bilinear(\n",
       "          in_features=1024, out_features=8192, bias=True\n",
       "          (gate): Identity()\n",
       "        )\n",
       "        (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (n1): Norm(\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (n2): Norm(\n",
       "        (norm): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (n_f): Norm(\n",
       "    (norm): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._envoy.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac752eae-19c7-4827-8edf-04bc629e8200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6368566d0809487abed034336c23f5c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/567 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e872211b034532bd8dd307bfd60076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9e2c7bed7a42fe947b668d3267b82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4ae0c32b6f4821b983cbea1a131a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca42238846604e038b820fe8c071b0b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/166M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ref_model = LanguageModel('EleutherAI/pythia-70m-deduped', device_map='cuda', dispatch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5548d58a-f28c-4563-9cc2-e41cbd11ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class args_c:\n",
    "    d_model: int\n",
    "    dict_id: str\n",
    "args = args_c(d_model =1024, dict_id = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "75724e12-e1a8-4d91-9502-4d3ae022f14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsae = SAE.from_pretrained(repo_id_or_model = 'tdooms/fw-nano-scope', point = ('mlp-out',1), expansion = 8, k = 30).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9c1ea4ef-7bcb-4de6-b14f-705256c40f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsae2 = DictionarySAE.from_pretrained(repo_id_or_model = 'tdooms/fw-nano-scope', point = ('mlp-out',2), expansion = 8, k = 30).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef623c8-28f7-4eae-ae27-e93c7b3af536",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsae2 = DictionarySAE.from_pretrained(repo_id_or_model = 'tdooms/fw-nano-scope', point = ('mlp-out',2), expansion = 8, k = 30).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb941d13-dc3e-447e-8fcc-654e19211b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3531cd34-7a09-4c50-9133-9c73912702a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text = \"Hey man how's it going childchild childrenchildren MrMr\"\n",
    "toked = ref_model.tokenizer(some_text, return_tensors = 'pt')\n",
    "inp = toked['input_ids'].to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c7a5c6c0-2598-4643-a38d-30c8be596a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Hey\n",
      "1  man\n",
      "2  how\n",
      "3 's\n",
      "4  it\n",
      "5  going\n",
      "6  child\n",
      "7 child\n",
      "8  children\n",
      "9 children\n",
      "10  Mr\n",
      "11 Mr\n"
     ]
    }
   ],
   "source": [
    "for i, l in enumerate(inp.squeeze().cpu().tolist()):\n",
    "    print(i,ref_model.tokenizer.decode(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0c09b473-0f16-4b59-bdb1-2c2ee252357d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with ref_model.trace(inp):\n",
    "    acts = ref_model.gpt_neox.embed_in.output\n",
    "    acts =acts.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "60d3060c-7317-4c5b-8741-c0945319a2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bilinear-feature-circuits/dictionary_learning/dictionary.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = t.load(path)\n"
     ]
    }
   ],
   "source": [
    "embed_sae = AutoEncoder.from_pretrained(\n",
    "            f'dictionaries/pythia-70m-deduped/embed/10_32768/ae.pt',\n",
    "            device='cuda'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "46f5128b-52cc-4e12-93d0-770971a962e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon, fets = embed_sae(acts, output_features = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a0970e29-46f2-4b92-86f5-ce58e9833605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7484e-05, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = recon - acts\n",
    "t.mean(a**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6a782c1b-1b48-4e05-8053-c9ff2bc2362b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  1,  1,  1,  2,  1,  1,  1,  1, 23,  1,  2]], device='cuda:0')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fets > 0).sum(dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4fdbfd83-7716-496f-8893-13b0b5f8e975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.6793e-05, device='cuda:0', grad_fn=<SqrtBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sqrt(t.mean(a**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3787d7d7-a45b-4ccb-a626-2fa227ddb504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0313, -0.0186,  0.0164,  ..., -0.0355,  0.0305,  0.0184],\n",
       "         [ 0.0171, -0.0135,  0.0199,  ..., -0.0371,  0.0147,  0.0258],\n",
       "         [-0.0123,  0.0048,  0.0109,  ...,  0.0037,  0.0529, -0.0054],\n",
       "         [ 0.0044, -0.0051,  0.0158,  ..., -0.0474,  0.0238, -0.0171],\n",
       "         [ 0.0005, -0.0850, -0.0151,  ..., -0.0151, -0.0358,  0.0355]]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acts[:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d8092-53c8-469c-bdc2-5adb6df4f367",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = model.gpt_neox.embed_in\n",
    "attns = [layer.attention for layer in model.gpt_neox.layers]\n",
    "mlps = [layer.mlp for layer in model.gpt_neox.layers]\n",
    "resids = [layer for layer in model.gpt_neox.layers]\n",
    "\n",
    "dictionaries = {}\n",
    "if args.dict_id == 'id':\n",
    "    from dictionary_learning.dictionary import IdentityDict\n",
    "    dictionaries[embed] = IdentityDict(args.d_model)\n",
    "    for i in range(len(model.gpt_neox.layers)):\n",
    "        dictionaries[attns[i]] = IdentityDict(args.d_model)\n",
    "        dictionaries[mlps[i]] = IdentityDict(args.d_model)\n",
    "        dictionaries[resids[i]] = IdentityDict(args.d_model)\n",
    "else:\n",
    "    dictionaries[embed] = AutoEncoder.from_pretrained(\n",
    "        f'{args.dict_path}/embed/{args.dict_id}_{args.dict_size}/ae.pt',\n",
    "        device=device\n",
    "    )\n",
    "    for i in range(len(model.gpt_neox.layers)):\n",
    "        dictionaries[attns[i]] = AutoEncoder.from_pretrained(\n",
    "            f'{args.dict_path}/attn_out_layer{i}/{args.dict_id}_{args.dict_size}/ae.pt',\n",
    "            device=device\n",
    "        )\n",
    "        dictionaries[mlps[i]] = AutoEncoder.from_pretrained(\n",
    "            f'{args.dict_path}/mlp_out_layer{i}/{args.dict_id}_{args.dict_size}/ae.pt',\n",
    "            device=device\n",
    "        )\n",
    "        dictionaries[resids[i]] = AutoEncoder.from_pretrained(\n",
    "            f'{args.dict_path}/resid_out_layer{i}/{args.dict_id}_{args.dict_size}/ae.pt',\n",
    "            device=device\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f2c3e2f-bba7-4828-a5c0-2f5e342c35cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from sae import SAE\n",
    "import torch as t\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "\n",
    "from activation_utils import SparseAct\n",
    "from attribution import patching_effect, jvp\n",
    "from circuit_plotting import plot_circuit, plot_circuit_posaligned\n",
    "from dictionary_learning import AutoEncoder\n",
    "from loading_utils import load_examples, load_examples_nopair\n",
    "from nnsight import LanguageModel\n",
    "from language import Transformer, Sight\n",
    "from sae_adopter import DictionarySAE\n",
    "\n",
    "###### utilities for dealing with sparse COO tensors ######\n",
    "def flatten_index(idxs, shape):\n",
    "    \"\"\"\n",
    "    index : a tensor of shape [n, len(shape)]\n",
    "    shape : a shape\n",
    "    return a tensor of shape [n] where each element is the flattened index\n",
    "    \"\"\"\n",
    "    idxs = idxs.t()\n",
    "    # get strides from shape\n",
    "    strides = [1]\n",
    "    for i in range(len(shape)-1, 0, -1):\n",
    "        strides.append(strides[-1]*shape[i])\n",
    "    strides = list(reversed(strides))\n",
    "    strides = t.tensor(strides).to(idxs.device)\n",
    "    # flatten index\n",
    "    return (idxs * strides).sum(dim=1).unsqueeze(0)\n",
    "\n",
    "def prod(l):\n",
    "    out = 1\n",
    "    for x in l: out *= x\n",
    "    return out\n",
    "\n",
    "def sparse_flatten(x):\n",
    "    x = x.coalesce()\n",
    "    return t.sparse_coo_tensor(\n",
    "        flatten_index(x.indices(), x.shape),\n",
    "        x.values(),\n",
    "        (prod(x.shape),)\n",
    "    )\n",
    "\n",
    "def reshape_index(index, shape):\n",
    "    \"\"\"\n",
    "    index : a tensor of shape [n]\n",
    "    shape : a shape\n",
    "    return a tensor of shape [n, len(shape)] where each element is the reshaped index\n",
    "    \"\"\"\n",
    "    multi_index = []\n",
    "    for dim in reversed(shape):\n",
    "        multi_index.append(index % dim)\n",
    "        index //= dim\n",
    "    multi_index.reverse()\n",
    "    return t.stack(multi_index, dim=-1)\n",
    "\n",
    "def sparse_reshape(x, shape):\n",
    "    \"\"\"\n",
    "    x : a sparse COO tensor\n",
    "    shape : a shape\n",
    "    return x reshaped to shape\n",
    "    \"\"\"\n",
    "    # first flatten x\n",
    "    x = sparse_flatten(x).coalesce()\n",
    "    new_indices = reshape_index(x.indices()[0], shape)\n",
    "    return t.sparse_coo_tensor(new_indices.t(), x.values(), shape)\n",
    "\n",
    "def sparse_mean(x, dim):\n",
    "    if isinstance(dim, int):\n",
    "        return x.sum(dim=dim) / x.shape[dim]\n",
    "    else:\n",
    "        return x.sum(dim=dim) / prod(x.shape[d] for d in dim)\n",
    "\n",
    "######## end sparse tensor utilities ########\n",
    "\n",
    "\n",
    "def get_circuit(\n",
    "        clean,\n",
    "        patch,\n",
    "        model,\n",
    "        embed,\n",
    "        attns,\n",
    "        mlps,\n",
    "        resids,\n",
    "        dictionaries,\n",
    "        metric_fn,\n",
    "        metric_kwargs=dict(),\n",
    "        aggregation='sum', # or 'none' for not aggregating across sequence position\n",
    "        nodes_only=False,\n",
    "        node_threshold=0.1,\n",
    "        edge_threshold=0.01,\n",
    "):\n",
    "    t.cuda.empty_cache()\n",
    "    all_submods = [embed] + [submod for layer_submods in zip(mlps, attns, resids) for submod in layer_submods]\n",
    "    \n",
    "    # first get the patching effect of everything on y\n",
    "    effects, deltas, grads, total_effect = patching_effect(\n",
    "        clean,\n",
    "        patch,\n",
    "        model,\n",
    "        all_submods,\n",
    "        dictionaries,\n",
    "        metric_fn,\n",
    "        metric_kwargs=metric_kwargs,\n",
    "        method='ig' # get better approximations for early layers by using ig\n",
    "    )\n",
    "\n",
    "    def unflatten(tensor): # will break if dictionaries vary in size between layers\n",
    "        b, s, f = effects[resids[0]].act.shape\n",
    "        unflattened = rearrange(tensor, '(b s x) -> b s x', b=b, s=s)\n",
    "        return SparseAct(act=unflattened[...,:f], res=unflattened[...,f:])\n",
    "    \n",
    "    features_by_submod = {\n",
    "        submod : (effects[submod].to_tensor().flatten().abs() > node_threshold).nonzero().flatten().tolist() for submod in all_submods\n",
    "    }\n",
    "\n",
    "    n_layers = len(resids)\n",
    "\n",
    "    nodes = {'y' : total_effect}\n",
    "    nodes['embed'] = effects[embed]\n",
    "    for i in range(n_layers):\n",
    "        nodes[f'attn_{i}'] = effects[attns[i]]\n",
    "        nodes[f'mlp_{i}'] = effects[mlps[i]]\n",
    "        nodes[f'resid_{i}'] = effects[resids[i]]\n",
    "\n",
    "    if nodes_only:\n",
    "        if aggregation == 'sum':\n",
    "            for k in nodes:\n",
    "                if k != 'y':\n",
    "                    nodes[k] = nodes[k].sum(dim=1)\n",
    "        nodes = {k : v.mean(dim=0) for k, v in nodes.items()}\n",
    "        return nodes, None\n",
    "\n",
    "    edges = defaultdict(lambda:{})\n",
    "    edges[f'resid_{len(resids)-1}'] = { 'y' : effects[resids[-1]].to_tensor().flatten().to_sparse() }\n",
    "\n",
    "    def N(upstream, downstream):\n",
    "        return jvp(\n",
    "            clean,\n",
    "            model,\n",
    "            dictionaries,\n",
    "            downstream,\n",
    "            features_by_submod[downstream],\n",
    "            upstream,\n",
    "            grads[downstream],\n",
    "            deltas[upstream],\n",
    "            return_without_right=True,\n",
    "        )\n",
    "\n",
    "    #print(f\"Upstream deltas{deltas}\")\n",
    "    # now we work backward through the model to get the edges\n",
    "    print('Edges time')\n",
    "    for layer in reversed(range(len(resids))):\n",
    "        print(f'\\nAt layer {layer}\\n')\n",
    "        resid = resids[layer]\n",
    "        mlp = mlps[layer]\n",
    "        attn = attns[layer]\n",
    "        \n",
    "        MR_effect, MR_grad = N(mlp, resid)\n",
    "        AR_effect, AR_grad = N(attn, resid)\n",
    "\n",
    "        edges[f'mlp_{layer}'][f'resid_{layer}'] = MR_effect\n",
    "        edges[f'attn_{layer}'][f'resid_{layer}'] = AR_effect\n",
    "\n",
    "        if layer > 0:\n",
    "            prev_resid = resids[layer-1]\n",
    "        else:\n",
    "            prev_resid = embed\n",
    "\n",
    "        RM_effect, _ = N(prev_resid, mlp)\n",
    "        RA_effect, _ = N(prev_resid, attn)\n",
    "\n",
    "        MR_grad = MR_grad.coalesce()\n",
    "        AR_grad = AR_grad.coalesce()\n",
    "\n",
    "        RMR_effect = jvp(\n",
    "            clean,\n",
    "            model,\n",
    "            dictionaries,\n",
    "            mlp,\n",
    "            features_by_submod[resid],\n",
    "            prev_resid,\n",
    "            {feat_idx : unflatten(MR_grad[feat_idx].to_dense()) for feat_idx in features_by_submod[resid]},\n",
    "            deltas[prev_resid],\n",
    "        )\n",
    "        RAR_effect = jvp(\n",
    "            clean,\n",
    "            model,\n",
    "            dictionaries,\n",
    "            attn,\n",
    "            features_by_submod[resid],\n",
    "            prev_resid,\n",
    "            {feat_idx : unflatten(AR_grad[feat_idx].to_dense()) for feat_idx in features_by_submod[resid]},\n",
    "            deltas[prev_resid],\n",
    "        )\n",
    "        RR_effect, _ = N(prev_resid, resid)\n",
    "\n",
    "        if layer > 0: \n",
    "            edges[f'resid_{layer-1}'][f'mlp_{layer}'] = RM_effect\n",
    "            edges[f'resid_{layer-1}'][f'attn_{layer}'] = RA_effect\n",
    "            edges[f'resid_{layer-1}'][f'resid_{layer}'] = RR_effect - RMR_effect - RAR_effect\n",
    "        else:\n",
    "            edges['embed'][f'mlp_{layer}'] = RM_effect\n",
    "            edges['embed'][f'attn_{layer}'] = RA_effect\n",
    "            edges['embed'][f'resid_0'] = RR_effect - RMR_effect - RAR_effect\n",
    "\n",
    "    # rearrange weight matrices\n",
    "    t.cuda.empty_cache()\n",
    "    for child in edges:\n",
    "        # get shape for child\n",
    "        bc, sc, fc = nodes[child].act.shape\n",
    "        for parent in edges[child]:\n",
    "            weight_matrix = edges[child][parent]\n",
    "            if parent == 'y':\n",
    "                weight_matrix = sparse_reshape(weight_matrix, (bc, sc, fc+1))\n",
    "            else:\n",
    "                bp, sp, fp = nodes[parent].act.shape\n",
    "                assert bp == bc\n",
    "                weight_matrix = sparse_reshape(weight_matrix, (bp, sp, fp+1, bc, sc, fc+1))\n",
    "            edges[child][parent] = weight_matrix\n",
    "    t.cuda.empty_cache()\n",
    "    if aggregation == 'sum':\n",
    "        # aggregate across sequence position\n",
    "        for child in edges:\n",
    "            for parent in edges[child]:\n",
    "                weight_matrix = edges[child][parent]\n",
    "                if parent == 'y':\n",
    "                    weight_matrix = weight_matrix.sum(dim=1)\n",
    "                else:\n",
    "                    weight_matrix = weight_matrix.sum(dim=(1, 4))\n",
    "                edges[child][parent] = weight_matrix\n",
    "        for node in nodes:\n",
    "            if node != 'y':\n",
    "                nodes[node] = nodes[node].sum(dim=1)\n",
    "\n",
    "        # aggregate across batch dimension\n",
    "        for child in edges:\n",
    "            bc, fc = nodes[child].act.shape\n",
    "            for parent in edges[child]:\n",
    "                weight_matrix = edges[child][parent]\n",
    "                if parent == 'y':\n",
    "                    weight_matrix = weight_matrix.sum(dim=0) / bc\n",
    "                else:\n",
    "                    bp, fp = nodes[parent].act.shape\n",
    "                    assert bp == bc\n",
    "                    weight_matrix = weight_matrix.sum(dim=(0,2)) / bc\n",
    "                edges[child][parent] = weight_matrix\n",
    "        for node in nodes:\n",
    "            if node != 'y':\n",
    "                nodes[node] = nodes[node].mean(dim=0)\n",
    "    \n",
    "    elif aggregation == 'none':\n",
    "\n",
    "        # aggregate across batch dimensions\n",
    "        for child in edges:\n",
    "            # get shape for child\n",
    "            bc, sc, fc = nodes[child].act.shape\n",
    "            for parent in edges[child]:\n",
    "                weight_matrix = edges[child][parent]\n",
    "                if parent == 'y':\n",
    "                    weight_matrix = sparse_reshape(weight_matrix, (bc, sc, fc+1))\n",
    "                    weight_matrix = weight_matrix.sum(dim=0) / bc\n",
    "                else:\n",
    "                    bp, sp, fp = nodes[parent].act.shape\n",
    "                    assert bp == bc\n",
    "                    weight_matrix = sparse_reshape(weight_matrix, (bp, sp, fp+1, bc, sc, fc+1))\n",
    "                    weight_matrix = weight_matrix.sum(dim=(0, 3)) / bc\n",
    "                edges[child][parent] = weight_matrix\n",
    "        for node in nodes:\n",
    "            nodes[node] = nodes[node].mean(dim=0)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown aggregation: {aggregation}\")\n",
    "    t.cuda.empty_cache()\n",
    "    return nodes, edges\n",
    "\n",
    "\n",
    "def get_circuit_cluster(dataset,\n",
    "                        model_name=\"EleutherAI/pythia-70m-deduped\",\n",
    "                        d_model=512,\n",
    "                        dict_id=10,\n",
    "                        dict_size=32768,\n",
    "                        max_length=64,\n",
    "                        max_examples=100,\n",
    "                        batch_size=2,\n",
    "                        node_threshold=0.1,\n",
    "                        edge_threshold=0.01,\n",
    "                        device=\"cuda:0\",\n",
    "                        dict_path=\"dictionaries/pythia-70m-deduped/\",\n",
    "                        dataset_name=\"cluster_circuit\",\n",
    "                        circuit_dir=\"circuits/\",\n",
    "                        plot_dir=\"circuits/figures/\",\n",
    "                        model=None,\n",
    "                        dictionaries=None,):\n",
    "    \n",
    "    model = LanguageModel(model_name, device_map=device, dispatch=True)\n",
    "\n",
    "    embed = model.gpt_neox.embed_in\n",
    "    attns = [layer.attention for layer in model.gpt_neox.layers]\n",
    "    mlps = [layer.mlp for layer in model.gpt_neox.layers]\n",
    "    resids = [layer for layer in model.gpt_neox.layers]\n",
    "    dictionaries = {}\n",
    "    dictionaries[embed] = AutoEncoder.from_pretrained(\n",
    "        os.path.join(dict_path, f'embed/{dict_id}_{dict_size}/ae.pt'),\n",
    "        device=device\n",
    "    )\n",
    "    for i in range(len(model.gpt_neox.layers)):\n",
    "        dictionaries[attns[i]] = AutoEncoder.from_pretrained(\n",
    "            os.path.join(dict_path, f'attn_out_layer{i}/{dict_id}_{dict_size}/ae.pt'),\n",
    "            device=device\n",
    "        )\n",
    "        dictionaries[mlps[i]] = AutoEncoder.from_pretrained(\n",
    "            os.path.join(dict_path, f'mlp_out_layer{i}/{dict_id}_{dict_size}/ae.pt'),\n",
    "            device=device\n",
    "        )\n",
    "        dictionaries[resids[i]] = AutoEncoder.from_pretrained(\n",
    "            os.path.join(dict_path, f'resid_out_layer{i}/{dict_id}_{dict_size}/ae.pt'),\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "    examples = load_examples_nopair(dataset, max_examples, model, length=max_length)\n",
    "\n",
    "    num_examples = min(len(examples), max_examples)\n",
    "    n_batches = math.ceil(num_examples / batch_size)\n",
    "    batches = [\n",
    "        examples[batch*batch_size:(batch+1)*batch_size] for batch in range(n_batches)\n",
    "    ]\n",
    "    if num_examples < max_examples: # warn the user\n",
    "        print(f\"Total number of examples is less than {max_examples}. Using {num_examples} examples instead.\")\n",
    "\n",
    "    running_nodes = None\n",
    "    running_edges = None\n",
    "\n",
    "    for batch in tqdm(batches, desc=\"Batches\"):\n",
    "        clean_inputs = t.cat([e['clean_prefix'] for e in batch], dim=0).to(device)\n",
    "        clean_answer_idxs = t.tensor([e['clean_answer'] for e in batch], dtype=t.long, device=device)\n",
    "\n",
    "        patch_inputs = None\n",
    "        def metric_fn(model):\n",
    "            return (\n",
    "                -1 * t.gather(\n",
    "                    t.nn.functional.log_softmax(model.embed_out.output[:,-1,:], dim=-1), dim=-1, index=clean_answer_idxs.view(-1, 1)\n",
    "                ).squeeze(-1)\n",
    "            )\n",
    "        \n",
    "        nodes, edges = get_circuit(\n",
    "            clean_inputs,\n",
    "            patch_inputs,\n",
    "            model,\n",
    "            embed,\n",
    "            attns,\n",
    "            mlps,\n",
    "            resids,\n",
    "            dictionaries,\n",
    "            metric_fn,\n",
    "            aggregation=\"sum\",\n",
    "            node_threshold=node_threshold,\n",
    "            edge_threshold=edge_threshold,\n",
    "        )\n",
    "\n",
    "        if running_nodes is None:\n",
    "            running_nodes = {k : len(batch) * nodes[k].to('cpu') for k in nodes.keys() if k != 'y'}\n",
    "            running_edges = { k : { kk : len(batch) * edges[k][kk].to('cpu') for kk in edges[k].keys() } for k in edges.keys()}\n",
    "        else:\n",
    "            for k in nodes.keys():\n",
    "                if k != 'y':\n",
    "                    running_nodes[k] += len(batch) * nodes[k].to('cpu')\n",
    "            for k in edges.keys():\n",
    "                for v in edges[k].keys():\n",
    "                    running_edges[k][v] += len(batch) * edges[k][v].to('cpu')\n",
    "        \n",
    "        # memory cleanup\n",
    "        del nodes, edges\n",
    "        gc.collect()\n",
    "\n",
    "    nodes = {k : v.to(device) / num_examples for k, v in running_nodes.items()}\n",
    "    edges = {k : {kk : 1/num_examples * v.to(device) for kk, v in running_edges[k].items()} for k in running_edges.keys()}\n",
    "\n",
    "    save_dict = {\n",
    "        \"examples\" : examples,\n",
    "        \"nodes\": nodes,\n",
    "        \"edges\": edges\n",
    "    }\n",
    "    save_basename = f\"{dataset_name}_dict{dict_id}_node{node_threshold}_edge{edge_threshold}_n{num_examples}_aggsum\"\n",
    "    with open(f'{circuit_dir}/{save_basename}.pt', 'wb') as outfile:\n",
    "        t.save(save_dict, outfile)\n",
    "\n",
    "    nodes = save_dict['nodes']\n",
    "    edges = save_dict['edges']\n",
    "\n",
    "    # feature annotations\n",
    "    try:\n",
    "        annotations = {}\n",
    "        with open(f'annotations/{dict_id}_{dict_size}.jsonl', 'r') as f:\n",
    "            for line in f:\n",
    "                line = json.loads(line)\n",
    "                annotations[line['Name']] = line['Annotation']\n",
    "    except:\n",
    "        annotations = None\n",
    "\n",
    "    plot_circuit(\n",
    "        nodes, \n",
    "        edges, \n",
    "        layers=len(model.gpt_neox.layers), \n",
    "        node_threshold=node_threshold, \n",
    "        edge_threshold=edge_threshold, \n",
    "        pen_thickness=1, \n",
    "        annotations=annotations, \n",
    "        save_dir=os.path.join(plot_dir, save_basename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ec0398-d65b-43e0-8714-5d66c702df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--dataset', '-d', type=str, default='simple_train',\n",
    "                        help=\"A subject-verb agreement dataset in data/, or a path to a cluster .json.\")\n",
    "    parser.add_argument('--num_examples', '-n', type=int, default=100,\n",
    "                        help=\"The number of examples from the --dataset over which to average indirect effects.\")\n",
    "    parser.add_argument('--example_length', '-l', type=int, default=None,\n",
    "                        help=\"The max length (if using sum aggregation) or exact length (if not aggregating) of examples.\")\n",
    "    parser.add_argument('--model', type=str, default='EleutherAI/pythia-70m-deduped',\n",
    "                        help=\"The Huggingface ID of the model you wish to test.\")\n",
    "    parser.add_argument(\"--dict_path\", type=str, default=\"dictionaries/pythia-70m-deduped/\",\n",
    "                        help=\"Path to all dictionaries for your language model.\")\n",
    "    parser.add_argument('--d_model', type=int, default=512,\n",
    "                        help=\"Hidden size of the language model.\")\n",
    "    parser.add_argument('--dict_id', type=str, default=10,\n",
    "                        help=\"ID of the dictionaries. Use `id` to obtain circuits on neurons/heads directly.\")\n",
    "    parser.add_argument('--dict_size', type=int, default=32768,\n",
    "                        help=\"The width of the dictionary encoder.\")\n",
    "    parser.add_argument('--batch_size', type=int, default=32,\n",
    "                        help=\"Number of examples to process at once when running circuit discovery.\")\n",
    "    parser.add_argument('--aggregation', type=str, default='sum',\n",
    "                        help=\"Aggregation across token positions. Should be one of `sum` or `none`.\")\n",
    "    parser.add_argument('--node_threshold', type=float, default=0.2,\n",
    "                        help=\"Indirect effect threshold for keeping circuit nodes.\")\n",
    "    parser.add_argument('--edge_threshold', type=float, default=0.02,\n",
    "                        help=\"Indirect effect threshold for keeping edges.\")\n",
    "    parser.add_argument('--pen_thickness', type=float, default=1,\n",
    "                        help=\"Scales the width of the edges in the circuit plot.\")\n",
    "    parser.add_argument('--nopair', default=False, action=\"store_true\",\n",
    "                        help=\"Use if your data does not contain contrastive (minimal) pairs.\")\n",
    "    parser.add_argument('--plot_circuit', default=False, action='store_true',\n",
    "                        help=\"Plot the circuit after discovering it.\")\n",
    "    parser.add_argument('--nodes_only', default=False, action='store_true',\n",
    "                        help=\"Only search for causally implicated features; do not draw edges.\")\n",
    "    parser.add_argument('--plot_only', action=\"store_true\",\n",
    "                        help=\"Do not run circuit discovery; just plot an existing circuit.\")\n",
    "    parser.add_argument(\"--circuit_dir\", type=str, default=\"circuits/\",\n",
    "                        help=\"Directory to save/load circuits.\")\n",
    "    parser.add_argument(\"--plot_dir\", type=str, default=\"circuits/figures/\",\n",
    "                        help=\"Directory to save figures.\")\n",
    "    parser.add_argument('--seed', type=int, default=12)\n",
    "    parser.add_argument('--device', type=str, default='cuda:0')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "\n",
    "    device = args.device\n",
    "\n",
    "    model = LanguageModel(args.model, device_map=device, dispatch=True)\n",
    "\n",
    "    embed = model._envoy.transformer.wte\n",
    "    attns = [layer.attn for layer in model._envoy.transformer.h]\n",
    "    mlps = [layer.mlp for layer in model._envoy.transformer.h]\n",
    "    resids = [layer for layer in model._envoy.transformer.h]\n",
    "\n",
    "    dictionaries = {}\n",
    "    if args.dict_id == 'id':\n",
    "        from dictionary_learning.dictionary import IdentityDict\n",
    "        dictionaries[embed] = IdentityDict(args.d_model)\n",
    "        for i in range(len(model._envoy.transformer.h)):\n",
    "            dictionaries[attns[i]] = IdentityDict(args.d_model)\n",
    "            dictionaries[mlps[i]] = IdentityDict(args.d_model)\n",
    "            dictionaries[resids[i]] = IdentityDict(args.d_model)\n",
    "    else:\n",
    "        dictionaries[embed] = DictionarySAE.from_pretrained() #add embed code\n",
    "        for i in range(len(model._envoy.transformer.h)):\n",
    "            dictionaries[attns[i]] = DictionarySAE.from_pretrained(repo_id_or_model = {args.dict_path}, point = ('attn-out',i), expansion = 8, k = 30).to(device = device)\n",
    "            dictionaries[mlps[i]] = DictionarySAE.from_pretrained(repo_id_or_model = {args.dict_path}, point = ('mlp-out',i), expansion = 8, k = 30).to(device = device)\n",
    "            dictionaries[resids[i]] = DictionarySAE.from_pretrained(repo_id_or_model = {args.dict_path}, point = ('resid-post',i), expansion = 8, k = 30).to(device = device)\n",
    "        \n",
    "        if args.nopair:\n",
    "            save_basename = os.path.splitext(os.path.basename(args.dataset))[0]\n",
    "            examples = load_examples_nopair(args.dataset, args.num_examples, model, length=args.example_length)\n",
    "        else:\n",
    "            data_path = f\"data/{args.dataset}.json\"\n",
    "            save_basename = args.dataset\n",
    "            if args.aggregation == \"sum\":\n",
    "                examples = load_examples(data_path, args.num_examples, model, pad_to_length=args.example_length)\n",
    "            else:\n",
    "                examples = load_examples(data_path, args.num_examples, model, length=args.example_length)\n",
    "    \n",
    "    batch_size = args.batch_size\n",
    "    num_examples = min([args.num_examples, len(examples)])\n",
    "    n_batches = math.ceil(num_examples / batch_size)\n",
    "    batches = [\n",
    "        examples[batch*batch_size:(batch+1)*batch_size] for batch in range(n_batches)\n",
    "    ]\n",
    "    if num_examples < args.num_examples: # warn the user\n",
    "        print(f\"Total number of examples is less than {args.num_examples}. Using {num_examples} examples instead.\")\n",
    "\n",
    "    if not args.plot_only:\n",
    "        running_nodes = None\n",
    "        running_edges = None\n",
    "\n",
    "        for batch in tqdm(batches, desc=\"Batches\"):\n",
    "                \n",
    "            clean_inputs = t.cat([e['clean_prefix'] for e in batch], dim=0).to(device)\n",
    "            clean_answer_idxs = t.tensor([e['clean_answer'] for e in batch], dtype=t.long, device=device)\n",
    "\n",
    "            if args.nopair:\n",
    "                patch_inputs = None\n",
    "                def metric_fn(model):\n",
    "                    return (\n",
    "                        -1 * t.gather(\n",
    "                            t.nn.functional.log_softmax(model.embed_out.output[:,-1,:], dim=-1), dim=-1, index=clean_answer_idxs.view(-1, 1)\n",
    "                        ).squeeze(-1)\n",
    "                    )\n",
    "            else:\n",
    "                patch_inputs = t.cat([e['patch_prefix'] for e in batch], dim=0).to(device)\n",
    "                patch_answer_idxs = t.tensor([e['patch_answer'] for e in batch], dtype=t.long, device=device)\n",
    "                def metric_fn(model):\n",
    "                    return (\n",
    "                        t.gather(model.embed_out.output[:,-1,:], dim=-1, index=patch_answer_idxs.view(-1, 1)).squeeze(-1) - \\\n",
    "                        t.gather(model.embed_out.output[:,-1,:], dim=-1, index=clean_answer_idxs.view(-1, 1)).squeeze(-1)\n",
    "                    )\n",
    "            \n",
    "            nodes, edges = get_circuit(\n",
    "                clean_inputs,\n",
    "                patch_inputs,\n",
    "                model,\n",
    "                embed,\n",
    "                attns,\n",
    "                mlps,\n",
    "                resids,\n",
    "                dictionaries,\n",
    "                metric_fn,\n",
    "                nodes_only=args.nodes_only,\n",
    "                aggregation=args.aggregation,\n",
    "                node_threshold=args.node_threshold,\n",
    "                edge_threshold=args.edge_threshold,\n",
    "            )\n",
    "\n",
    "            if running_nodes is None:\n",
    "                running_nodes = {k : len(batch) * nodes[k].to('cpu') for k in nodes.keys() if k != 'y'}\n",
    "                if not args.nodes_only: running_edges = { k : { kk : len(batch) * edges[k][kk].to('cpu') for kk in edges[k].keys() } for k in edges.keys()}\n",
    "            else:\n",
    "                for k in nodes.keys():\n",
    "                    if k != 'y':\n",
    "                        running_nodes[k] += len(batch) * nodes[k].to('cpu')\n",
    "                if not args.nodes_only:\n",
    "                    for k in edges.keys():\n",
    "                        for v in edges[k].keys():\n",
    "                            running_edges[k][v] += len(batch) * edges[k][v].to('cpu')\n",
    "            \n",
    "            # memory cleanup\n",
    "            del nodes, edges\n",
    "            gc.collect()\n",
    "\n",
    "        nodes = {k : v.to(device) / num_examples for k, v in running_nodes.items()}\n",
    "        if not args.nodes_only: \n",
    "            edges = {k : {kk : 1/num_examples * v.to(device) for kk, v in running_edges[k].items()} for k in running_edges.keys()}\n",
    "        else: edges = None\n",
    "\n",
    "        save_dict = {\n",
    "            \"examples\" : examples,\n",
    "            \"nodes\": nodes,\n",
    "            \"edges\": edges\n",
    "        }\n",
    "        with open(f'{args.circuit_dir}/{save_basename}_dict{args.dict_id}_node{args.node_threshold}_edge{args.edge_threshold}_n{num_examples}_agg{args.aggregation}.pt', 'wb') as outfile:\n",
    "            t.save(save_dict, outfile)\n",
    "\n",
    "    else:\n",
    "        with open(f'{args.circuit_dir}/{save_basename}_dict{args.dict_id}_node{args.node_threshold}_edge{args.edge_threshold}_n{num_examples}_agg{args.aggregation}.pt', 'rb') as infile:\n",
    "            save_dict = t.load(infile)\n",
    "        nodes = save_dict['nodes']\n",
    "        edges = save_dict['edges']\n",
    "\n",
    "    # feature annotations\n",
    "    try:\n",
    "        annotations = {}\n",
    "        with open(f\"annotations/{args.dict_id}_{args.dict_size}.jsonl\", 'r') as annotations_data:\n",
    "            for annotation_line in annotations_data:\n",
    "                annotation = json.loads(annotation_line)\n",
    "                annotations[annotation[\"Name\"]] = annotation[\"Annotation\"]\n",
    "    except:\n",
    "        annotations = None\n",
    "\n",
    "    if args.aggregation == \"none\":\n",
    "        example = model.tokenizer.batch_decode(examples[0][\"clean_prefix\"])[0]\n",
    "        plot_circuit_posaligned(\n",
    "            nodes, \n",
    "            edges,\n",
    "            layers=len(model.gpt_neox.layers), \n",
    "            length=args.example_length,\n",
    "            example_text=example,\n",
    "            node_threshold=args.node_threshold, \n",
    "            edge_threshold=args.edge_threshold, \n",
    "            pen_thickness=args.pen_thickness, \n",
    "            annotations=annotations, \n",
    "            save_dir=f'{args.plot_dir}/{save_basename}_dict{args.dict_id}_node{args.node_threshold}_edge{args.edge_threshold}_n{num_examples}_agg{args.aggregation}'\n",
    "        )\n",
    "    else:\n",
    "        plot_circuit(\n",
    "            nodes, \n",
    "            edges, \n",
    "            layers=len(model.gpt_neox.layers), \n",
    "            node_threshold=args.node_threshold, \n",
    "            edge_threshold=args.edge_threshold, \n",
    "            pen_thickness=args.pen_thickness, \n",
    "            annotations=annotations, \n",
    "            save_dir=f'{args.plot_dir}/{save_basename}_dict{args.dict_id}_node{args.node_threshold}_edge{args.edge_threshold}_n{num_examples}_agg{args.aggregation}'\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
