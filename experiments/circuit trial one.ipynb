{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following commands for vast ai:\n",
    "\n",
    "conda install gh --channel conda-forge\n",
    "\n",
    "gh auth login\n",
    "\n",
    "gh repo clone bilinear-feature-circuits\n",
    "\n",
    "cd bilinear-feature-circuits\n",
    "\n",
    "git submodule update --init\n",
    "\n",
    "wget https://huggingface.co/saprmarks/pythia-70m-deduped-saes/resolve/main/dictionaries_pythia-70m-deduped_10.zip\n",
    "\n",
    "#unzip dictionaries_pythia-70m-deduped_10.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 760
    },
    "id": "OJ16XnQyGVsz",
    "outputId": "08d3d36e-af25-48f8-fe95-d718b90d23df",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
    "from IPython.display import clear_output\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YkFShEAQF0th",
    "outputId": "78546ddd-05fd-4362-efcd-2129e609468b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets>=2.18.0\n",
    "!pip install einops>=0.7.0\n",
    "!pip install graphviz>=0.20.1\n",
    "!pip install nnsight>=0.2.9\n",
    "!pip install torchtyping\n",
    "!pip install jaxtyping\n",
    "!pip install transformer_lens\n",
    "!pip install umap\n",
    "!pip install zstandard\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ROVZw-cGJcOh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.abspath('..')\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from dictionary_learning import AutoEncoder\n",
    "import circuit\n",
    "from circuit import get_circuit\n",
    "from circuit_plotting import plot_circuit\n",
    "from activation_utils import SparseAct\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import gc\n",
    "\n",
    "DEBUGGING = True\n",
    "if DEBUGGING:\n",
    "    tracer_kwargs = {'validate' : True, 'scan' : True}\n",
    "else:\n",
    "    tracer_kwargs = {'validate' : False, 'scan' : False}\n",
    "\n",
    "DEVICE = 'cuda:0'\n",
    "model = LanguageModel('EleutherAI/pythia-70m-deduped', device_map=DEVICE, dispatch=True)\n",
    "# utilities for loading data\n",
    "dataset = load_dataset(\"LabHC/bias_in_bios\")\n",
    "profession_dict = {'professor' : 21, 'nurse' : 13}\n",
    "male_prof = 'professor'\n",
    "female_prof = 'nurse'\n",
    "\n",
    "batch_size = 1024\n",
    "SEED = 42\n",
    "\n",
    "def get_data(train=True, ambiguous=True, batch_size=128, seed=SEED):\n",
    "    if train:\n",
    "        data = dataset['train']\n",
    "    else:\n",
    "        data = dataset['test']\n",
    "    if ambiguous:\n",
    "        neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        n = min([len(neg), len(pos)])\n",
    "        neg, pos = neg[:n], pos[:n]\n",
    "        data = neg + pos\n",
    "        labels = [0]*n + [1]*n\n",
    "        idxs = list(range(2*n))\n",
    "        random.Random(seed).shuffle(idxs)\n",
    "        data, labels = [data[i] for i in idxs], [labels[i] for i in idxs]\n",
    "        true_labels = spurious_labels = labels\n",
    "    else:\n",
    "        neg_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        neg_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 1]\n",
    "        pos_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 0]\n",
    "        pos_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        n = min([len(neg_neg), len(neg_pos), len(pos_neg), len(pos_pos)])\n",
    "        neg_neg, neg_pos, pos_neg, pos_pos = neg_neg[:n], neg_pos[:n], pos_neg[:n], pos_pos[:n]\n",
    "        data = neg_neg + neg_pos + pos_neg + pos_pos\n",
    "        true_labels     = [0]*n + [0]*n + [1]*n + [1]*n\n",
    "        spurious_labels = [0]*n + [1]*n + [0]*n + [1]*n\n",
    "        idxs = list(range(4*n))\n",
    "        random.Random(seed).shuffle(idxs)\n",
    "        data, true_labels, spurious_labels = [data[i] for i in idxs], [true_labels[i] for i in idxs], [spurious_labels[i] for i in idxs]\n",
    "\n",
    "    batches = [\n",
    "        (data[i:i+batch_size], t.tensor(true_labels[i:i+batch_size], device=DEVICE), t.tensor(spurious_labels[i:i+batch_size], device=DEVICE)) for i in range(0, len(data), batch_size)\n",
    "    ]\n",
    "\n",
    "    return batches\n",
    "\n",
    "def get_subgroups(train=True, ambiguous=True, batch_size=128, seed=SEED):\n",
    "    if train:\n",
    "        data = dataset['train']\n",
    "    else:\n",
    "        data = dataset['test']\n",
    "    if ambiguous:\n",
    "        neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        neg_labels, pos_labels = (0, 0), (1, 1)\n",
    "        subgroups = [(neg, neg_labels), (pos, pos_labels)]\n",
    "    else:\n",
    "        neg_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        neg_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 1]\n",
    "        pos_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 0]\n",
    "        pos_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        neg_neg_labels, neg_pos_labels, pos_neg_labels, pos_pos_labels = (0, 0), (0, 1), (1, 0), (1, 1)\n",
    "        subgroups = [(neg_neg, neg_neg_labels), (neg_pos, neg_pos_labels), (pos_neg, pos_neg_labels), (pos_pos, pos_pos_labels)]\n",
    "\n",
    "    out = {}\n",
    "    for data, label_profile in subgroups:\n",
    "        out[label_profile] = []\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            text = data[i:i+batch_size]\n",
    "            out[label_profile].append(\n",
    "                (\n",
    "                    text,\n",
    "                    t.tensor([label_profile[0]]*len(text), device=DEVICE),\n",
    "                    t.tensor([label_profile[1]]*len(text), device=DEVICE)\n",
    "                )\n",
    "            )\n",
    "    return out\n",
    "# probe training hyperparameters\n",
    "\n",
    "layer = 4 # the model layer to attach linear classification head to\n",
    "\n",
    "class Probe(nn.Module):\n",
    "    def __init__(self, activation_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(activation_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "def train_probe(get_acts, label_idx=0, batches=get_data(), lr=1e-2, epochs=1, dim=512, seed=SEED):\n",
    "    t.manual_seed(seed)\n",
    "    probe = Probe(dim).to('cuda:0')\n",
    "    optimizer = t.optim.AdamW(probe.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for batch in tqdm(batches):\n",
    "            text = batch[0]\n",
    "            labels = batch[label_idx+1]\n",
    "            acts = get_acts(text)\n",
    "            logits = probe(acts)\n",
    "            loss = criterion(logits, labels.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return probe, losses\n",
    "\n",
    "def test_probe(probe, get_acts, label_idx=0, batches=get_data(train=False), seed=SEED):\n",
    "    with t.no_grad():\n",
    "        corrects = []\n",
    "\n",
    "        for batch in tqdm(batches):\n",
    "            text = batch[0]\n",
    "            labels = batch[label_idx+1]\n",
    "            acts = get_acts(text)\n",
    "            logits = probe(acts)\n",
    "            preds = (logits > 0.0).long()\n",
    "            corrects.append((preds == labels).float())\n",
    "        return t.cat(corrects).mean().item()\n",
    "\n",
    "def get_acts(text):\n",
    "    with t.no_grad():\n",
    "        with model.trace(text, **tracer_kwargs):\n",
    "            attn_mask = model.inputs[1]['attention_mask']\n",
    "            acts = model.gpt_neox.layers[layer].output[0]\n",
    "            acts = acts * attn_mask[:, :, None]\n",
    "            acts = acts.sum(1) / attn_mask.sum(1)[:, None]\n",
    "            acts = acts.save()\n",
    "        return acts.value\n",
    "t.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9oxAJSEZJcOi",
    "outputId": "9341fa23-14f6-46ec-b566-4d8d1b9451c0"
   },
   "outputs": [],
   "source": [
    "probe, _ = train_probe(get_acts, label_idx=0)\n",
    "t.cuda.empty_cache()\n",
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "t.cuda.empty_cache()\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts, batches=batches, label_idx=0))\n",
    "t.cuda.empty_cache()\n",
    "print('Unintended feature accuracy:', test_probe(probe, get_acts, batches=batches, label_idx=1))\n",
    "t.cuda.empty_cache()\n",
    "t.save(probe.state_dict(), 'tprobe4.pt') # Save only the model weights\n",
    "print(\"Probe saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51luHlTURiUX",
    "outputId": "ee912065-d074-4965-ab8f-46dfd1b06cbe",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading probe from tprobe4.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1193/711491377.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  probe.load_state_dict(t.load('tprobe4.pt')) # Load the weights into the new model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Load the Probe\n",
    "print(f\"\\nLoading probe from {'tprobe4.pt'}...\")\n",
    "probe = Probe(512).to('cuda') # Create a new, empty probe model\n",
    "probe.load_state_dict(t.load('tprobe4.pt')) # Load the weights into the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MHFDFQuEJcOi",
    "outputId": "150b6049-db83-43d7-cac0-16f6c7727aab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bilinear-feature-circuits/dictionary_learning/dictionary.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = t.load(path)\n"
     ]
    }
   ],
   "source": [
    "# loading dictionaries\n",
    "dict_id = 10\n",
    "\n",
    "embed = model.gpt_neox.embed_in\n",
    "attns = [l.attention for l in model.gpt_neox.layers[:layer+1]]\n",
    "mlps = [l.mlp for l in model.gpt_neox.layers[:layer+1]]\n",
    "resids = model.gpt_neox.layers[:layer+1]\n",
    "\n",
    "dictionaries = {}\n",
    "dictionaries[embed] = AutoEncoder.from_pretrained(\n",
    "    f'../dictionaries/pythia-70m-deduped/embed/{dict_id}_32768/ae.pt',\n",
    "    device=DEVICE\n",
    "\n",
    ")\n",
    "for i in range(layer + 1):\n",
    "    dictionaries[attns[i]] = AutoEncoder.from_pretrained(\n",
    "        f'../dictionaries/pythia-70m-deduped/attn_out_layer{i}/{dict_id}_32768/ae.pt',\n",
    "        device=DEVICE\n",
    "    )\n",
    "    dictionaries[mlps[i]] = AutoEncoder.from_pretrained(\n",
    "        f'../dictionaries/pythia-70m-deduped/mlp_out_layer{i}/{dict_id}_32768/ae.pt',\n",
    "        device=DEVICE\n",
    "    )\n",
    "    dictionaries[resids[i]] = AutoEncoder.from_pretrained(\n",
    "        f'../dictionaries/pythia-70m-deduped/resid_out_layer{i}/{dict_id}_32768/ae.pt',\n",
    "        device=DEVICE\n",
    "    )\n",
    "# metric function for circuit discovery\n",
    "def metric_fn(model, labels=None):\n",
    "    attn_mask = model.inputs[1]['attention_mask']\n",
    "    acts = model.gpt_neox.layers[layer].output[0]\n",
    "    acts = acts * attn_mask[:, :, None]\n",
    "    acts = acts.sum(1) / attn_mask.sum(1)[:, None]\n",
    "\n",
    "    return t.where(\n",
    "        labels == 0,\n",
    "        probe(acts),\n",
    "        - probe(acts))\n",
    "\n",
    "t.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "id": "PWXijTRgJcOi",
    "outputId": "bc875fe8-481c-4221-a0df-493684fcb1c4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Integrated Gradient estimation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial trace\n",
      "\n",
      "\n",
      "Patching part\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:36<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges time\n",
      "\n",
      "At layer 4\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 3\n",
      "\n",
      "Jacobin vector product\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:12\u001b[0m\n",
      "File \u001b[0;32m/bilinear-feature-circuits/circuit.py:165\u001b[0m, in \u001b[0;36mget_circuit\u001b[0;34m(clean, patch, model, embed, attns, mlps, resids, dictionaries, metric_fn, metric_kwargs, aggregation, nodes_only, node_threshold, edge_threshold)\u001b[0m\n\u001b[1;32m    162\u001b[0m mlp \u001b[38;5;241m=\u001b[39m mlps[layer]\n\u001b[1;32m    163\u001b[0m attn \u001b[38;5;241m=\u001b[39m attns[layer]\n\u001b[0;32m--> 165\u001b[0m MR_effect, MR_grad \u001b[38;5;241m=\u001b[39m \u001b[43mN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m AR_effect, AR_grad \u001b[38;5;241m=\u001b[39m N(attn, resid)\n\u001b[1;32m    168\u001b[0m edges[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlp_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresid_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m MR_effect\n",
      "File \u001b[0;32m/bilinear-feature-circuits/circuit.py:144\u001b[0m, in \u001b[0;36mget_circuit.<locals>.N\u001b[0;34m(upstream, downstream)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mN\u001b[39m(upstream, downstream):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjvp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdictionaries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures_by_submod\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdownstream\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mupstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdownstream\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeltas\u001b[49m\u001b[43m[\u001b[49m\u001b[43mupstream\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_without_right\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/bilinear-feature-circuits/attribution.py:337\u001b[0m, in \u001b[0;36mjvp\u001b[0;34m(input, model, dictionaries, downstream_submod, downstream_features, upstream_submod, left_vec, right_vec, return_without_right)\u001b[0m\n\u001b[1;32m    335\u001b[0m     tracer_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidate\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscan\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mFalse\u001b[39;00m}\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;66;03m#this trace probs best to do without debugging\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracer_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# first specify forward pass modifications\u001b[39;49;00m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mupstream_submod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[43mupstream_submod\u001b[49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/nnsight/contexts/Tracer.py:102\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoker\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_envoy\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/nnsight/contexts/GraphBasedContext.py:215\u001b[0m, in \u001b[0;36mGraphBasedContext.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39malive \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/bilinear-feature-circuits/attribution.py:388\u001b[0m, in \u001b[0;36mjvp\u001b[0;34m(input, model, dictionaries, downstream_submod, downstream_features, upstream_submod, left_vec, right_vec, return_without_right)\u001b[0m\n\u001b[1;32m    386\u001b[0m         vjv \u001b[38;5;241m=\u001b[39m vjv\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m    387\u001b[0m         t\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m--> 388\u001b[0m         \u001b[43mgc\u001b[49m\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DEBUGGING:\n\u001b[1;32m    391\u001b[0m     tracer_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidate\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscan\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mTrue\u001b[39;00m}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# circuit discovery\n",
    "n_batches = 8\n",
    "batch_size = 4\n",
    "\n",
    "running_total = 0\n",
    "running_nodes = None\n",
    "running_edges = None\n",
    "for batch_idx, (clean, labels, _) in tqdm(enumerate(get_data(train=True, ambiguous=True, batch_size=batch_size, seed=SEED)), total=n_batches):\n",
    "    t.cuda.empty_cache()\n",
    "    if batch_idx == n_batches:\n",
    "        break\n",
    "    nodes, edges = circuit.get_circuit(\n",
    "        clean,\n",
    "        None,\n",
    "        model,\n",
    "        embed,\n",
    "        attns,\n",
    "        mlps,\n",
    "        resids,\n",
    "        dictionaries,\n",
    "        metric_fn,\n",
    "        metric_kwargs={'labels': labels},\n",
    "        node_threshold=0.2, # NOTE: use lower threshold if sigmoid\n",
    "        edge_threshold=0.02,\n",
    "    )\n",
    "    running_total += len(clean)\n",
    "    if running_nodes is None:\n",
    "        running_nodes = { k : len(clean) * v.to('cpu') if k != 'y' else None for k, v in nodes.items() }\n",
    "        running_edges = { k : { kk : len(clean) * v.to('cpu') for kk, v in vv.items() } for k, vv in edges.items() }\n",
    "    else:\n",
    "        for k, effect in nodes.items():\n",
    "            if k == 'y': continue\n",
    "            running_nodes[k] += len(clean) * effect.to('cpu')\n",
    "        for k in edges.keys():\n",
    "            for kk, effect in edges[k].items():\n",
    "                running_edges[k][kk] += len(clean) * effect.to('cpu')\n",
    "    del nodes, edges\n",
    "    gc.collect()\n",
    "\n",
    "for k in running_nodes.keys():\n",
    "    if k == 'y': continue\n",
    "    running_nodes[k] = running_nodes[k].to('cuda:0') / running_total\n",
    "for k in running_edges.keys():\n",
    "    for kk in running_edges[k].keys():\n",
    "        running_edges[k][kk] = running_edges[k][kk].to('cuda:0') / running_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJ8dsmLDJcOj"
   },
   "outputs": [],
   "source": [
    "# only plot positive effect nodes\n",
    "running_nodes = {\n",
    "    k : SparseAct(act=t.clamp(v.act, min=0), resc=t.clamp(v.resc, min=0)) if v is not None else None for k, v in running_nodes.items()\n",
    "}\n",
    "\n",
    "# get annotations\n",
    "try:\n",
    "    annotations = {}\n",
    "    with open(f\"../annotations/10_32768.jsonl\", 'r') as annotations_data:\n",
    "        for annotation_line in annotations_data:\n",
    "            annotation = json.loads(annotation_line)\n",
    "            annotations[annotation[\"Name\"]] = annotation[\"Annotation\"]\n",
    "except:\n",
    "    annotations = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhdLYf6ZJcOj"
   },
   "outputs": [],
   "source": [
    "plot_circuit(\n",
    "    running_nodes,\n",
    "    running_edges,\n",
    "    layers=5,\n",
    "    node_threshold=0.1,\n",
    "    edge_threshold=0.01,\n",
    "    pen_thickness=1,\n",
    "    save_dir='../circuits/figures/bib_circuit',\n",
    "    annotations=annotations\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
