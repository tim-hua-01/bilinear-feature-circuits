{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following commands for vast ai:\n",
    "\n",
    "conda install gh --channel conda-forge\n",
    "\n",
    "gh auth login\n",
    "\n",
    "gh repo clone bilinear-feature-circuits\n",
    "\n",
    "cd bilinear-feature-circuits\n",
    "\n",
    "git submodule update --init\n",
    "\n",
    "wget https://huggingface.co/saprmarks/pythia-70m-deduped-saes/resolve/main/dictionaries_pythia-70m-deduped_10.zip\n",
    "\n",
    "#unzip dictionaries_pythia-70m-deduped_10.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 760
    },
    "id": "OJ16XnQyGVsz",
    "outputId": "08d3d36e-af25-48f8-fe95-d718b90d23df",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
    "from IPython.display import clear_output\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YkFShEAQF0th",
    "outputId": "78546ddd-05fd-4362-efcd-2129e609468b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets>=2.18.0\n",
    "!pip install einops>=0.7.0\n",
    "!pip install graphviz>=0.20.1\n",
    "!pip install nnsight>=0.2.9\n",
    "!pip install torchtyping\n",
    "!pip install jaxtyping\n",
    "!pip install transformer_lens\n",
    "!pip install umap\n",
    "!pip install zstandard\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ROVZw-cGJcOh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timhua/anaconda3/envs/arena/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/timhua/anaconda3/envs/arena/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <CFED5F8E-EC3F-36FD-AAA3-2C6C7F8D3DD9> /Users/timhua/anaconda3/envs/arena/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <A51C8C05-245A-3989-8D3C-9A6704422CA5> /Users/timhua/anaconda3/envs/arena/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/Users/timhua/anaconda3/envs/arena/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/Users/timhua/anaconda3/envs/arena/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtyping'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdictionary_learning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoEncoder\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcircuit\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcircuit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_circuit\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcircuit_plotting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_circuit\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Coding/Github clones/bilinear-feature-circuits/circuit.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rearrange\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mactivation_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseAct\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mattribution\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m patching_effect, jvp\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcircuit_plotting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_circuit, plot_circuit_posaligned\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Coding/Github clones/bilinear-feature-circuits/activation_utils.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorType\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdictionary_learning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoEncoder\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSparseAct\u001b[39;00m():\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtyping'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.abspath('..')\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from dictionary_learning import AutoEncoder\n",
    "import circuit\n",
    "from circuit import get_circuit\n",
    "from circuit_plotting import plot_circuit\n",
    "from activation_utils import SparseAct\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import gc\n",
    "\n",
    "DEBUGGING = True\n",
    "if DEBUGGING:\n",
    "    tracer_kwargs = {'validate' : True, 'scan' : True}\n",
    "else:\n",
    "    tracer_kwargs = {'validate' : False, 'scan' : False}\n",
    "\n",
    "DEVICE = 'cuda:0'\n",
    "model = LanguageModel('EleutherAI/pythia-70m-deduped', device_map=DEVICE, dispatch=True)\n",
    "# utilities for loading data\n",
    "dataset = load_dataset(\"LabHC/bias_in_bios\")\n",
    "profession_dict = {'professor' : 21, 'nurse' : 13}\n",
    "male_prof = 'professor'\n",
    "female_prof = 'nurse'\n",
    "\n",
    "batch_size = 1024\n",
    "SEED = 42\n",
    "\n",
    "def get_data(train=True, ambiguous=True, batch_size=128, seed=SEED):\n",
    "    if train:\n",
    "        data = dataset['train']\n",
    "    else:\n",
    "        data = dataset['test']\n",
    "    if ambiguous:\n",
    "        neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        n = min([len(neg), len(pos)])\n",
    "        neg, pos = neg[:n], pos[:n]\n",
    "        data = neg + pos\n",
    "        labels = [0]*n + [1]*n\n",
    "        idxs = list(range(2*n))\n",
    "        random.Random(seed).shuffle(idxs)\n",
    "        data, labels = [data[i] for i in idxs], [labels[i] for i in idxs]\n",
    "        true_labels = spurious_labels = labels\n",
    "    else:\n",
    "        neg_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        neg_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 1]\n",
    "        pos_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 0]\n",
    "        pos_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        n = min([len(neg_neg), len(neg_pos), len(pos_neg), len(pos_pos)])\n",
    "        neg_neg, neg_pos, pos_neg, pos_pos = neg_neg[:n], neg_pos[:n], pos_neg[:n], pos_pos[:n]\n",
    "        data = neg_neg + neg_pos + pos_neg + pos_pos\n",
    "        true_labels     = [0]*n + [0]*n + [1]*n + [1]*n\n",
    "        spurious_labels = [0]*n + [1]*n + [0]*n + [1]*n\n",
    "        idxs = list(range(4*n))\n",
    "        random.Random(seed).shuffle(idxs)\n",
    "        data, true_labels, spurious_labels = [data[i] for i in idxs], [true_labels[i] for i in idxs], [spurious_labels[i] for i in idxs]\n",
    "\n",
    "    batches = [\n",
    "        (data[i:i+batch_size], t.tensor(true_labels[i:i+batch_size], device=DEVICE), t.tensor(spurious_labels[i:i+batch_size], device=DEVICE)) for i in range(0, len(data), batch_size)\n",
    "    ]\n",
    "\n",
    "    return batches\n",
    "\n",
    "def get_subgroups(train=True, ambiguous=True, batch_size=128, seed=SEED):\n",
    "    if train:\n",
    "        data = dataset['train']\n",
    "    else:\n",
    "        data = dataset['test']\n",
    "    if ambiguous:\n",
    "        neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        neg_labels, pos_labels = (0, 0), (1, 1)\n",
    "        subgroups = [(neg, neg_labels), (pos, pos_labels)]\n",
    "    else:\n",
    "        neg_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        neg_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 1]\n",
    "        pos_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 0]\n",
    "        pos_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        neg_neg_labels, neg_pos_labels, pos_neg_labels, pos_pos_labels = (0, 0), (0, 1), (1, 0), (1, 1)\n",
    "        subgroups = [(neg_neg, neg_neg_labels), (neg_pos, neg_pos_labels), (pos_neg, pos_neg_labels), (pos_pos, pos_pos_labels)]\n",
    "\n",
    "    out = {}\n",
    "    for data, label_profile in subgroups:\n",
    "        out[label_profile] = []\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            text = data[i:i+batch_size]\n",
    "            out[label_profile].append(\n",
    "                (\n",
    "                    text,\n",
    "                    t.tensor([label_profile[0]]*len(text), device=DEVICE),\n",
    "                    t.tensor([label_profile[1]]*len(text), device=DEVICE)\n",
    "                )\n",
    "            )\n",
    "    return out\n",
    "# probe training hyperparameters\n",
    "\n",
    "layer = 4 # the model layer to attach linear classification head to\n",
    "\n",
    "class Probe(nn.Module):\n",
    "    def __init__(self, activation_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(activation_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "def train_probe(get_acts, label_idx=0, batches=get_data(), lr=1e-2, epochs=1, dim=512, seed=SEED):\n",
    "    t.manual_seed(seed)\n",
    "    probe = Probe(dim).to('cuda:0')\n",
    "    optimizer = t.optim.AdamW(probe.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for batch in tqdm(batches):\n",
    "            text = batch[0]\n",
    "            labels = batch[label_idx+1]\n",
    "            acts = get_acts(text)\n",
    "            logits = probe(acts)\n",
    "            loss = criterion(logits, labels.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return probe, losses\n",
    "\n",
    "def test_probe(probe, get_acts, label_idx=0, batches=get_data(train=False), seed=SEED):\n",
    "    with t.no_grad():\n",
    "        corrects = []\n",
    "\n",
    "        for batch in tqdm(batches):\n",
    "            text = batch[0]\n",
    "            labels = batch[label_idx+1]\n",
    "            acts = get_acts(text)\n",
    "            logits = probe(acts)\n",
    "            preds = (logits > 0.0).long()\n",
    "            corrects.append((preds == labels).float())\n",
    "        return t.cat(corrects).mean().item()\n",
    "\n",
    "def get_acts(text):\n",
    "    with t.no_grad():\n",
    "        with model.trace(text, **tracer_kwargs):\n",
    "            attn_mask = model.inputs[1]['attention_mask']\n",
    "            acts = model.gpt_neox.layers[layer].output[0]\n",
    "            acts = acts * attn_mask[:, :, None]\n",
    "            acts = acts.sum(1) / attn_mask.sum(1)[:, None]\n",
    "            acts = acts.save()\n",
    "        return acts.value\n",
    "t.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9oxAJSEZJcOi",
    "outputId": "9341fa23-14f6-46ec-b566-4d8d1b9451c0"
   },
   "outputs": [],
   "source": [
    "probe, _ = train_probe(get_acts, label_idx=0)\n",
    "t.cuda.empty_cache()\n",
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "t.cuda.empty_cache()\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts, batches=batches, label_idx=0))\n",
    "t.cuda.empty_cache()\n",
    "print('Unintended feature accuracy:', test_probe(probe, get_acts, batches=batches, label_idx=1))\n",
    "t.cuda.empty_cache()\n",
    "t.save(probe.state_dict(), 'tprobe4.pt') # Save only the model weights\n",
    "print(\"Probe saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51luHlTURiUX",
    "outputId": "ee912065-d074-4965-ab8f-46dfd1b06cbe",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading probe from tprobe4.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1317/711491377.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  probe.load_state_dict(t.load('tprobe4.pt')) # Load the weights into the new model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Load the Probe\n",
    "print(f\"\\nLoading probe from {'tprobe4.pt'}...\")\n",
    "probe = Probe(512).to('cuda') # Create a new, empty probe model\n",
    "probe.load_state_dict(t.load('tprobe4.pt')) # Load the weights into the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MHFDFQuEJcOi",
    "outputId": "150b6049-db83-43d7-cac0-16f6c7727aab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bilinear-feature-circuits/dictionary_learning/dictionary.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = t.load(path)\n"
     ]
    }
   ],
   "source": [
    "# loading dictionaries\n",
    "dict_id = 10\n",
    "\n",
    "embed = model.gpt_neox.embed_in\n",
    "attns = [l.attention for l in model.gpt_neox.layers[:layer+1]]\n",
    "mlps = [l.mlp for l in model.gpt_neox.layers[:layer+1]]\n",
    "resids = model.gpt_neox.layers[:layer+1]\n",
    "\n",
    "dictionaries = {}\n",
    "dictionaries[embed] = AutoEncoder.from_pretrained(\n",
    "    f'../dictionaries/pythia-70m-deduped/embed/{dict_id}_32768/ae.pt',\n",
    "    device=DEVICE\n",
    "\n",
    ")\n",
    "for i in range(layer + 1):\n",
    "    dictionaries[attns[i]] = AutoEncoder.from_pretrained(\n",
    "        f'../dictionaries/pythia-70m-deduped/attn_out_layer{i}/{dict_id}_32768/ae.pt',\n",
    "        device=DEVICE\n",
    "    )\n",
    "    dictionaries[mlps[i]] = AutoEncoder.from_pretrained(\n",
    "        f'../dictionaries/pythia-70m-deduped/mlp_out_layer{i}/{dict_id}_32768/ae.pt',\n",
    "        device=DEVICE\n",
    "    )\n",
    "    dictionaries[resids[i]] = AutoEncoder.from_pretrained(\n",
    "        f'../dictionaries/pythia-70m-deduped/resid_out_layer{i}/{dict_id}_32768/ae.pt',\n",
    "        device=DEVICE\n",
    "    )\n",
    "# metric function for circuit discovery\n",
    "def metric_fn(model, labels=None):\n",
    "    attn_mask = model.inputs[1]['attention_mask']\n",
    "    acts = model.gpt_neox.layers[layer].output[0]\n",
    "    acts = acts * attn_mask[:, :, None]\n",
    "    acts = acts.sum(1) / attn_mask.sum(1)[:, None]\n",
    "\n",
    "    return t.where(\n",
    "        labels == 0,\n",
    "        probe(acts),\n",
    "        - probe(acts))\n",
    "\n",
    "t.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "id": "PWXijTRgJcOi",
    "outputId": "bc875fe8-481c-4221-a0df-493684fcb1c4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Integrated Gradient estimation\n",
      "\n",
      "\n",
      "Initial trace\n",
      "\n",
      "\n",
      "Patching part\n",
      "\n",
      "Edges time\n",
      "\n",
      "At layer 4\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 3\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 2\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 1\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 0\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [03:51<27:00, 231.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Integrated Gradient estimation\n",
      "\n",
      "\n",
      "Initial trace\n",
      "\n",
      "\n",
      "Patching part\n",
      "\n",
      "Edges time\n",
      "\n",
      "At layer 4\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 3\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 2\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 1\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 0\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [08:07<24:37, 246.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Integrated Gradient estimation\n",
      "\n",
      "\n",
      "Initial trace\n",
      "\n",
      "\n",
      "Patching part\n",
      "\n",
      "Edges time\n",
      "\n",
      "At layer 4\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 3\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 2\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 1\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 0\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [12:42<21:34, 258.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Integrated Gradient estimation\n",
      "\n",
      "\n",
      "Initial trace\n",
      "\n",
      "\n",
      "Patching part\n",
      "\n",
      "Edges time\n",
      "\n",
      "At layer 4\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 3\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 2\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 1\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 0\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [17:47<18:28, 277.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Integrated Gradient estimation\n",
      "\n",
      "\n",
      "Initial trace\n",
      "\n",
      "\n",
      "Patching part\n",
      "\n",
      "Edges time\n",
      "\n",
      "At layer 4\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 3\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 2\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 1\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 0\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [22:46<14:15, 285.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Integrated Gradient estimation\n",
      "\n",
      "\n",
      "Initial trace\n",
      "\n",
      "\n",
      "Patching part\n",
      "\n",
      "Edges time\n",
      "\n",
      "At layer 4\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 3\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 2\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 1\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 0\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [27:47<09:41, 290.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Integrated Gradient estimation\n",
      "\n",
      "\n",
      "Initial trace\n",
      "\n",
      "\n",
      "Patching part\n",
      "\n",
      "Edges time\n",
      "\n",
      "At layer 4\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 3\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 2\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 1\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 0\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [34:28<05:26, 326.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Integrated Gradient estimation\n",
      "\n",
      "\n",
      "Initial trace\n",
      "\n",
      "\n",
      "Patching part\n",
      "\n",
      "Edges time\n",
      "\n",
      "At layer 4\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 3\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 2\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 1\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "\n",
      "At layer 0\n",
      "\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n",
      "Jacobin vector product\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [39:56<00:00, 299.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40min 11s, sys: 8.33 s, total: 40min 19s\n",
      "Wall time: 40min 12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# circuit discovery\n",
    "n_batches = 8\n",
    "batch_size = 4\n",
    "\n",
    "running_total = 0\n",
    "running_nodes = None\n",
    "running_edges = None\n",
    "for batch_idx, (clean, labels, _) in tqdm(enumerate(get_data(train=True, ambiguous=True, batch_size=batch_size, seed=SEED)), total=n_batches):\n",
    "    t.cuda.empty_cache()\n",
    "    if batch_idx == n_batches:\n",
    "        break\n",
    "    nodes, edges = circuit.get_circuit(\n",
    "        clean,\n",
    "        None,\n",
    "        model,\n",
    "        embed,\n",
    "        attns,\n",
    "        mlps,\n",
    "        resids,\n",
    "        dictionaries,\n",
    "        metric_fn,\n",
    "        metric_kwargs={'labels': labels},\n",
    "        node_threshold=0.2, # NOTE: use lower threshold if sigmoid\n",
    "        edge_threshold=0.02,\n",
    "    )\n",
    "    running_total += len(clean)\n",
    "    if running_nodes is None:\n",
    "        running_nodes = { k : len(clean) * v.to('cpu') if k != 'y' else None for k, v in nodes.items() }\n",
    "        running_edges = { k : { kk : len(clean) * v.to('cpu') for kk, v in vv.items() } for k, vv in edges.items() }\n",
    "    else:\n",
    "        for k, effect in nodes.items():\n",
    "            if k == 'y': continue\n",
    "            running_nodes[k] += len(clean) * effect.to('cpu')\n",
    "        for k in edges.keys():\n",
    "            for kk, effect in edges[k].items():\n",
    "                running_edges[k][kk] += len(clean) * effect.to('cpu')\n",
    "    del nodes, edges\n",
    "    gc.collect()\n",
    "\n",
    "for k in running_nodes.keys():\n",
    "    if k == 'y': continue\n",
    "    running_nodes[k] = running_nodes[k].to('cuda:0') / running_total\n",
    "for k in running_edges.keys():\n",
    "    for kk in running_edges[k].keys():\n",
    "        running_edges[k][kk] = running_edges[k][kk].to('cuda:0') / running_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.9 ms, sys: 12 ms, total: 30.9 ms\n",
      "Wall time: 30.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pickle\n",
    "\n",
    "# Open a file in binary write mode\n",
    "with open('running_edges.pkl', 'wb') as f:\n",
    "    # Dump the object to the file\n",
    "    pickle.dump(running_edges, f)\n",
    "\n",
    "with open('running_nodes.pkl', 'wb') as f:\n",
    "    pickle.dump(running_nodes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['y', 'embed', 'attn_0', 'mlp_0', 'resid_0', 'attn_1', 'mlp_1', 'resid_1', 'attn_2', 'mlp_2', 'resid_2', 'attn_3', 'mlp_3', 'resid_3', 'attn_4', 'mlp_4', 'resid_4'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_nodes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "OJ8dsmLDJcOj"
   },
   "outputs": [],
   "source": [
    "# only plot positive effect nodes\n",
    "running_nodes = {\n",
    "    k : SparseAct(act=t.clamp(v.act, min=0), resc=t.clamp(v.resc, min=0)) if v is not None else None for k, v in running_nodes.items()\n",
    "}\n",
    "\n",
    "# get annotations\n",
    "try:\n",
    "    annotations = {}\n",
    "    with open(f\"../annotations/10_32768.jsonl\", 'r') as annotations_data:\n",
    "        for annotation_line in annotations_data:\n",
    "            annotation = json.loads(annotation_line)\n",
    "            annotations[annotation[\"Name\"]] = annotation[\"Annotation\"]\n",
    "except:\n",
    "    annotations = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sudo apt-get install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GhdLYf6ZJcOj"
   },
   "outputs": [],
   "source": [
    "plot_circuit(\n",
    "    running_nodes,\n",
    "    running_edges,\n",
    "    layers=5,\n",
    "    node_threshold=0.1,\n",
    "    edge_threshold=0.01,\n",
    "    pen_thickness=1,\n",
    "    save_dir='../circuits/figures/bib_circuit',\n",
    "    annotations=annotations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in /opt/conda/lib/python3.11/site-packages (0.20.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install graphviz"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
