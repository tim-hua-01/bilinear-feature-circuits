{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to understand, from the weights, how a single attn_out feature comes about\n",
    "\n",
    "I mean this feature is literally after a single attention layer. It's just a direction. How hard could it be?? (I couldn't do it :())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tim\\Documents\\bilinear-feature-circuits\n"
     ]
    }
   ],
   "source": [
    "#Load in all the code\n",
    "#You might need to run like git submodule --init or something to get the submodules\n",
    "import os\n",
    "import sys\n",
    "current_dir = os.path.abspath('.')\n",
    "print(current_dir)\n",
    "sys.path.append(current_dir + '/bilinear_interp_tim')    \n",
    "sys.path.append(current_dir + '/dictionary_learning')\n",
    "sys.path.append(current_dir)\n",
    "import argparse\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import torch as t\n",
    "from einops import rearrange, einsum\n",
    "from tqdm import tqdm\n",
    "\n",
    "from activation_utils import SparseAct\n",
    "from attribution import patching_effect, jvp\n",
    "from circuit_plotting import plot_circuit, plot_circuit_posaligned\n",
    "from dictionary_learning import AutoEncoder\n",
    "from loading_utils import load_examples, load_examples_nopair\n",
    "from nnsight import LanguageModel\n",
    "from language import Transformer, Sight\n",
    "from sae_adopter import DictionarySAE\n",
    "from bilinear_circuits_v0 import initialize_model_and_dictionaries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean and patch inputs of different shapes.\n",
      "Clean: 3 Patch: 4\n",
      "Clean and patch inputs of different shapes.\n",
      "Clean: 4 Patch: 3\n"
     ]
    }
   ],
   "source": [
    "device, model, embed, attns, mlps, resids, dictionaries, save_basename, examples, batch_size, num_examples, n_batches, batches = initialize_model_and_dictionaries(\n",
    "    device='cuda:0',\n",
    "    model_name=\"tdooms/fw-nano\",\n",
    "    dict_id='10',  # Note: This was originally an int, but the function expects a string.\n",
    "    d_model=1024,\n",
    "    dict_path='tdooms/fw-nano-scope',\n",
    "    dataset='simple_train',\n",
    "    num_examples=20,\n",
    "    example_length=None,\n",
    "    batch_size=4,\n",
    "    aggregation='sum',\n",
    "    nopair=False,\n",
    ")\n",
    "# We don't actually need a lot of this stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   330,  8366,   349,  5682,   298, 27854,   264,  3638,   302,\n",
       "           272, 15022, 28725,   390,  4249,   288,  8708,  8570, 28725, 21750,\n",
       "           288,  5373, 28725, 10313, 19863, 28725,  2839,   272,   312, 13112,\n",
       "          1759,   442,  5681,   272,  2007,   794,  3324,   297,   516,  6125,\n",
       "         10573,  2696, 28723]], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Picked some subset of fineweb-edu where the model achieves very good loss on some of the tokens\n",
    "input_tensor = t.tensor(model.tokenizer(\"A trigger is designed to activate a task of the virus, as displaying strange messages, deleting files, sending emails, begin the replicate process or whatever the programmer write in his malicious code.\")['input_ids'], device=device).unsqueeze(0)\n",
    "input_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6315, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with t.no_grad():\n",
    "    basic_out = model._model.forward(input_ids = input_tensor, labels = input_tensor)\n",
    "print(basic_out.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current_token</th>\n",
       "      <th>current_token_id</th>\n",
       "      <th>next_token</th>\n",
       "      <th>next_token_id</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>330</td>\n",
       "      <td>-3.425913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>330</td>\n",
       "      <td>trigger</td>\n",
       "      <td>8366</td>\n",
       "      <td>-10.745537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trigger</td>\n",
       "      <td>8366</td>\n",
       "      <td>is</td>\n",
       "      <td>349</td>\n",
       "      <td>-1.523020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is</td>\n",
       "      <td>349</td>\n",
       "      <td>designed</td>\n",
       "      <td>5682</td>\n",
       "      <td>-7.466187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>designed</td>\n",
       "      <td>5682</td>\n",
       "      <td>to</td>\n",
       "      <td>298</td>\n",
       "      <td>-0.119675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>to</td>\n",
       "      <td>298</td>\n",
       "      <td>activate</td>\n",
       "      <td>27854</td>\n",
       "      <td>-3.481380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>activate</td>\n",
       "      <td>27854</td>\n",
       "      <td>a</td>\n",
       "      <td>264</td>\n",
       "      <td>-1.152386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a</td>\n",
       "      <td>264</td>\n",
       "      <td>task</td>\n",
       "      <td>3638</td>\n",
       "      <td>-6.570916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>task</td>\n",
       "      <td>3638</td>\n",
       "      <td>of</td>\n",
       "      <td>302</td>\n",
       "      <td>-5.884786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>of</td>\n",
       "      <td>302</td>\n",
       "      <td>the</td>\n",
       "      <td>272</td>\n",
       "      <td>-1.444378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>the</td>\n",
       "      <td>272</td>\n",
       "      <td>virus</td>\n",
       "      <td>15022</td>\n",
       "      <td>-9.084792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>virus</td>\n",
       "      <td>15022</td>\n",
       "      <td>,</td>\n",
       "      <td>28725</td>\n",
       "      <td>-2.764476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>,</td>\n",
       "      <td>28725</td>\n",
       "      <td>as</td>\n",
       "      <td>390</td>\n",
       "      <td>-4.901610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>as</td>\n",
       "      <td>390</td>\n",
       "      <td>display</td>\n",
       "      <td>4249</td>\n",
       "      <td>-12.524866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>display</td>\n",
       "      <td>4249</td>\n",
       "      <td>ing</td>\n",
       "      <td>288</td>\n",
       "      <td>-0.476694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ing</td>\n",
       "      <td>288</td>\n",
       "      <td>strange</td>\n",
       "      <td>8708</td>\n",
       "      <td>-9.494614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>strange</td>\n",
       "      <td>8708</td>\n",
       "      <td>messages</td>\n",
       "      <td>8570</td>\n",
       "      <td>-5.086015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>messages</td>\n",
       "      <td>8570</td>\n",
       "      <td>,</td>\n",
       "      <td>28725</td>\n",
       "      <td>-2.739127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>,</td>\n",
       "      <td>28725</td>\n",
       "      <td>delet</td>\n",
       "      <td>21750</td>\n",
       "      <td>-7.519023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>delet</td>\n",
       "      <td>21750</td>\n",
       "      <td>ing</td>\n",
       "      <td>288</td>\n",
       "      <td>-1.134084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ing</td>\n",
       "      <td>288</td>\n",
       "      <td>files</td>\n",
       "      <td>5373</td>\n",
       "      <td>-3.762735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>files</td>\n",
       "      <td>5373</td>\n",
       "      <td>,</td>\n",
       "      <td>28725</td>\n",
       "      <td>-0.584074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>,</td>\n",
       "      <td>28725</td>\n",
       "      <td>sending</td>\n",
       "      <td>10313</td>\n",
       "      <td>-6.512169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>sending</td>\n",
       "      <td>10313</td>\n",
       "      <td>emails</td>\n",
       "      <td>19863</td>\n",
       "      <td>-4.000904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>emails</td>\n",
       "      <td>19863</td>\n",
       "      <td>,</td>\n",
       "      <td>28725</td>\n",
       "      <td>-0.541030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>,</td>\n",
       "      <td>28725</td>\n",
       "      <td>begin</td>\n",
       "      <td>2839</td>\n",
       "      <td>-14.272171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>begin</td>\n",
       "      <td>2839</td>\n",
       "      <td>the</td>\n",
       "      <td>272</td>\n",
       "      <td>-3.975727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>the</td>\n",
       "      <td>272</td>\n",
       "      <td>re</td>\n",
       "      <td>312</td>\n",
       "      <td>-5.929152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>re</td>\n",
       "      <td>312</td>\n",
       "      <td>plicate</td>\n",
       "      <td>13112</td>\n",
       "      <td>-5.814777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>plicate</td>\n",
       "      <td>13112</td>\n",
       "      <td>process</td>\n",
       "      <td>1759</td>\n",
       "      <td>-1.830484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>process</td>\n",
       "      <td>1759</td>\n",
       "      <td>or</td>\n",
       "      <td>442</td>\n",
       "      <td>-3.641128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>or</td>\n",
       "      <td>442</td>\n",
       "      <td>whatever</td>\n",
       "      <td>5681</td>\n",
       "      <td>-5.610075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>whatever</td>\n",
       "      <td>5681</td>\n",
       "      <td>the</td>\n",
       "      <td>272</td>\n",
       "      <td>-2.812156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>the</td>\n",
       "      <td>272</td>\n",
       "      <td>program</td>\n",
       "      <td>2007</td>\n",
       "      <td>-5.361582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>program</td>\n",
       "      <td>2007</td>\n",
       "      <td>mer</td>\n",
       "      <td>794</td>\n",
       "      <td>-1.936232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>mer</td>\n",
       "      <td>794</td>\n",
       "      <td>write</td>\n",
       "      <td>3324</td>\n",
       "      <td>-11.275936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>write</td>\n",
       "      <td>3324</td>\n",
       "      <td>in</td>\n",
       "      <td>297</td>\n",
       "      <td>-3.503885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>in</td>\n",
       "      <td>297</td>\n",
       "      <td>his</td>\n",
       "      <td>516</td>\n",
       "      <td>-5.248083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>his</td>\n",
       "      <td>516</td>\n",
       "      <td>mal</td>\n",
       "      <td>6125</td>\n",
       "      <td>-8.726947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>mal</td>\n",
       "      <td>6125</td>\n",
       "      <td>icious</td>\n",
       "      <td>10573</td>\n",
       "      <td>-0.860507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>icious</td>\n",
       "      <td>10573</td>\n",
       "      <td>code</td>\n",
       "      <td>2696</td>\n",
       "      <td>-0.394718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>code</td>\n",
       "      <td>2696</td>\n",
       "      <td>.</td>\n",
       "      <td>28723</td>\n",
       "      <td>-0.390856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   current_token  current_token_id next_token  next_token_id       loss\n",
       "0            <s>                 1          A            330  -3.425913\n",
       "1              A               330    trigger           8366 -10.745537\n",
       "2        trigger              8366         is            349  -1.523020\n",
       "3             is               349   designed           5682  -7.466187\n",
       "4       designed              5682         to            298  -0.119675\n",
       "5             to               298   activate          27854  -3.481380\n",
       "6       activate             27854          a            264  -1.152386\n",
       "7              a               264       task           3638  -6.570916\n",
       "8           task              3638         of            302  -5.884786\n",
       "9             of               302        the            272  -1.444378\n",
       "10           the               272      virus          15022  -9.084792\n",
       "11         virus             15022          ,          28725  -2.764476\n",
       "12             ,             28725         as            390  -4.901610\n",
       "13            as               390    display           4249 -12.524866\n",
       "14       display              4249        ing            288  -0.476694\n",
       "15           ing               288    strange           8708  -9.494614\n",
       "16       strange              8708   messages           8570  -5.086015\n",
       "17      messages              8570          ,          28725  -2.739127\n",
       "18             ,             28725      delet          21750  -7.519023\n",
       "19         delet             21750        ing            288  -1.134084\n",
       "20           ing               288      files           5373  -3.762735\n",
       "21         files              5373          ,          28725  -0.584074\n",
       "22             ,             28725    sending          10313  -6.512169\n",
       "23       sending             10313     emails          19863  -4.000904\n",
       "24        emails             19863          ,          28725  -0.541030\n",
       "25             ,             28725      begin           2839 -14.272171\n",
       "26         begin              2839        the            272  -3.975727\n",
       "27           the               272         re            312  -5.929152\n",
       "28            re               312    plicate          13112  -5.814777\n",
       "29       plicate             13112    process           1759  -1.830484\n",
       "30       process              1759         or            442  -3.641128\n",
       "31            or               442   whatever           5681  -5.610075\n",
       "32      whatever              5681        the            272  -2.812156\n",
       "33           the               272    program           2007  -5.361582\n",
       "34       program              2007        mer            794  -1.936232\n",
       "35           mer               794      write           3324 -11.275936\n",
       "36         write              3324         in            297  -3.503885\n",
       "37            in               297        his            516  -5.248083\n",
       "38           his               516        mal           6125  -8.726947\n",
       "39           mal              6125     icious          10573  -0.860507\n",
       "40        icious             10573       code           2696  -0.394718\n",
       "41          code              2696          .          28723  -0.390856"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the per token loss\n",
    "per_tok_loss = t.log_softmax(basic_out.logits[0,:-1,:], dim = -1).gather(dim = -1, index = input_tensor[0,1:].unsqueeze(-1)).squeeze(-1)\n",
    "#quick dataframe with what every token is and the next token that the loss is on\n",
    "df = pd.DataFrame({'current_token': [model.tokenizer.decode(s) for s in input_tensor[0, :-1].cpu().numpy()], \n",
    "                   'current_token_id': input_tensor[0, :-1].cpu().numpy(),\n",
    "                   'next_token': [model.tokenizer.decode(s) for s in input_tensor[0, 1:].cpu().numpy()],\n",
    "                   'next_token_id': input_tensor[0, 1:].cpu().numpy(),\n",
    "                     'loss': per_tok_loss.cpu().numpy()})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_submods = [embed] + [submod for layer_submods in zip(mlps, attns, resids) for submod in layer_submods]\n",
    "#all_submods[2] is the first attention layer and the one we're interested in. \n",
    "\n",
    "#We're going to look at the effect on the single correct token logit: the \"to\" token after \"A trigger is designed\"\n",
    "def single_tok_logit_metric(tok_ind:int) -> Callable[[LanguageModel],t.Tensor]:\n",
    "    def metric_fn(model: LanguageModel):\n",
    "        return t.gather(model.lm_head.output[:,-1,:], dim=-1, index=t.tensor([tok_ind], device=model.device).view(-1, 1)).squeeze(-1)\n",
    "    return metric_fn\n",
    "short_input = input_tensor[:, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Integrated Gradient estimation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial trace\n",
      "\n",
      "\n",
      "Patching part\n",
      "\n"
     ]
    }
   ],
   "source": [
    "effects, deltas, grads, total_effect = patching_effect(\n",
    "        clean = short_input,\n",
    "        patch = None,\n",
    "        model = model,\n",
    "        submodules = all_submods,\n",
    "        dictionaries = dictionaries,\n",
    "        metric_fn = single_tok_logit_metric(298),\n",
    "        metric_kwargs=dict(),\n",
    "        method='ig' # get better approximations for early layers by using ig\n",
    "    )\n",
    "#Use integrated gradients to get the effect of each SAE latent on the logit of the \"to\" token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cuda.empty_cache()\n",
    "t.set_printoptions(precision=3, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At submodule 0\n",
      "Embedding(32000, 1024)\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.084, 0.044, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000], device='cuda:0'),\n",
      "indices=tensor([2894, 6113,   67,   66,   60,   56,   65,   63,   39,   38,   34,   37,\n",
      "          50,   48,   62,   53,   23,   19,   13,   10,    0,    4,    6,    8,\n",
      "          26,   24,   15,   16,   28,   29,   51], device='cuda:0'))\n",
      "Error term effect\n",
      "tensor(0.000, device='cuda:0')\n",
      "At submodule 1\n",
      "MLP(\n",
      "  (w): Bilinear(\n",
      "    in_features=1024, out_features=8192, bias=True\n",
      "    (gate): Identity()\n",
      "  )\n",
      "  (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.099, 0.090, 0.081, 0.034, 0.003, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000], device='cuda:0'),\n",
      "indices=tensor([6422, 3841,  391,  487,   41,   38,   36,   37,   19,   18,   16,   35,\n",
      "          32,   28,   33,   26,    6,    5,    3,    4,   11,    2,    0,    1,\n",
      "          12,    7,   13,   17,   22,   20,   25], device='cuda:0'))\n",
      "Error term effect\n",
      "tensor(0.048, device='cuda:0')\n",
      "At submodule 2\n",
      "Attention(\n",
      "  (rotary): Rotary()\n",
      "  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.931, 0.350, 0.333, 0.075, 0.048, 0.036, 0.026, 0.021, 0.021, 0.018,\n",
      "        0.015, 0.015, 0.011, 0.010, 0.009, 0.004, 0.004, 0.003, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000], device='cuda:0'),\n",
      "indices=tensor([3514, 5042, 2764, 2891, 6467, 6874, 3583, 6616, 2440, 2188, 3472,   64,\n",
      "        6496, 8054, 3069,  617, 6342,  507,   25,   24,   20,   15,   23,   22,\n",
      "           5,    4,    1,    0,    8,   12,   21], device='cuda:0'))\n",
      "Error term effect\n",
      "tensor(-0.001, device='cuda:0')\n",
      "At submodule 3\n",
      "Layer(\n",
      "  (attn): Attention(\n",
      "    (rotary): Rotary()\n",
      "    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "    (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (mlp): MLP(\n",
      "    (w): Bilinear(\n",
      "      in_features=1024, out_features=8192, bias=True\n",
      "      (gate): Identity()\n",
      "    )\n",
      "    (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (n1): Norm(\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (n2): Norm(\n",
      "    (norm): Identity()\n",
      "  )\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.080, 0.061, 0.050, 0.021, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000], device='cuda:0'),\n",
      "indices=tensor([1002,  980, 6715, 1703,   43,   42,   41,   39,   27,   25,   19,   23,\n",
      "          37,   36,   38,   35,   11,   10,    7,    8,    6,    4,    1,    2,\n",
      "          13,   12,   15,   17,   32,   30,   34], device='cuda:0'))\n",
      "Error term effect\n",
      "tensor(-0.004, device='cuda:0')\n",
      "At submodule 4\n",
      "MLP(\n",
      "  (w): Bilinear(\n",
      "    in_features=1024, out_features=8192, bias=True\n",
      "    (gate): Identity()\n",
      "  )\n",
      "  (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.040, 0.031, 0.023, 0.019, 0.007, 0.007, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000], device='cuda:0'),\n",
      "indices=tensor([3321, 1472, 6532, 2948,  647, 2293,   36,   35,   23,   22,   34,   33,\n",
      "          30,   29,   31,   27,   10,    9,    4,    5,   14,   12,    3,    1,\n",
      "          17,   18,   20,   21,   25,   24,   26], device='cuda:0'))\n",
      "Error term effect\n",
      "tensor(-0.004, device='cuda:0')\n",
      "At submodule 5\n",
      "Attention(\n",
      "  (rotary): Rotary()\n",
      "  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.061, 0.060, 0.054, 0.052, 0.051, 0.046, 0.043, 0.034, 0.026, 0.018,\n",
      "        0.016, 0.015, 0.007, 0.002, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000], device='cuda:0'),\n",
      "indices=tensor([4382,  116, 7823, 3271, 4954, 1817, 7141, 6633, 7570, 1084, 3971, 3539,\n",
      "        3260, 2159,   36,   35,    0,    1,   34,   31,   27,   25,   29,   24,\n",
      "          14,   11,    6,   10,   17,   15,   23], device='cuda:0'))\n",
      "Error term effect\n",
      "tensor(    -0.000, device='cuda:0')\n",
      "At submodule 6\n",
      "Layer(\n",
      "  (attn): Attention(\n",
      "    (rotary): Rotary()\n",
      "    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "    (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (mlp): MLP(\n",
      "    (w): Bilinear(\n",
      "      in_features=1024, out_features=8192, bias=True\n",
      "      (gate): Identity()\n",
      "    )\n",
      "    (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (n1): Norm(\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (n2): Norm(\n",
      "    (norm): Identity()\n",
      "  )\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.235, 0.123, 0.092, 0.031, 0.025, 0.012, 0.002, 0.000, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000], device='cuda:0'),\n",
      "indices=tensor([6042, 5710, 4411, 6741, 6283, 7934, 5861,   44,   20,   38,   36,   34,\n",
      "          28,   27,   31,   26,    6,    5,    3,    4,   10,    7,   11,    2,\n",
      "          13,   19,   14,   15,   24,   23,   25], device='cuda:0'))\n",
      "Error term effect\n",
      "tensor(-0.009, device='cuda:0')\n",
      "At submodule 7\n",
      "MLP(\n",
      "  (w): Bilinear(\n",
      "    in_features=1024, out_features=8192, bias=True\n",
      "    (gate): Identity()\n",
      "  )\n",
      "  (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.114, 0.099, 0.096, 0.087, 0.078, 0.049, 0.047, 0.036, 0.033, 0.020,\n",
      "        0.011, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000], device='cuda:0'),\n",
      "indices=tensor([4827, 2332, 7291,  921, 7952, 2875,  418, 3208,  180, 6060, 2668,   57,\n",
      "          41,   53,   51,   44,    5,    4,    9,    0,   15,   36,   43,   32,\n",
      "          23,   22,   19,   21,   26,   25,   29], device='cuda:0'))\n",
      "Error term effect\n",
      "tensor(-0.011, device='cuda:0')\n",
      "At submodule 8\n",
      "Attention(\n",
      "  (rotary): Rotary()\n",
      "  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.512, 0.120, 0.103, 0.084, 0.067, 0.056, 0.050, 0.050, 0.043, 0.030,\n",
      "        0.029, 0.019, 0.018, 0.012, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000], device='cuda:0'),\n",
      "indices=tensor([ 223, 1916, 2159,  775, 1594, 7353, 1300, 5586, 2277, 5588, 6687, 3275,\n",
      "         217, 2144,   43,   38,    4,    6,   36,   35,   28,   26,   32,   25,\n",
      "          18,   17,   10,   12,   21,   20,   23], device='cuda:0'))\n",
      "Error term effect\n",
      "tensor(0.003, device='cuda:0')\n",
      "At submodule 9\n",
      "Layer(\n",
      "  (attn): Attention(\n",
      "    (rotary): Rotary()\n",
      "    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "    (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (mlp): MLP(\n",
      "    (w): Bilinear(\n",
      "      in_features=1024, out_features=8192, bias=True\n",
      "      (gate): Identity()\n",
      "    )\n",
      "    (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (n1): Norm(\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (n2): Norm(\n",
      "    (norm): Identity()\n",
      "  )\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.166, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000], device='cuda:0'),\n",
      "indices=tensor([6345,   60,   57,   58,   54,   53,   55,   56,   42,   38,   31,   35,\n",
      "          47,   45,   48,   51,   21,   16,   10,   14,    2,    0,    9,   11,\n",
      "          23,   20,   18,   19,   26,   22,   28], device='cuda:0'))\n",
      "Error term effect\n",
      "tensor(0.010, device='cuda:0')\n",
      "At submodule 10\n",
      "MLP(\n",
      "  (w): Bilinear(\n",
      "    in_features=1024, out_features=8192, bias=True\n",
      "    (gate): Identity()\n",
      "  )\n",
      "  (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.347, 0.113, 0.071, 0.006, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000], device='cuda:0'),\n",
      "indices=tensor([5700, 5652, 1815, 1704,   39,   38,   37,   36,   24,   23,   18,   21,\n",
      "          33,   32,   35,   31,   12,   11,    7,    8,    6,    4,    0,    2,\n",
      "          14,   13,   16,   17,   29,   28,   30], device='cuda:0'))\n",
      "Error term effect\n",
      "tensor(0., device='cuda:0')\n",
      "At submodule 11\n",
      "Attention(\n",
      "  (rotary): Rotary()\n",
      "  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.054, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000], device='cuda:0'),\n",
      "indices=tensor([7634,   45,   43,   44,   39,   38,   41,   42,   28,   26,   24,   25,\n",
      "          35,   32,   36,   37,   14,    8,    3,    6,    1,    0,    2,    5,\n",
      "          19,   12,   10,   11,   20,   16,   23], device='cuda:0'))\n",
      "Error term effect\n",
      "tensor(0., device='cuda:0')\n",
      "At submodule 12\n",
      "Layer(\n",
      "  (attn): Attention(\n",
      "    (rotary): Rotary()\n",
      "    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "    (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (mlp): MLP(\n",
      "    (w): Bilinear(\n",
      "      in_features=1024, out_features=8192, bias=True\n",
      "      (gate): Identity()\n",
      "    )\n",
      "    (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (n1): Norm(\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (n2): Norm(\n",
      "    (norm): Identity()\n",
      "  )\n",
      ")\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.185, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000], device='cuda:0'),\n",
      "indices=tensor([5729,   37,   35,   36,   32,   31,   33,   34,   23,   22,   20,   21,\n",
      "          26,   24,   29,   30,   15,    9,    4,    8,    2,    0,    3,    5,\n",
      "          17,   12,   10,   11,   18,   16,   19], device='cuda:0'))\n",
      "Error term effect\n",
      "tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i, m in enumerate(effects):\n",
    "    print(f\"At submodule {i}\")\n",
    "    print(m)\n",
    "    print(t.topk(effects[m].act[0,-1], k = 31))\n",
    "    print(\"Error term effect\")\n",
    "    print(effects[m].resc[0,0,-1]) #All pretty low actually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 8192])\n",
      "torch.return_types.topk(\n",
      "values=tensor([[    0.310,     0.171,     0.144,     0.039,     0.032,     0.027,\n",
      "             0.024,     0.022,     0.020,     0.009,     0.007,     0.003,\n",
      "             0.003,     0.001,     0.001,     0.001,     0.000,     0.000,\n",
      "             0.000,     0.000,     0.000,     0.000,     0.000,     0.000,\n",
      "             0.000,     0.000,     0.000,     0.000,     0.000,     0.000,\n",
      "             0.000],\n",
      "        [    0.020,     0.013,     0.010,     0.006,     0.005,     0.002,\n",
      "             0.002,     0.001,     0.001,     0.001,     0.000,     0.000,\n",
      "             0.000,     0.000,     0.000,     0.000,     0.000,     0.000,\n",
      "             0.000,     0.000,     0.000,     0.000,     0.000,     0.000,\n",
      "             0.000,     0.000,     0.000,     0.000,     0.000,     0.000,\n",
      "             0.000],\n",
      "        [    0.067,     0.041,     0.027,     0.025,     0.018,     0.016,\n",
      "             0.014,     0.011,     0.011,     0.008,     0.006,     0.006,\n",
      "             0.005,     0.005,     0.004,     0.004,     0.003,     0.002,\n",
      "             0.002,     0.000,     0.000,     0.000,     0.000,     0.000,\n",
      "             0.000,     0.000,     0.000,     0.000,     0.000,     0.000,\n",
      "             0.000],\n",
      "        [    0.174,     0.052,     0.049,     0.034,     0.032,     0.028,\n",
      "             0.025,     0.022,     0.010,     0.009,     0.007,     0.007,\n",
      "             0.005,     0.004,     0.002,     0.002,     0.000,     0.000,\n",
      "             0.000,     0.000,     0.000,     0.000,     0.000,     0.000,\n",
      "             0.000,     0.000,     0.000,     0.000,     0.000,     0.000,\n",
      "             0.000],\n",
      "        [    0.931,     0.350,     0.333,     0.075,     0.048,     0.036,\n",
      "             0.026,     0.021,     0.021,     0.018,     0.015,     0.015,\n",
      "             0.011,     0.010,     0.009,     0.004,     0.004,     0.003,\n",
      "             0.000,     0.000,     0.000,     0.000,     0.000,     0.000,\n",
      "             0.000,     0.000,     0.000,     0.000,     0.000,     0.000,\n",
      "             0.000]], device='cuda:0'),\n",
      "indices=tensor([[3514, 3008, 2764, 4836, 5448,  693, 2477,  999, 6320, 6467, 1113, 1652,\n",
      "         6042, 6405, 5523, 7999, 2407,   25,   21,   22,   17,   16,   19,   20,\n",
      "           11,    8,    5,    6,   13,    9,   15],\n",
      "        [6681, 4384, 7147,  999, 1652, 2525,  993, 1888, 3069, 6467, 2861,   25,\n",
      "           17,   23,   22,   20,    2,    1,    3,    0,    4,   16,   19,   15,\n",
      "            9,    8,    5,    7,   13,   10,   14],\n",
      "        [3514, 2891, 2496, 2764, 7849, 3581, 6467, 1120, 6496, 2722, 2169, 7467,\n",
      "         6280, 6503, 7280, 2583, 3838, 4633, 7319,   23,   13,   22,   16,   15,\n",
      "            4,    3,    7,    2,    8,   10,   14],\n",
      "        [3514, 1703, 1069, 2871, 1342, 7635, 4554, 4384, 7046, 6280, 4204, 6467,\n",
      "         6503, 5423, 2644, 1247,   30,   28,   26,   25,   18,   16,   19,   15,\n",
      "            8,    5,    0,    1,   12,   10,   13],\n",
      "        [3514, 5042, 2764, 2891, 6467, 6874, 3583, 6616, 2440, 2188, 3472,   64,\n",
      "         6496, 8054, 3069,  617, 6342,  507,   25,   24,   20,   15,   23,   22,\n",
      "            5,    4,    1,    0,    8,   12,   21]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "print(effects[all_submods[2]].act.shape)\n",
    "print(t.topk(effects[all_submods[2]].act[0], k = 31)) #we're going to focus on feature 5042, which has a large effect and isn't on every token position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector norm 9.78896713256836, error norm 2.0771453380584717\n"
     ]
    }
   ],
   "source": [
    "with model.trace(short_input): #collect some caches\n",
    "    post_embedding = model._envoy.transformer.h[0].input\n",
    "    post_embedding.save()\n",
    "    attn_out = all_submods[2].output\n",
    "    attn_out.save()\n",
    "    attn_features = dictionaries[all_submods[2]].encode(attn_out)\n",
    "    attn_features.save()\n",
    "    #error term\n",
    "    error = attn_out - dictionaries[all_submods[2]].decode(attn_features)\n",
    "    error.save()\n",
    "\n",
    "print(f\"Vector norm {attn_out[0,-1].norm()}, error norm {error[0,-1].norm()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([4.720, 3.842, 3.603, 2.715, 2.527, 1.947, 1.082, 1.031, 0.874, 0.794,\n",
       "        0.614, 0.548, 0.467, 0.418, 0.417, 0.407, 0.385, 0.366, 0.358, 0.350,\n",
       "        0.350, 0.339, 0.318, 0.318, 0.315, 0.313, 0.313, 0.302, 0.301, 0.295,\n",
       "        0.000], device='cuda:0', grad_fn=<TopkBackward0>),\n",
       "indices=tensor([3514, 6153, 5042,   57,  993, 2764, 3472, 6342, 6874, 6467, 2177, 2452,\n",
       "        5177, 2891, 5705, 3069, 6496,  693, 1688, 3583, 2440,   64, 7546, 6616,\n",
       "        6597, 2188, 7805,  617, 8054,  507,   10], device='cuda:0'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at the feature activations instead of just the attributions. 5042 is the third largest active sae latent. \n",
    "t.topk(attn_features[0,-1], k=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dir3514 = dictionaries[all_submods[2]].w_dec.weight.data[:,3514]\n",
    "latent_dir5042 = dictionaries[all_submods[2]].w_dec.weight.data[:,5042]\n",
    "attn_l1 = all_submods[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at various dot products and norms to get a sense for this latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.874, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.norm(attn_out[0,-1] - (latent_dir5042 * 3.6028))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.426, device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.nn.functional.cosine_similarity(attn_out[0,-1],latent_dir5042 * 3.6028 , dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.297,  0.953, -0.140,  0.634,  4.171]], device='cuda:0',\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "einsum(attn_out,latent_dir5042, 'b s dmod, dmod -> b s') #We see that this direction is much more prominent on the last token position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of learning how to use pytorch hooks like a good person I went and recalculated the intermediate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mypy-ignore\n",
    "with t.no_grad():\n",
    "    qkv = attn_l1.qkv(post_embedding)\n",
    "    q, k, v = rearrange(qkv, 'batch seq (n n_head d_head) -> n batch n_head seq d_head', n=3, n_head=16).unbind(dim=0)\n",
    "    q, k = attn_l1.rotary(q, k)\n",
    "    scores = einsum(q, k, \"batch n_head seq_q d_head, batch n_head seq_k d_head -> batch n_head seq_q seq_k\")\n",
    "    scores = scores / (t.tensor(q.size(-1), device=post_embedding.device).sqrt())\n",
    "    scores = scores.masked_fill(attn_l1.mask[:,:,:post_embedding.size(1),:post_embedding.size(1)].to(scores.device) == 0, -t.inf)\n",
    "    pattern = attn_l1.softmax(scores)\n",
    "    z = einsum(pattern, v, \"batch n_head seq_q seq_k, batch n_head seq_k d_head -> batch n_head seq_q d_head\")\n",
    "    o_tensor = rearrange(attn_l1.o.weight, 'd_model (n_head d_head) -> n_head d_model d_head', n_head=16)\n",
    "    fin_out = einsum(o_tensor, z,'n_head d_model d_head, batch n_head seq d_head -> batch seq n_head d_model' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.048, device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(t.abs(einops.reduce(fin_out,'batch seq n_head d_model ->batch seq d_model', 'sum') - attn_out))*100000 #check that this is calculated correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[     0.132,     -0.026,     -0.011,      0.014,     -0.089,\n",
       "              -0.060,      0.030,      0.003,     -0.026,     -0.036,\n",
       "              -0.007,      0.109,      0.071,     -0.526,      0.023,\n",
       "               0.103],\n",
       "         [     0.109,     -0.012,      0.230,      0.004,      0.377,\n",
       "              -0.001,     -0.123,      0.011,     -0.026,      0.019,\n",
       "               0.095,      0.057,      0.071,     -0.093,      0.034,\n",
       "               0.202],\n",
       "         [     0.069,      0.002,     -0.022,      0.010,     -0.371,\n",
       "              -0.045,     -0.230,      0.038,     -0.002,     -0.036,\n",
       "               0.059,     -0.067,      0.026,      0.069,      0.090,\n",
       "               0.270],\n",
       "         [     0.033,     -0.007,      0.546,      0.016,     -0.037,\n",
       "              -0.065,     -0.017,      0.019,     -0.020,     -0.008,\n",
       "               0.030,     -0.251,      0.056,      0.072,      0.013,\n",
       "               0.255],\n",
       "         [     0.165,      0.024,      0.043,      0.029,      1.171,\n",
       "              -0.002,      0.027,      0.029,     -0.013,     -0.035,\n",
       "               0.041,      0.456,      0.268,      0.591,      1.216,\n",
       "               0.162]]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we can do direct logit attribution but to that specific feature\n",
    "#finout is batch seq n_head d_model\n",
    "einsum(fin_out, latent_dir5042, 'batch seq n_head d_model, d_model -> batch seq n_head')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the two main head that are responsible are head 4 and head 14 (or -2). Let's take a look at their attention patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.000, 0.000, 0.000, 0.000, 0.000],\n",
       "        [0.213, 0.787, 0.000, 0.000, 0.000],\n",
       "        [0.348, 0.032, 0.619, 0.000, 0.000],\n",
       "        [0.849, 0.014, 0.041, 0.096, 0.000],\n",
       "        [0.243, 0.010, 0.029, 0.004, 0.713]], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern[0,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.000, 0.000, 0.000, 0.000, 0.000],\n",
       "        [0.445, 0.555, 0.000, 0.000, 0.000],\n",
       "        [0.300, 0.036, 0.664, 0.000, 0.000],\n",
       "        [0.618, 0.217, 0.054, 0.110, 0.000],\n",
       "        [0.210, 0.017, 0.024, 0.014, 0.735]], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern[0,14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[    1.000,     0.000,     0.000,     0.000,     0.000],\n",
       "          [    0.874,     0.126,     0.000,     0.000,     0.000],\n",
       "          [    0.500,     0.349,     0.151,     0.000,     0.000],\n",
       "          [    0.206,     0.649,     0.075,     0.070,     0.000],\n",
       "          [    0.451,     0.101,     0.089,     0.137,     0.222]],\n",
       "\n",
       "         [[    1.000,     0.000,     0.000,     0.000,     0.000],\n",
       "          [    0.955,     0.045,     0.000,     0.000,     0.000],\n",
       "          [    0.853,     0.073,     0.075,     0.000,     0.000],\n",
       "          [    0.918,     0.033,     0.023,     0.026,     0.000],\n",
       "          [    0.749,     0.022,     0.053,     0.103,     0.074]],\n",
       "\n",
       "         [[    1.000,     0.000,     0.000,     0.000,     0.000],\n",
       "          [    0.023,     0.977,     0.000,     0.000,     0.000],\n",
       "          [    0.614,     0.129,     0.257,     0.000,     0.000],\n",
       "          [    0.002,     0.000,     0.000,     0.998,     0.000],\n",
       "          [    0.333,     0.066,     0.215,     0.061,     0.325]],\n",
       "\n",
       "         [[    1.000,     0.000,     0.000,     0.000,     0.000],\n",
       "          [    0.810,     0.190,     0.000,     0.000,     0.000],\n",
       "          [    0.899,     0.070,     0.030,     0.000,     0.000],\n",
       "          [    0.829,     0.106,     0.033,     0.032,     0.000],\n",
       "          [    0.721,     0.080,     0.044,     0.092,     0.062]],\n",
       "\n",
       "         [[    1.000,     0.000,     0.000,     0.000,     0.000],\n",
       "          [    0.213,     0.787,     0.000,     0.000,     0.000],\n",
       "          [    0.348,     0.032,     0.619,     0.000,     0.000],\n",
       "          [    0.849,     0.014,     0.041,     0.096,     0.000],\n",
       "          [    0.243,     0.010,     0.029,     0.004,     0.713]],\n",
       "\n",
       "         [[    1.000,     0.000,     0.000,     0.000,     0.000],\n",
       "          [    0.465,     0.535,     0.000,     0.000,     0.000],\n",
       "          [    0.202,     0.058,     0.740,     0.000,     0.000],\n",
       "          [    0.557,     0.035,     0.047,     0.361,     0.000],\n",
       "          [    0.437,     0.053,     0.171,     0.085,     0.253]],\n",
       "\n",
       "         [[    1.000,     0.000,     0.000,     0.000,     0.000],\n",
       "          [    0.146,     0.854,     0.000,     0.000,     0.000],\n",
       "          [    0.075,     0.194,     0.731,     0.000,     0.000],\n",
       "          [    0.814,     0.066,     0.071,     0.049,     0.000],\n",
       "          [    0.620,     0.019,     0.061,     0.047,     0.253]],\n",
       "\n",
       "         [[    1.000,     0.000,     0.000,     0.000,     0.000],\n",
       "          [    0.958,     0.042,     0.000,     0.000,     0.000],\n",
       "          [    0.812,     0.162,     0.026,     0.000,     0.000],\n",
       "          [    0.849,     0.101,     0.010,     0.040,     0.000],\n",
       "          [    0.695,     0.114,     0.040,     0.075,     0.075]],\n",
       "\n",
       "         [[    1.000,     0.000,     0.000,     0.000,     0.000],\n",
       "          [    0.949,     0.051,     0.000,     0.000,     0.000],\n",
       "          [    0.541,     0.300,     0.159,     0.000,     0.000],\n",
       "          [    0.808,     0.132,     0.029,     0.032,     0.000],\n",
       "          [    0.763,     0.009,     0.029,     0.122,     0.076]],\n",
       "\n",
       "         [[    1.000,     0.000,     0.000,     0.000,     0.000],\n",
       "          [    0.015,     0.985,     0.000,     0.000,     0.000],\n",
       "          [    1.000,     0.000,     0.000,     0.000,     0.000],\n",
       "          [    0.001,     0.335,     0.300,     0.365,     0.000],\n",
       "          [    0.932,     0.019,     0.007,     0.027,     0.016]],\n",
       "\n",
       "         [[    1.000,     0.000,     0.000,     0.000,     0.000],\n",
       "          [    0.197,     0.803,     0.000,     0.000,     0.000],\n",
       "          [    0.313,     0.460,     0.227,     0.000,     0.000],\n",
       "          [    0.481,     0.215,     0.122,     0.183,     0.000],\n",
       "          [    0.102,     0.173,     0.086,     0.570,     0.069]],\n",
       "\n",
       "         [[    1.000,     0.000,     0.000,     0.000,     0.000],\n",
       "          [    0.015,     0.985,     0.000,     0.000,     0.000],\n",
       "          [    0.026,     0.010,     0.964,     0.000,     0.000],\n",
       "          [    0.198,     0.013,     0.035,     0.754,     0.000],\n",
       "          [    0.070,     0.012,     0.028,     0.147,     0.743]],\n",
       "\n",
       "         [[    1.000,     0.000,     0.000,     0.000,     0.000],\n",
       "          [    0.828,     0.172,     0.000,     0.000,     0.000],\n",
       "          [    0.433,     0.126,     0.441,     0.000,     0.000],\n",
       "          [    0.231,     0.675,     0.025,     0.069,     0.000],\n",
       "          [    0.143,     0.049,     0.065,     0.040,     0.703]],\n",
       "\n",
       "         [[    1.000,     0.000,     0.000,     0.000,     0.000],\n",
       "          [    0.290,     0.710,     0.000,     0.000,     0.000],\n",
       "          [    0.014,     0.490,     0.495,     0.000,     0.000],\n",
       "          [    0.002,     0.857,     0.045,     0.097,     0.000],\n",
       "          [    0.002,     0.083,     0.105,     0.128,     0.682]],\n",
       "\n",
       "         [[    1.000,     0.000,     0.000,     0.000,     0.000],\n",
       "          [    0.445,     0.555,     0.000,     0.000,     0.000],\n",
       "          [    0.300,     0.036,     0.664,     0.000,     0.000],\n",
       "          [    0.618,     0.217,     0.054,     0.110,     0.000],\n",
       "          [    0.210,     0.017,     0.024,     0.014,     0.735]],\n",
       "\n",
       "         [[    1.000,     0.000,     0.000,     0.000,     0.000],\n",
       "          [    0.807,     0.193,     0.000,     0.000,     0.000],\n",
       "          [    0.527,     0.317,     0.155,     0.000,     0.000],\n",
       "          [    0.437,     0.262,     0.205,     0.095,     0.000],\n",
       "          [    0.569,     0.035,     0.082,     0.163,     0.151]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Can also look at all the patterns\n",
    "pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaving the attention pattern aside, I wanted to decompose the ov circuit and see if low rank approximations of the OV matrix can help me understand how this feature is calculated, and what other inputs could trigger the same feature (ignoring the bias for now, which should be ok)\n",
    "\n",
    "The idea is that you do A = USV^T, and then you find some U (which is in the column space) that's similar to latent_dir5042, and then you look for the cooresponding V.\n",
    "\n",
    "My initial guess is that the V would have high dot product with a bunch of verbs in past tense that often comes before \"to\".\n",
    "\n",
    "We'll start with head #14, which had the highest DLA score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vh = t.linalg.svd(model.ov[0,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2.505,     1.884,     1.761,  ...,     0.000,     0.000,\n",
       "            0.000], device='cuda:0', grad_fn=<LinalgSvdBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I do a bunch of stuff to convince myself I did the SVD correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.019,  0.059,  0.011,  ...,  0.050,  0.029, -0.051], device='cuda:0',\n",
       "       grad_fn=<MvBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U @ t.diag_embed(S) @ Vh @ Vh[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.019,  0.059,  0.011,  ...,  0.050,  0.029, -0.051], device='cuda:0',\n",
       "       grad_fn=<MvBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.ov[0,-2] @ Vh[2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I look for plausible low rank approximations to the OV matrix that's relevant for my specific feature.\n",
    "\n",
    "Reminder that only the absolute cosine similarity matters, since U and V are unique up to a sign change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular vector 0 with value 2.505485\n",
      "Cosine similarity with the latent:-0.03854630887508392\n",
      "Singular vector 1 with value 1.884241\n",
      "Cosine similarity with the latent:-0.0847596526145935\n",
      "Singular vector 2 with value 1.761036\n",
      "Cosine similarity with the latent:-0.23725509643554688\n",
      "Singular vector 3 with value 1.668547\n",
      "Cosine similarity with the latent:-0.03744002431631088\n",
      "Singular vector 4 with value 1.622306\n",
      "Cosine similarity with the latent:-0.04328746348619461\n",
      "Singular vector 5 with value 1.570366\n",
      "Cosine similarity with the latent:-0.03828049078583717\n",
      "Singular vector 6 with value 1.558665\n",
      "Cosine similarity with the latent:0.029338840395212173\n",
      "Singular vector 7 with value 1.411765\n",
      "Cosine similarity with the latent:0.04809585586190224\n",
      "Singular vector 8 with value 1.362805\n",
      "Cosine similarity with the latent:-0.06523540616035461\n",
      "Singular vector 9 with value 1.231059\n",
      "Cosine similarity with the latent:0.02344028651714325\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"Singular vector {i} with value {S[i].detach().cpu().item():2f}\")\n",
    "    print(f\"Cosine similarity with the latent:{t.nn.functional.cosine_similarity(latent_dir5042, U[:,i], dim = 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.237, device='cuda:0', grad_fn=<AbsBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.nn.functional.cosine_similarity(latent_dir5042, model.ov[0,-2] @ Vh[2,:], dim = -1).abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try singular vector 2. We can see how much the inputs dot product with it, and calculate the corresponding contribution to the latent direction from this one low-rank transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.026, -0.213, -0.413, -0.097, -1.213]], device='cuda:0',\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "einops.einsum(post_embedding, Vh[2,:], 'b s dmod, dmod -> b s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.507, device='cuda:0', grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dot(latent_dir5042,model.ov[0,-2] @ (Vh[2,:] * -1.213))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.625, device='cuda:0', grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dot(latent_dir5042,model.ov[0,-2] @ post_embedding[0,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it's honest work but it aint much. This singular value component didn't even make up more than half the contribution from that one head! Kinda sad.\n",
    "\n",
    "Let's look at Vh and see if it correspond to a recogonizable bunch of tokens in the embedding space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([2.911, 2.378, 1.551, 1.425, 1.414, 1.380, 1.374, 1.357, 1.301, 1.300,\n",
       "        1.293, 1.279, 1.269, 1.265, 1.257, 1.247, 1.246, 1.241, 1.220, 1.219,\n",
       "        1.216, 1.212, 1.191, 1.188, 1.187, 1.181, 1.181, 1.180, 1.174, 1.171,\n",
       "        1.160, 1.148, 1.148, 1.146, 1.146, 1.142, 1.140, 1.137, 1.131, 1.129],\n",
       "       device='cuda:0', grad_fn=<TopkBackward0>),\n",
       "indices=tensor([28725, 28723, 28745,   548, 30943,  2892, 25648, 27146,  4420,   557,\n",
       "         1777, 26421, 17795,  3159,   562,   369, 21396, 31110, 31044, 18971,\n",
       "         2750, 19556,  1561, 26366, 10988,  6270, 30671,  8951,  4627, 30612,\n",
       "        19494, 13121, 31916, 19231, 29397, 28533, 12934, 13064, 17000, 27078],\n",
       "       device='cuda:0'))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.topk(einsum(model.w_e.T, Vh[2,:], 'd_vocab d_model, d_model -> d_vocab'), k = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token , with value 2.910895\n",
      "Token . with value 2.378175\n",
      "Token ; with value 1.551426\n",
      "Token \", with value 1.425128\n",
      "Token  with value 1.414175\n",
      "Token n with value 1.379538\n",
      "Token qu with value 1.373690\n",
      "Token  with value 1.357427\n",
      " with value 1.301352\n",
      "Token ), with value 1.299810\n",
      "Token $, with value 1.292855\n",
      "Token Wahl with value 1.278691\n",
      "Token  with value 1.268872\n",
      "Token ogn with value 1.265384\n",
      "Token but with value 1.257416\n",
      "Token that with value 1.247159\n",
      "Token junto with value 1.245826\n",
      "Token  with value 1.241187\n",
      "Token  with value 1.220372\n",
      "Token  with value 1.218560\n",
      "Token ido with value 1.215914\n",
      "Token d with value 1.211870\n",
      "Token *, with value 1.191067\n",
      "Token midt with value 1.187572\n",
      "Token auer with value 1.187172\n",
      "Token hs with value 1.180913\n",
      "Token  with value 1.180857\n",
      "Token scape with value 1.179776\n",
      "Token  with value 1.174117\n",
      "Token  with value 1.170934\n",
      "Token curr with value 1.160333\n",
      "Token imiento with value 1.148400\n",
      "Token  with value 1.148233\n",
      "Token ivi with value 1.146211\n",
      "Token  with value 1.145830\n",
      "Token  with value 1.141541\n",
      "Token Ign with value 1.140115\n",
      "Token views with value 1.136921\n",
      "Token  with value 1.131469\n",
      "Token backup with value 1.129435\n"
     ]
    }
   ],
   "source": [
    "tk_vals = t.topk(einsum(model.w_e.T, Vh[2,:], 'd_vocab d_model, d_model -> d_vocab'), k = 40)\n",
    "for i in range(tk_vals.values.size(0)):\n",
    "    print(f\"Token {model.tokenizer.decode(tk_vals.indices[i].item())} with value {tk_vals.values[i].item():2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope still totally incoherent. I try again with a different head and it's still not great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "U4, S4, Vh4 = t.linalg.svd(model.ov[0,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular vector 0 with value 2.630258\n",
      "tensor(0.004, device='cuda:0', grad_fn=<SumBackward1>)\n",
      "Singular vector 1 with value 2.290593\n",
      "tensor(-0.023, device='cuda:0', grad_fn=<SumBackward1>)\n",
      "Singular vector 2 with value 2.011209\n",
      "tensor(-0.053, device='cuda:0', grad_fn=<SumBackward1>)\n",
      "Singular vector 3 with value 1.875044\n",
      "tensor(0.118, device='cuda:0', grad_fn=<SumBackward1>)\n",
      "Singular vector 4 with value 1.730801\n",
      "tensor(-0.165, device='cuda:0', grad_fn=<SumBackward1>)\n",
      "Singular vector 5 with value 1.661326\n",
      "tensor(0.042, device='cuda:0', grad_fn=<SumBackward1>)\n",
      "Singular vector 6 with value 1.580658\n",
      "tensor(0.259, device='cuda:0', grad_fn=<SumBackward1>)\n",
      "Singular vector 7 with value 1.500666\n",
      "tensor(0.089, device='cuda:0', grad_fn=<SumBackward1>)\n",
      "Singular vector 8 with value 1.470811\n",
      "tensor(0.108, device='cuda:0', grad_fn=<SumBackward1>)\n",
      "Singular vector 9 with value 1.449576\n",
      "tensor(-0.068, device='cuda:0', grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"Singular vector {i} with value {S4[i].detach().cpu().item():4f}\")\n",
    "    print(t.nn.functional.cosine_similarity(latent_dir5042, U4[:,i], dim = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.259, device='cuda:0', grad_fn=<AbsBackward0>)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.nn.functional.cosine_similarity(latent_dir5042, U4[:,6], dim = -1).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.259, device='cuda:0', grad_fn=<AbsBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.nn.functional.cosine_similarity(latent_dir5042, model.ov[0,4] @ Vh4[6,:], dim = -1).abs() #yep it's the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.033,  0.423, -0.356,  1.013,  0.809]], device='cuda:0',\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "einops.einsum(post_embedding, Vh4[6,:], 'b s dmod, dmod -> b s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.331, device='cuda:0', grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dot(latent_dir5042,model.ov[0,4] @ (Vh4[6,:] * 0.809))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.331, device='cuda:0', grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dot(latent_dir5042,U4[:,6] * 0.809 * S4[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
