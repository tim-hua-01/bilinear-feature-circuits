{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bilinear-feature-circuits\n"
     ]
    }
   ],
   "source": [
    "%cd /bilinear-feature-circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean and patch inputs of different shapes.\n",
      "Clean: 3 Patch: 4\n",
      "Clean and patch inputs of different shapes.\n",
      "Clean: 4 Patch: 3\n",
      "\n",
      "Integrated Gradient estimation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial trace\n",
      "\n",
      "\n",
      "Patching part\n",
      "\n",
      "Error creating visualizer for Embedding(32000, 1024): File does not contain tensor 0-resid_pre\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.abspath('.')\n",
    "sys.path.append(parent_dir)\n",
    "from model_utils import *\n",
    "t.set_printoptions(precision=4, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive effects at Attention(\n",
      "  (rotary): Rotary()\n",
      "  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (softmax): Softmax(dim=-1)\n",
      "):\n",
      "torch.return_types.topk(\n",
      "values=tensor([[[    1.4124,     0.4015,     0.0221,     0.0081,     0.0072,\n",
      "              0.0044,     0.0032,     0.0029,     0.0026,     0.0015,\n",
      "              0.0013,     0.0011,     0.0011,     0.0010,     0.0007,\n",
      "              0.0003,     0.0002,     0.0002,     0.0001,     0.0000,\n",
      "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "              0.0000],\n",
      "         [    0.0105,     0.0052,     0.0038,     0.0030,     0.0026,\n",
      "              0.0024,     0.0015,     0.0012,     0.0011,     0.0009,\n",
      "              0.0008,     0.0008,     0.0007,     0.0003,     0.0001,\n",
      "              0.0001,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "              0.0000],\n",
      "         [    0.0563,     0.0374,     0.0077,     0.0076,     0.0060,\n",
      "              0.0046,     0.0029,     0.0025,     0.0024,     0.0023,\n",
      "              0.0022,     0.0018,     0.0017,     0.0014,     0.0013,\n",
      "              0.0012,     0.0006,     0.0004,     0.0002,     0.0000,\n",
      "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "              0.0000],\n",
      "         [    0.0666,     0.0351,     0.0257,     0.0234,     0.0146,\n",
      "              0.0140,     0.0124,     0.0121,     0.0109,     0.0081,\n",
      "              0.0068,     0.0031,     0.0028,     0.0027,     0.0026,\n",
      "              0.0020,     0.0011,     0.0000,     0.0000,     0.0000,\n",
      "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "              0.0000],\n",
      "         [    0.0455,     0.0447,     0.0320,     0.0133,     0.0130,\n",
      "              0.0093,     0.0057,     0.0057,     0.0056,     0.0043,\n",
      "              0.0036,     0.0034,     0.0026,     0.0024,     0.0018,\n",
      "              0.0010,     0.0003,     0.0003,     0.0002,     0.0000,\n",
      "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "              0.0000]]], device='cuda:0'),\n",
      "indices=tensor([[[3514, 5177, 3008, 7147, 1087, 6467, 6684, 6250, 1652,  999, 2688,\n",
      "          1025, 5448,  693, 5975, 6042, 4490, 6054, 2407, 7999,   18,   16,\n",
      "            14,   10,    1,    0,    2,    4,    7,    5,    8],\n",
      "         [1342, 5177,  789,  557, 6467,  999,   30, 4796, 3008, 5448, 6280,\n",
      "          2891, 2407, 6496, 3094,  538,   34,   33,   31,   26,   24,   22,\n",
      "            25,   16,    7,    6,    0,    3,   11,   10,   12],\n",
      "         [2764, 3514, 2891, 7919, 2407, 2291, 7467,  250, 1423, 4633, 3650,\n",
      "          6496, 6593, 5092, 7280,  103, 3633, 6503, 3838,   24,   18,   23,\n",
      "            21,   20,   11,    1,   12,    0,   14,   17,   19],\n",
      "         [3514, 5546,  162, 1342, 6503, 6467, 6876, 7635, 2644, 1703, 7046,\n",
      "          3069, 2871,  974, 6496, 5423, 5622,   34,   29,   33,   19,   14,\n",
      "            20,   24,    6,    4,    1,    3,   11,    5,   12],\n",
      "         [3514, 6153,   57, 2764, 3472,  993, 2177, 5042, 1688, 6467, 2891,\n",
      "          5177, 6496, 3583,  507,  617, 2452, 3069, 6616,   25,   13,   23,\n",
      "            19,   18,    9,    7,   10,    2,   11,   12,   16]]],\n",
      "       device='cuda:0'))\n",
      "Latent activations at Attention(\n",
      "  (rotary): Rotary()\n",
      "  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (softmax): Softmax(dim=-1)\n",
      "):\n",
      "torch.return_types.topk(\n",
      "values=tensor([[[18.3971,  7.9595,  6.8693,  1.9330,  1.6942,  1.4460,  1.4387,\n",
      "           0.9448,  0.7494,  0.7097,  0.6759,  0.6071,  0.5890,  0.5587,\n",
      "           0.5449,  0.4995,  0.4843,  0.4240,  0.3536,  0.3138,  0.2989,\n",
      "           0.2554,  0.2554,  0.2117,  0.1771,  0.0625,  0.0317,  0.0317,\n",
      "           0.0316,  0.0307,  0.0000],\n",
      "         [ 8.0108,  5.2743,  3.6956,  2.8252,  2.6843,  2.3589,  2.2052,\n",
      "           1.8876,  1.8460,  1.7075,  1.3831,  1.1027,  1.0854,  0.8913,\n",
      "           0.8811,  0.8545,  0.8243,  0.7725,  0.6509,  0.6380,  0.5341,\n",
      "           0.4350,  0.3495,  0.2942,  0.2648,  0.2348,  0.2318,  0.1464,\n",
      "           0.1439,  0.1437,  0.0000],\n",
      "         [ 4.6417,  2.9423,  2.0445,  1.8711,  1.7678,  1.5500,  1.1846,\n",
      "           1.0464,  1.0023,  0.8932,  0.8561,  0.8451,  0.8179,  0.7340,\n",
      "           0.7167,  0.6962,  0.6736,  0.6723,  0.6688,  0.6646,  0.6612,\n",
      "           0.6426,  0.6395,  0.6191,  0.6138,  0.6119,  0.6074,  0.5904,\n",
      "           0.5791,  0.5640,  0.0000],\n",
      "         [ 6.5389,  3.7323,  3.6998,  3.5655,  3.5323,  3.1610,  2.8598,\n",
      "           2.6839,  2.5693,  2.4635,  2.3669,  2.3500,  2.3230,  2.2828,\n",
      "           2.1469,  1.9063,  1.7784,  1.6516,  1.5616,  1.5438,  1.5098,\n",
      "           1.5058,  1.4615,  1.3478,  0.8932,  0.6847,  0.6346,  0.5430,\n",
      "           0.5148,  0.5113,  0.0000],\n",
      "         [ 4.7195,  3.8420,  3.6028,  2.7154,  2.5271,  1.9471,  1.0818,\n",
      "           1.0308,  0.8742,  0.7943,  0.6145,  0.5480,  0.4669,  0.4183,\n",
      "           0.4171,  0.4069,  0.3847,  0.3663,  0.3576,  0.3503,  0.3502,\n",
      "           0.3388,  0.3180,  0.3176,  0.3152,  0.3129,  0.3127,  0.3016,\n",
      "           0.3008,  0.2949,  0.0000]]], device='cuda:0',\n",
      "       grad_fn=<TopkBackward0>),\n",
      "indices=tensor([[[3514, 5177, 3008, 5448, 2764, 2688, 7147, 6280, 5622, 4836,  999,\n",
      "          6250, 8069, 6684, 2477,  693, 6467, 1087, 6320, 4490, 1025, 1652,\n",
      "          5975, 6042, 1113, 5523, 2407, 6405, 7999, 6054,    2],\n",
      "         [3514,  103,  557, 5177, 6681,  789, 1342,   30,  999, 4384, 3008,\n",
      "          6467, 4796, 2891, 6496, 1652, 5448, 7147, 2861, 6280, 5622, 2407,\n",
      "           538, 1888, 6042, 3069,  993, 2525, 6637, 3094,    1],\n",
      "         [3514, 2764, 2583, 6496, 3069, 2891, 2722, 7849, 2407, 1120, 2496,\n",
      "          3838, 7467, 4633, 2169,  250, 7280,  103, 5092, 7919, 3633, 7319,\n",
      "          6503, 3581, 1423, 2291, 6467, 6593, 3650, 6280,    1],\n",
      "         [3514, 1703, 6967,  162, 1069,  974, 6876, 4554, 3327, 2644, 1342,\n",
      "          5546, 6496, 5423, 3069, 2871, 2386, 6467, 1247, 2242, 6503, 4384,\n",
      "          7635,  557, 7046, 1537, 3472, 6280, 4204, 5622,    3],\n",
      "         [3514, 6153, 5042,   57,  993, 2764, 3472, 6342, 6874, 6467, 2177,\n",
      "          2452, 5177, 2891, 5705, 3069, 6496,  693, 1688, 3583, 2440,   64,\n",
      "          7546, 6616, 6597, 2188, 7805,  617, 8054,  507,   10]]],\n",
      "       device='cuda:0'))\n",
      "Residual effects at Attention(\n",
      "  (rotary): Rotary()\n",
      "  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (softmax): Softmax(dim=-1)\n",
      "):\n",
      "tensor([[[    -0.0001],\n",
      "         [    -0.0003],\n",
      "         [     0.0146],\n",
      "         [     0.0085],\n",
      "         [    -0.0196]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print_effects_and_residual_effects(attns[0], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn0_out = compute_aggregated_latents(attns[0], {1: [3514,103], 2: [3514,2764], 3: [3514,1703], 4: [3514, 5042]}, norm_method = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Token:\n",
      "  Token ID: 298, Token: to\n",
      "  Reference Log Prob: -0.1197\n",
      "  New Log Prob:       -0.2632\n",
      "  Change:             -0.1435\n",
      "\n",
      "Reference Distribution Top 5 Tokens:\n",
      "  Rank 1: ID 298, Token: to, Log Prob: -0.1197\n",
      "  Rank 2: ID 354, Token: for, Log Prob: -3.6161\n",
      "  Rank 3: ID 297, Token: in, Log Prob: -4.4850\n",
      "  Rank 4: ID 390, Token: as, Log Prob: -4.5395\n",
      "  Rank 5: ID 579, Token: so, Log Prob: -4.6242\n",
      "\n",
      "New Distribution Top 5 Tokens:\n",
      "  Rank 1: ID 298, Token: to, Log Prob: -0.2632\n",
      "  Rank 2: ID 297, Token: in, Log Prob: -3.3576\n",
      "  Rank 3: ID 354, Token: for, Log Prob: -3.5114\n",
      "  Rank 4: ID 579, Token: so, Log Prob: -3.7230\n",
      "  Rank 5: ID 369, Token: that, Log Prob: -3.8358\n",
      "\n",
      "Overlap in Top 5 Tokens:\n",
      "  in, to, for, so\n"
     ]
    }
   ],
   "source": [
    "#replace attention:\n",
    "with model.trace(short_input):\n",
    "    model._envoy.transformer.h[0].attn.output[0,1:] = attn0_out[0]\n",
    "    attn0_replaced = model.output\n",
    "    new_mlp_latents = dictionaries[mlps[0]].encode(model._envoy.transformer.h[0].mlp.output)\n",
    "    new_mlp_latents.save()\n",
    "    attn0_replaced.save()\n",
    "\n",
    "compare_distributions(attn0_replaced, baseline_out, 298)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([[4.2448, 3.2571, 3.0683, 2.9286, 2.8218, 2.7125, 2.6674, 2.4292, 2.3732,\n",
      "         2.2394, 2.0772, 1.9808, 1.8038, 1.7446, 1.6532, 1.4899, 1.3204, 1.3106,\n",
      "         1.2189, 1.1216, 0.9584, 0.9271, 0.8843, 0.8699, 0.5678, 0.5538, 0.4690,\n",
      "         0.3779, 0.3720, 0.3298, 0.0000],\n",
      "        [4.3517, 4.3262, 4.1144, 3.6906, 3.6122, 3.5934, 3.4363, 3.3969, 3.3551,\n",
      "         3.0004, 2.7879, 2.5060, 2.0691, 2.0239, 1.9917, 1.9281, 1.8964, 1.8300,\n",
      "         1.7971, 1.7694, 1.6457, 1.3221, 1.0970, 1.0534, 1.0524, 0.9772, 0.9130,\n",
      "         0.6048, 0.4634, 0.4530, 0.0000],\n",
      "        [2.4135, 2.3403, 2.2945, 2.1932, 2.0827, 1.8783, 1.8047, 1.6589, 1.3712,\n",
      "         1.3683, 1.2982, 1.0325, 1.0047, 0.9931, 0.9724, 0.9546, 0.9190, 0.8521,\n",
      "         0.8505, 0.8238, 0.7539, 0.7000, 0.6755, 0.6502, 0.6229, 0.6176, 0.5922,\n",
      "         0.5207, 0.5025, 0.4818, 0.0000],\n",
      "        [6.9547, 4.4225, 4.2674, 3.6945, 3.6849, 3.3687, 3.2781, 3.2281, 3.0975,\n",
      "         2.9361, 2.7653, 2.4209, 2.1971, 1.9976, 1.9753, 1.9276, 1.8563, 1.7950,\n",
      "         1.5217, 1.3690, 1.2625, 0.7155, 0.6197, 0.5409, 0.3485, 0.2805, 0.2488,\n",
      "         0.2074, 0.1718, 0.1152, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<TopkBackward0>),\n",
      "indices=tensor([[2482,   39, 3529, 4704, 6933, 4565, 2935, 1969, 5906, 1408, 4194, 4633,\n",
      "         1346, 2400, 4616, 5511, 4969, 4009, 7968, 3547, 5613, 5480,  241, 2835,\n",
      "         5846, 3541, 7612, 2022, 1194, 7963,   65],\n",
      "        [7012,  485, 6288, 4572,  704, 3302, 4618, 1877, 6508, 7671, 1119, 1209,\n",
      "         4833, 1533, 5522, 6381, 5733, 5719, 4295,  409, 1594, 2146, 4830, 2951,\n",
      "         6519, 6674, 2883, 3624, 5480, 8042,    4],\n",
      "        [5407, 5796, 1141, 3128, 6617, 2081, 6286, 2882, 4114, 3432,  757, 4761,\n",
      "         4390, 4689,   94, 3874, 1134, 2831, 3698, 4250, 3058, 2297, 7159, 1725,\n",
      "         7420, 5480, 4420, 5314,  679,  862,   35],\n",
      "        [8184, 1216, 5679, 6179, 6399,  577, 2571, 2071, 6422,  487, 3047,  792,\n",
      "         7519, 7913, 6578, 3841, 7197, 4171,  841,  369,  391, 4723,  289, 4905,\n",
      "         2838, 4758,  943, 5837, 3318, 5991,  193]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "print(t.topk(new_mlp_latents[0,1:], k = 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 32000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._model.w_e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive effects at MLP(\n",
      "  (w): Bilinear(\n",
      "    in_features=1024, out_features=8192, bias=True\n",
      "    (gate): Identity()\n",
      "  )\n",
      "  (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "):\n",
      "torch.return_types.topk(\n",
      "values=tensor([[[    0.0180,     0.0119,     0.0117,     0.0087,     0.0072,\n",
      "              0.0055,     0.0053,     0.0047,     0.0041,     0.0041,\n",
      "              0.0035,     0.0003,     0.0001,     0.0000,     0.0000,\n",
      "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "              0.0000],\n",
      "         [    0.0456,     0.0345,     0.0297,     0.0121,     0.0076,\n",
      "              0.0055,     0.0041,     0.0037,     0.0029,     0.0016,\n",
      "              0.0014,     0.0008,     0.0007,     0.0006,     0.0002,\n",
      "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "              0.0000],\n",
      "         [    0.1203,     0.1109,     0.0898,     0.0681,     0.0470,\n",
      "              0.0393,     0.0366,     0.0254,     0.0239,     0.0233,\n",
      "              0.0226,     0.0202,     0.0181,     0.0150,     0.0134,\n",
      "              0.0117,     0.0109,     0.0094,     0.0092,     0.0052,\n",
      "              0.0039,     0.0022,     0.0016,     0.0009,     0.0000,\n",
      "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "              0.0000],\n",
      "         [    0.0484,     0.0335,     0.0266,     0.0219,     0.0207,\n",
      "              0.0125,     0.0088,     0.0084,     0.0081,     0.0079,\n",
      "              0.0056,     0.0049,     0.0047,     0.0043,     0.0042,\n",
      "              0.0030,     0.0026,     0.0024,     0.0023,     0.0022,\n",
      "              0.0015,     0.0013,     0.0012,     0.0004,     0.0002,\n",
      "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "              0.0000],\n",
      "         [    0.8115,     0.3661,     0.3204,     0.3011,     0.1575,\n",
      "              0.1483,     0.1278,     0.1075,     0.1044,     0.0997,\n",
      "              0.0585,     0.0575,     0.0562,     0.0541,     0.0487,\n",
      "              0.0451,     0.0355,     0.0351,     0.0259,     0.0232,\n",
      "              0.0169,     0.0151,     0.0132,     0.0055,     0.0039,\n",
      "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "              0.0000]]], device='cuda:0'),\n",
      "indices=tensor([[[2882,  943, 6093,  525, 5828, 8042, 5669, 1396, 1062, 2703,  349,\n",
      "          4838, 4689, 7479,   40,   37,    0,    1,   34,   31,   24,   23,\n",
      "            25,   19,    9,    7,    4,    6,   12,   11,   16],\n",
      "         [6933, 4704, 1408,  241, 3529, 5906, 4565, 1194, 7612, 4969, 5846,\n",
      "          3541, 4616, 7968, 1969,   31,    0,   29,   27,   25,   19,   16,\n",
      "            23,   12,    4,    3,    1,    2,    9,    7,   11],\n",
      "         [6288,  704, 4572, 7012, 1877, 1594, 3302, 4833, 3624, 4618, 6508,\n",
      "          5719, 1209, 6674, 6381, 2146, 1533, 4295, 8042, 1119, 2951, 4830,\n",
      "          5522, 6519,   10,    6,    5,    4,    1,    0,    2],\n",
      "         [5407, 2882, 6286, 3128, 3432, 4114, 4250, 6617, 3717, 7420,   94,\n",
      "           757, 5796, 5480, 7497, 4390, 7968,  862, 3698, 4689, 2297, 2081,\n",
      "          2386, 4761, 5830,   11,    7,   10,    5,    2,    6],\n",
      "         [8184,  577, 3047, 5679, 6578, 6179,  792,  369, 7913, 1216, 7519,\n",
      "          4171, 4723, 2571,  289, 6399, 2838, 4905,  841, 3318,  943, 2071,\n",
      "          2951, 2386,   41,   16,   14,   15,   10,    8,   11]]],\n",
      "       device='cuda:0'))\n",
      "Latent activations at MLP(\n",
      "  (w): Bilinear(\n",
      "    in_features=1024, out_features=8192, bias=True\n",
      "    (gate): Identity()\n",
      "  )\n",
      "  (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "):\n",
      "torch.return_types.topk(\n",
      "values=tensor([[[0.6282, 0.4623, 0.4142, 0.3826, 0.3293, 0.3190, 0.2812, 0.2802,\n",
      "          0.2619, 0.2431, 0.2272, 0.2095, 0.1794, 0.1782, 0.1770, 0.1686,\n",
      "          0.1645, 0.1587, 0.1478, 0.1472, 0.1370, 0.1080, 0.0770, 0.0529,\n",
      "          0.0159, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
      "         [3.6214, 2.7004, 2.5436, 2.3509, 2.2379, 2.1812, 1.9981, 1.9393,\n",
      "          1.8417, 1.7749, 1.7152, 1.6614, 1.6604, 1.4229, 1.3249, 1.2681,\n",
      "          1.2352, 0.9722, 0.9212, 0.8888, 0.7160, 0.6994, 0.5114, 0.4746,\n",
      "          0.4745, 0.4233, 0.2638, 0.2144, 0.1865, 0.1852, 0.0000],\n",
      "         [4.8516, 4.4922, 4.3207, 3.9737, 3.8676, 3.6960, 3.6756, 3.6559,\n",
      "          3.5797, 2.7986, 2.7352, 2.5085, 2.3664, 2.3154, 2.1793, 2.1132,\n",
      "          1.9850, 1.9782, 1.9754, 1.8998, 1.7070, 1.5174, 1.3566, 1.2460,\n",
      "          0.9629, 0.9619, 0.9039, 0.7962, 0.7296, 0.6970, 0.0000],\n",
      "         [1.9387, 1.7721, 1.4221, 1.1830, 1.1288, 1.0138, 0.8436, 0.6443,\n",
      "          0.6049, 0.5461, 0.4421, 0.4052, 0.3824, 0.3771, 0.3697, 0.3641,\n",
      "          0.3631, 0.3346, 0.3314, 0.2744, 0.2667, 0.2411, 0.2369, 0.2167,\n",
      "          0.2117, 0.1939, 0.1872, 0.1582, 0.1323, 0.1316, 0.0000],\n",
      "         [8.1322, 5.3656, 4.9480, 4.4870, 4.3960, 4.2185, 3.8075, 3.7992,\n",
      "          3.4562, 3.4097, 3.0187, 2.8953, 2.7886, 2.4886, 2.4792, 2.4753,\n",
      "          2.1262, 2.1188, 2.0520, 1.5288, 1.5006, 0.8630, 0.7565, 0.6725,\n",
      "          0.6167, 0.5292, 0.4272, 0.2087, 0.1783, 0.1640, 0.0000]]],\n",
      "       device='cuda:0', grad_fn=<TopkBackward0>),\n",
      "indices=tensor([[[3358, 7420, 6093, 5277, 3929, 2882, 2703, 1943,  943, 1062,  525,\n",
      "           349, 7927, 3619,  724, 6981, 5669,  862, 5925, 5828, 1396, 2883,\n",
      "          8042, 7479, 3052,    5,    3,    4,    1,    0,    2],\n",
      "         [2482,   39, 3529, 4565, 2935, 4704, 1969, 6933, 5906, 1408, 5480,\n",
      "          4194, 2400, 4633, 1346, 4616, 2835, 4009, 3547, 5511, 4969,  241,\n",
      "          5613, 7612, 5846, 3541, 3052, 1194, 7968, 4064,  225],\n",
      "         [7012,  485, 6288,  704, 4572, 3302, 4618, 6508, 1877, 7671, 1209,\n",
      "          4295, 4833, 1119, 5733, 1594, 5719, 5522,  409, 6381, 1533, 2146,\n",
      "          6674, 6519, 4830, 2951, 3624, 8042, 2883, 4729,    4],\n",
      "         [5407, 1141, 2081, 3128, 6286, 4689, 5796, 2882, 3432,  757, 4250,\n",
      "          3058, 4114, 6617, 4390, 3874, 5480, 4761, 3717, 7420, 7968, 3698,\n",
      "            94, 1093, 7497,  862, 5830, 1134, 2386, 2297,  301],\n",
      "         [8184, 1216, 5679, 6399, 6179,  577, 2071, 2571, 6422,  487, 3047,\n",
      "           792, 7519, 7913, 6578, 7197, 4171, 3841,  841,  391,  369,  289,\n",
      "          4905,  943, 4723, 3318, 2838, 2386,   41, 2951,  278]]],\n",
      "       device='cuda:0'))\n",
      "Residual effects at MLP(\n",
      "  (w): Bilinear(\n",
      "    in_features=1024, out_features=8192, bias=True\n",
      "    (gate): Identity()\n",
      "  )\n",
      "  (p): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "):\n",
      "tensor([[[-0.0003],\n",
      "         [ 0.0142],\n",
      "         [ 0.0048],\n",
      "         [ 0.0063],\n",
      "         [-0.0588]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#weird thing with the MLP layer\n",
    "print_effects_and_residual_effects(mlps[0], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae import Tracer\n",
    "mlp_0_tracer = Tracer(model._model, 0, out = dict(name = 'mlp-out', expansion = 8, k = 30), inp = dict(name = 'resid-mid', expansion = 8, k = 30))\n",
    "mlp_0_inp_vis = Visualizer(model._model, mlp_0_tracer.inp)\n",
    "mlp_0_out_vis = Visualizer(model._model, mlp_0_tracer.out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0322,  0.0873],\n",
       "        [-0.0208,  0.0505],\n",
       "        [-0.0175,  0.0118],\n",
       "        ...,\n",
       "        [ 0.0096,  0.0349],\n",
       "        [ 0.0021,  0.0787],\n",
       "        [-0.0465, -0.0740]], device='cuda:0', grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.stack((attn0_out[0][-1]/attn0_out[0][-1].norm(),DESIGNED/DESIGNED.norm()), dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0039, -0.0043],\n",
       "        [-0.0043,  0.0154]], device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_0_tracer.custom_q(mlp_0_tracer.out.w_dec.weight.data[:, 8184],t.stack((attn0_out[0][-1]/attn0_out[0][-1].norm(),DESIGNED/DESIGNED.norm()), dim = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model.trace(short_input):\n",
    "    mlp_0_in = model._envoy.transformer.h[0].n2.output\n",
    "    mlp_0_in.save()\n",
    "    mlp_0_out = model._envoy.transformer.h[0].mlp.output\n",
    "    mlp_0_out.save()\n",
    "q_8184 = mlp_0_tracer.custom_q(mlp_0_tracer.out.w_dec.weight.data[:, 8184])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0080, 0.0159, 0.0670,  ..., 0.0510, 0.0329, 0.0048], device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_0_tracer.out.w_dec.weight.data[:, 8184]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0183,  0.0201,  0.0076,  ...,  0.0574, -0.0068, -0.0547],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_0_tracer.out_latents[8184]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_8184_2 = mlp_0_tracer.q(8184)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.3424]], device='cuda:0', grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_0_in[:,-1] @ q_81842 @ mlp_0_in[:,-1].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.9216]], device='cuda:0', grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_0_out[:,-1] @ mlp_0_tracer.out.w_dec.weight.data[:, 8184].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_57_acts = compute_aggregated_latents(attns[0], {4: [57]}, norm_method = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1024])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_57_acts[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer_input = input_tensor[:, :6]\n",
    "baseline_longer = model.forward(longer_input)\n",
    "with model.trace(longer_input):\n",
    "    model._envoy.transformer.h[0].attn.output[0,-2] -= feat_57_acts[0][-1]\n",
    "    ablated_longer = model.output.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Token:\n",
      "  Token ID: 27854, Token: activate\n",
      "  Reference Log Prob: -3.4814\n",
      "  New Log Prob:       -3.4586\n",
      "  Change:             0.0227\n",
      "\n",
      "Reference Distribution Top 5 Tokens:\n",
      "  Rank 1: ID 8366, Token: trigger, Log Prob: -2.2230\n",
      "  Rank 2: ID 347, Token: be, Log Prob: -2.5693\n",
      "  Rank 3: ID 4244, Token: cause, Log Prob: -2.6290\n",
      "  Rank 4: ID 27854, Token: activate, Log Prob: -3.4814\n",
      "  Rank 5: ID 1840, Token: keep, Log Prob: -3.6992\n",
      "\n",
      "New Distribution Top 5 Tokens:\n",
      "  Rank 1: ID 8366, Token: trigger, Log Prob: -2.2237\n",
      "  Rank 2: ID 347, Token: be, Log Prob: -2.5647\n",
      "  Rank 3: ID 4244, Token: cause, Log Prob: -2.6401\n",
      "  Rank 4: ID 27854, Token: activate, Log Prob: -3.4586\n",
      "  Rank 5: ID 1840, Token: keep, Log Prob: -3.6425\n",
      "\n",
      "Overlap in Top 5 Tokens:\n",
      "  be, keep, cause, trigger, activate\n"
     ]
    }
   ],
   "source": [
    "compare_distributions(ablated_longer, baseline_longer, 27854)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
