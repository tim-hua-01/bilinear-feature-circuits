{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/miniconda3/envs/new/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.abspath('.')\n",
    "sys.path.append(parent_dir + '/bilinear_interp_tim')\n",
    "sys.path.append(parent_dir + '/dictionary_learning')\n",
    "sys.path.append(parent_dir)\n",
    "import torch as t\n",
    "from einops import rearrange, repeat, reduce,einsum\n",
    "from language import Transformer, Sight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer.from_pretrained(\"tdooms/fw-nano\", device=\"cuda\")\n",
    "model = Sight(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._model.config.dataset = 'fineweb'\n",
    "mlp_0 = model._model.transformer.h[0].mlp\n",
    "w_l_0 = model._model.transformer.h[0].mlp.w.weight.chunk(2, dim=0)[0]\n",
    "w_r_0 = model._model.transformer.h[0].mlp.w.weight.chunk(2, dim=0)[1]\n",
    "w_p_0 = model._model.transformer.h[0].mlp.p.weight\n",
    "l_bias, r_bias = model._model.transformer.h[0].mlp.w.bias.chunk(2, dim = 0)\n",
    "w_p_bias = model._model.transformer.h[0].mlp.p.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5353, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "t.manual_seed(123121)\n",
    "rand_vec_1 = t.randn(1024, device = 'cuda')/10\n",
    "rand_vec_2 = t.randn(1024, device = 'cuda')/10\n",
    "\n",
    "test_input = rand_vec_1 + rand_vec_2\n",
    "output = mlp_0(test_input)\n",
    "print(output.norm())\n",
    "normed_output = output / output.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5113]], device='cuda:0', grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "interaction = t.einsum(\"mi,mj,om,...o->...ij\", w_l_0, w_r_0, w_p_0, normed_output)\n",
    "interaction = 0.5 * (interaction + interaction.mT)\n",
    "interaction_output = test_input.unsqueeze(0) @ interaction @ test_input.unsqueeze(1)\n",
    "print(interaction_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0012]], device='cuda:0', grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "linear_component = t.diag(l_bias) @ w_l_0 +  t.diag(r_bias) @ w_r_0\n",
    "linear_hidden = linear_component @ test_input.unsqueeze(1)\n",
    "linear_output = normed_output.unsqueeze(0) @ w_p_0 @ linear_hidden\n",
    "print(linear_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0092]], device='cuda:0', grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hidden_bias = l_bias*r_bias\n",
    "bias_out = normed_output.unsqueeze(0) @ w_p_0 @ hidden_bias.unsqueeze(1)\n",
    "print(bias_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7390, device='cuda:0', grad_fn=<DotBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mlp_out_bias_out = t.dot(normed_output, w_p_bias)\n",
    "print(mlp_out_bias_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.2423]], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decomposed_contributions = mlp_out_bias_out + bias_out + linear_output + interaction_output\n",
    "decomposed_contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.norm()==decomposed_contributions.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def q_linear_and_bias(self, out_dir: torch.Tensor, project_mat: Optional[torch.Tensor] = None):\n",
    "    if project_mat is not None:\n",
    "        assert project_mat.shape[0] == out_dir.shape[0], f\"Projection matric shape {project_mat.shape} must match output direction shape {out_dir.shape}\"\n",
    "    model, layer = self.model, self.layer\n",
    "    interaction = torch.einsum(\"mi,mj,om,...o->...ij\", model.w_l[layer], model.w_r[layer], model.w_p[layer], out_dir)\n",
    "    interaction = 0.5 * (interaction + interaction.mT)\n",
    "    if project_mat is not None:\n",
    "        interaction = torch.einsum(\"il,jk,...ij->...lk\", project_mat, project_mat, interaction)\n",
    "    \n",
    "    l_bias, r_bias = model.transformer.h[layer].mlp.w.bias.chunk(2, dim = 0)\n",
    "\n",
    "    linear_component = torch.diag(l_bias) @ model.w_l[layer]  +  torch.diag(r_bias) @ model.w_r[layer]\n",
    "\n",
    "    pure_bias = l_bias*r_bias\n",
    "\n",
    "    return interaction, linear_component, pure_bias\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
